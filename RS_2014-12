{"secure_media_embed":{},"title":"It seems like the first step toward good machine vision is scale/distance logic","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417397464","retrieved_on":1441043796,"ups":1,"media":null,"selftext":"Is this the current favored technique?\n\nBackground: Natural or man made (Color, texture, pattern)         \n\nJudging Distance to camera: Visible texture, Atmospheric diffusion        \n\nJudging distance to background: Shading, lighting  \n         \nTexture: Metallic - shiny, machine scraped texture, silvery or coppery. Particulates? Fibrous?         \n\nBig or small: Diffusion, visible texture pattern, lighting/shading, blurred close-up effect, using logic relative to size of background\n\nMan Made: Perfect/straight lines + Metallic. Perfect curved surface\n\nIt seems like judging scale and distance is the very first step to determining what it is you're looking at. And it seems the best way to do that is to analyze diffusion, texture, camera effects (Blurring), and reasoning out the distance of the background and the object.\n\nIs this currently done?\n\n","permalink":"/r/MachineLearning/comments/2nw9u8/it_seems_like_the_first_step_toward_good_machine/","link_flair_text":null,"is_self":true,"author":"[deleted]","from":null,"id":"2nw9u8","archived":true,"link_flair_css_class":null,"name":"t3_2nw9u8","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":3,"quarantine":false,"edited":1417397727,"score":1,"stickied":false,"subreddit_id":"t5_2r3gv","created":1417397464,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nw9u8/it_seems_like_the_first_step_toward_good_machine/","secure_media":null,"from_id":null}
{"permalink":"/r/MachineLearning/comments/2nwrmu/interest_in_machine_learning_reading_group_meetup/","link_flair_text":null,"media":null,"selftext":"How many people would be interested in machine learning reading group meetup following this course http://www.seas.harvard.edu/courses/cs281/.\nWe can meet weekly and read the book \"Kevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press.\". We can also do the assignment from the course.\nThis is a tough course with heavy mathematics. The prerequisites are:\n\n1) any intro to machine learning\n\n2) statistical inference berger and cassella\n\n3) r programming (the book suggest matlab code, but we can easily translate it to r)\n\nP.S.: reading this book won't guarantee you a data science job. please see it as learning something complex for fun.","retrieved_on":1441043565,"ups":13,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417407851","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Interest in machine learning reading group meetup following this course http://www.seas.harvard.edu/courses/cs281/","secure_media_embed":{},"url":"http://www.reddit.com/r/MachineLearning/comments/2nwrmu/interest_in_machine_learning_reading_group_meetup/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417407851,"from_id":null,"stickied":false,"quarantine":false,"score":13,"edited":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":5,"archived":true,"id":"2nwrmu","name":"t3_2nwrmu","link_flair_css_class":null,"gilded":0,"author":"[deleted]","from":null,"is_self":true}
{"author_flair_text":null,"created_utc":"1417413511","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Dimensionality Reduction for non numerical data?","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nx0eg/dimensionality_reduction_for_non_numerical_data/","selftext":"If I have datasets with lot of features, some of them are non numerical (symbolic), is it possible to apply Dimensionality Reduction like with PCA and/or using Feature Extraction like RBM ?","media":null,"ups":6,"retrieved_on":1441043451,"author_flair_css_class":null,"num_comments":11,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2nx0eg","gilded":0,"id":"2nx0eg","archived":true,"from":null,"author":"IllSc","is_self":true,"from_id":null,"created":1417413511,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nx0eg/dimensionality_reduction_for_non_numerical_data/","stickied":false,"score":6,"edited":false,"quarantine":false}
{"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417419993,"url":"http://www.reddit.com/r/MachineLearning/comments/2nx8hr/deep_learning_with_nonimageaudiotext_data_proteins/","secure_media":null,"from_id":null,"quarantine":false,"score":31,"edited":false,"id":"2nx8hr","archived":true,"link_flair_css_class":null,"name":"t3_2nx8hr","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":17,"is_self":true,"author":"ddofer","from":null,"permalink":"/r/MachineLearning/comments/2nx8hr/deep_learning_with_nonimageaudiotext_data_proteins/","link_flair_text":null,"retrieved_on":1441043346,"ups":31,"media":null,"selftext":"Hi,\nI work with data that isn't Vision/images, audio or natural text, (Protein sequences). \nI've done a fair bit of work on extracting features from the sequences using a variety of representations (K-mer frequencies, discrete and continuous properties, time-signal representations, etc' ), and have worked with this using mainly SVMs and Random Forests for the best results up until now. \n\nI'm interested in applying some more \"advanced\" methods to this, mainly:\nA) techniques which can take advantage of huge amounts of unlabelled (incredibly heterogenous) samples (i.e. some of the 80 million protein sequences known). \n From what I know, Deep belief networks, and the various autoencoders + layers of thereof would be a good starting point for this? \nB) Use of multi-layer networks (EG multilayer perceptrons) and other techniques to better classify the data. (In conjunction with A presumably). \n\nThe features are mostly partially linear but weak, (i.e. individually relevant strong features, - a linear SVM does work, but with mediocre performance); And dimensionality of the \"raw\" features is moderately high - ~hundreds to tens of thousands (depending on what type of N-Gram length I extract). \n(I've tried applying PCA but performance dropped). \n\nI frame this as a binary classification task usually, In most cases my positive set is a few hundred to a few thousand samples, while my negative set is a random sampling from the \"unknown\" background of many millions of proteins not annotated as having the properties of the positive set, (i.e P-U learning framed as a binary case). \n(So - overfitting is a very big issue, even with Cross validation and relatively simple models such as RF/ensembles). \n\nAll my work so far has been with scikit learn (Python 3+). \n Nolearn and Breze look good, but I haven't managed to get them running on my home Windows PC, while the Lab Linux PC is limited in what I can install. \nEase of use is a major concern for me, I'm a neophyte with no rigorous background in ML, and I completely lack intuitions when it comes to nets. EG: How many layers to start with? Should they be larger or smaller than the input dimensionality [the literature is conflicted], should the layers go : [Large, medium, small, medium, large], or something else? Does it really make sense to try stacked sparse filtering and to have the first layer be larger than the input dimensionality? (Example from the FastML blog). etc' \n\nAlmost all the literature I've seen really focuses on Images, or audio text, and not \"Here's an unknown dataset, get better results than a RF with it\"..  (Again, a sole exception is the FastML \"Kaggle BlackBox challenge\" post).\nThank you very much!\n\n\nPS -\nHere's a basic form of the features and datasets I have in mind:\nhttp://www.ncbi.nlm.nih.gov/pubmed/24336809\n\nPPS -\n\n If it'll make it easier, imagine I'm giving as an example the Kaggle \"Forest Cover Type Prediction\" challenge. Same issues apply in terms of lack of intuition.\n\nThanks very much for any tips/advice/folk wisdom!\n\n","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417419993","secure_media_embed":{},"title":"\"Deep Learning\" with non-Image/Audio/Text data. (Proteins)","distinguished":null,"thumbnail":"self","downs":0}
{"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417423786","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Undefined function 'binornd' for input arguments of type 'double'","thumbnail":"default","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nxc6p/undefined_function_binornd_for_input_arguments_of/","ups":0,"retrieved_on":1441043299,"selftext":"my matlab is not finding binornd can anyone upload their source code for the file????\n(matlab R2014b)","media":null,"link_flair_css_class":null,"gilded":0,"name":"t3_2nxc6p","archived":true,"id":"2nxc6p","num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"[deleted]","stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417423786,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nxc6p/undefined_function_binornd_for_input_arguments_of/","score":0,"edited":false,"quarantine":false}
{"is_self":true,"from":null,"author":"[deleted]","link_flair_css_class":null,"name":"t3_2nxdi5","gilded":0,"archived":true,"id":"2nxdi5","num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"edited":1417425584,"score":1,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nxdi5/lstmnetwork_and_multivariate_timeseries/","from_kind":null,"created":1417425216,"subreddit_id":"t5_2r3gv","title":"LSTM-network and multivariate Time-Series","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417425216","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":1,"retrieved_on":1441043281,"selftext":"I'm currently working on a project with multivariate time-series data. Hence, each time point *t* of an input sequence consists of a vector of *n* values. I need to use LSTM networks for predicting a single scalar output value at each time point *t* from the input sequence. \n\nMy problem is, I don't think there is existing software which is able to deal with such a input data-structure. Therefore, I'm trying to mitigate this fact by adjusting the network topology to make standard approaches work. \n\nThe only idea which came to my mind is to train *n* seperate networks (with a standard univariate time-series) and combine their output at the last stage with another neuronal net. The problem is that this approach leads to sideeffects which I would like to omit.\n\nDo you have some other ideas/hints?\n\nEdit: typos/grammar\n\n","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nxdi5/lstmnetwork_and_multivariate_timeseries/"}
{"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417428377,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nxgfs/robustness_to_wrong_labels/","from_id":null,"quarantine":false,"edited":false,"score":3,"id":"2nxgfs","archived":true,"link_flair_css_class":null,"name":"t3_2nxgfs","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":9,"is_self":true,"author":"pl47","from":null,"permalink":"/r/MachineLearning/comments/2nxgfs/robustness_to_wrong_labels/","link_flair_text":null,"retrieved_on":1441043243,"ups":3,"media":null,"selftext":"I was wondering if someone cold refer to some literature on the robustness to wrongly labelled learning data. I currently have a problem where I have a limited amount of examples for which I have some estimate of the robustness of the label. \nSo which algorithms are best suited for this ? \nI  know adaboost is particularly bad for this due to the exponential loss function, and hence a Huber loss function would be more robust. \nBut what about classification ? There the loss function tends to be deviance or cross-entropy, would one just up-weight the training examples for which you the labels are more robust ? \nIs there a  robust-loss function you can use for classification ? \nIs there any consensus if NN or RF are more robust to mislabeled data ?   \n\nMany thanks for your help.\n\n\n \n\n\n\n\n        ","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417428377","secure_media_embed":{},"title":"Robustness to wrong labels","distinguished":null,"thumbnail":"self","downs":0}
{"permalink":"/r/MachineLearning/comments/2ny3vd/upcoming_workshop_combining_machine_learning_with/","link_flair_text":null,"media":null,"selftext":"Hi I am a research assistant to Dr. Verma at the University of Houston, \n\nDr. Verma and 4 other professors from the University of Texas, University of Houston, and the University of Massachusetts are organizing the \"International Workshop on Security and Privacy Analytics (SPA 2015)\" co-located with ACM CODASPY 2015.\n\nThe workshop is looking to publish new research in the combined fields of security and analytics including Machine Learning and NLP. This is the official statement:\n\n\n&gt; The workshop will focus on, but is not limited to, the following areas:\n\n&gt; * Natural Language Processing for security/privacy;\n&gt; * Data Mining techniques for security/privacy;\n&gt; * Machine learning for security/privacy;\n&gt; * Statistics for security/privacy;\n&gt; * Inference Control;\n&gt; * Privacy-preserving data mining;\n&gt; * Security of machine learning;\n&gt; * Security of data mining;\n&gt; * Security of natural language processing; \n&gt; * Case studies;\n&gt; * Educational topics and courses.\n\n\nIf any of you are are working on new research that would apply to this workshop and are interested, please let me know or follow the information at http://capex.cs.uh.edu/?q=secanalysis2015. I will be at the workshop and if any one from here decides to go or submit work please feel free to contact me!\n\nThank you!","retrieved_on":1441042939,"ups":0,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417448580","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Upcoming workshop combining Machine Learning with Security: Looking for papers","secure_media_embed":{},"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ny3vd/upcoming_workshop_combining_machine_learning_with/","created":1417448580,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0,"saved":false,"over_18":false,"num_comments":2,"author_flair_css_class":null,"archived":true,"id":"2ny3vd","gilded":0,"link_flair_css_class":null,"name":"t3_2ny3vd","author":"caedin8","from":null,"is_self":true}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"measuring classifier concordance","author_flair_text":null,"created_utc":"1417452609","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"Suppose I have a collection of binary classifiers, each of which outputs a \"probability\" value given a test vector. Quotes because outputs are not calibrated or necessarily between 0 and 1, but correlated in some way with label probability. \n\nHow would you quantify the similarity between these classifier's predictions, lacking any knowledge of the true labels?","media":null,"ups":1,"retrieved_on":1441042843,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nybef/measuring_classifier_concordance/","from":null,"author":"[deleted]","is_self":true,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"gilded":0,"name":"t3_2nybef","link_flair_css_class":null,"archived":true,"id":"2nybef","score":1,"edited":false,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417452609,"url":"http://www.reddit.com/r/MachineLearning/comments/2nybef/measuring_classifier_concordance/","secure_media":null,"stickied":false}
{"score":7,"edited":false,"quarantine":false,"from_id":null,"created":1417454864,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://hunch.net/?p=151364","stickied":false,"from":null,"author":"vkhuc","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2nyfsh","gilded":0,"link_flair_css_class":null,"id":"2nyfsh","archived":true,"selftext":"","media":null,"ups":7,"retrieved_on":1441042786,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nyfsh/allreduce_or_mpi_vs_parameter_server_approaches/","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Allreduce (or MPI) vs. Parameter server approaches","author_flair_text":null,"created_utc":"1417454864","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"hunch.net"}
{"ups":0,"retrieved_on":1441042104,"selftext":"For example, I can develop a model (say a random forest) in R but then have to deploy it where I can't use R and have to write all the code myself.\n\nA possible way is to use R to generate all possible predictions and then fit or even over fit a linear regression model to my predictions aiming for 100% R squared. This could be difficult to achieve with linear models.\n\nAny better methods?\n","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2nzwbi/whats_the_best_way_to_deploy_a_complicated/","title":"What's the best way to deploy a complicated machine learning model into a simple environment that has no libraries?","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417481683","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2nzwbi/whats_the_best_way_to_deploy_a_complicated/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1417481683,"from_kind":null,"is_self":true,"from":null,"author":"inoddy","link_flair_css_class":null,"gilded":0,"name":"t3_2nzwbi","id":"2nzwbi","archived":true,"num_comments":7,"author_flair_css_class":null,"saved":false,"over_18":false}
{"author_flair_text":null,"created_utc":"1417489277","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"How does a single neuron!!!","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o0amd/how_does_a_single_neuron/","selftext":"I am not able to understand how a single neuron learns, the specific problem that I am trying to solve is that I have been given a single neuron which obeys the Gradient Descent Optimization and there are n training samples {Xi} where each Xi is a 2D vector and I have to use a logistic sigmoid activation function for the neuron, along with n input vectors I have been given an output vector Tn which consists of the expected output values for the input vectors, we perform feed forward propagation technique on the neuron using \"an = w0 + w1*x1n + w2*x2n\"  and the output of the neuron is yn = 1/(1+exp(-an)) now I need to write a formula for sum of squares error function with weight decay regularizer for the neuron(firstly I don't understand what this weight decay regularizer does and how does it help), secondly I need to perform the gradient descent and derive the weight update iteration for w0, w1 and w2 (I don't understand this sadly :( ).\n\nWould be great if someone could help?","media":null,"ups":0,"retrieved_on":1441041920,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2o0amd","archived":true,"id":"2o0amd","from":null,"author":"[deleted]","is_self":true,"from_id":null,"created":1417489277,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o0amd/how_does_a_single_neuron/","stickied":false,"score":0,"edited":false,"quarantine":false}
{"from":null,"author":"[deleted]","is_self":true,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"name":"t3_2o0bbm","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2o0bbm","edited":false,"score":1,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o0bbm/what_metaheuristics_have_the_smallest_time/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1417489645,"stickied":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"What metaheuristics have the smallest time complexity w.r.t. number of variables?","secure_media_embed":{},"created_utc":"1417489645","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"Maybe there isn't a definite answer to this, as I can't find one when looking around. I'm talking about algorithms which would optimize a function which is given (for example) a binary string as the parameters. \n","media":null,"ups":1,"retrieved_on":1441041910,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o0bbm/what_metaheuristics_have_the_smallest_time/"}
{"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417489926","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Will expectation maximization always improve upon the last iteration until convergence?","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o0btc/will_expectation_maximization_always_improve_upon/","ups":1,"retrieved_on":1441041904,"selftext":"I am trying to implement an expectation maximization algorithm, and my implementation tends towards the optimal solution, but the tedancy is not consistent (calculated from the kullback leibler divergence). Is that evidence that my algorithm is not correctly implemented?","media":null,"name":"t3_2o0btc","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2o0btc","author_flair_css_class":null,"num_comments":4,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"[deleted]","stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o0btc/will_expectation_maximization_always_improve_upon/","secure_media":null,"created":1417489926,"subreddit_id":"t5_2r3gv","from_kind":null,"edited":false,"score":1,"quarantine":false}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o10zs/deep_learning_for_chess/","selftext":"","media":null,"ups":101,"retrieved_on":1441041577,"author_flair_text":null,"created_utc":"1417506869","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"erikbern.com","thumbnail":"http://b.thumbs.redditmedia.com/fFNJHG1Msh3tvumbI6p9knqiJhAv1utwQqDuozciJIE.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Deep learning for chess","from_id":null,"created":1417506869,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://erikbern.com/?p=841","secure_media":null,"stickied":false,"edited":false,"score":101,"quarantine":false,"author_flair_css_class":null,"num_comments":24,"over_18":false,"saved":false,"name":"t3_2o10zs","gilded":0,"link_flair_css_class":null,"id":"2o10zs","archived":true,"from":null,"author":"benanne","is_self":false}
{"author":"[deleted]","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"archived":true,"id":"2o12uv","link_flair_css_class":null,"gilded":0,"name":"t3_2o12uv","quarantine":false,"score":0,"edited":false,"created":1417508785,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o12uv/how_to_get_a_job_at_deepmind_and_elon_musks/","from_id":null,"stickied":false,"distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"How to get a job at DeepMind. And Elon Musk's comments.","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417508785","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"What would someone without a PhD need to do to get a job at a top 10 research group on deep learning / artificial intelligence?  Would you get a job offer if you broke some important machine learning benchmark?  Or won a competition?  Would that be enough?\n\nDo you really need to be at one of these places like DeepMind to be at the cutting edge of the field?  I mean, a lot of the job openings for DeepMind seem to be more mundane stuff, i.e. stuff where on the day to day, you might just be coding on a need to know basis, and be kept in the dark about the bigger picture stuff.\n\nThe reason I want to get a job at DeepMind, is after hearing Elon Musk's comment about how within 5 years A.I. is going to be dangerous.  I know what a true A.I. could do, and basically, if someone had a true A.I. with the IQ of a human, they could take over the world within 2 years.  That is not a joke.  Once it reached human level intelligence it could boostrap itself up to 100x or 1,000x a human, and then we are talking about a machine that could invent things we can't even imagine and do it in a matter of hours or minutes.\n\nSo, do you think that DeepMind is so much ahead of the rest of the field that they could do this?  I mean, we already heard from a very rational person who claims they have \"Superintelligences\".  Seeing how close deep learning is to the way the human brain works, I think it is very plausible, especially with the money and talent Google is undoubtedly throwing at it.\n\nAnother bad scenario would be that the government is going to realize what a weapon A.I. is and they will make it completely illegal.\n","retrieved_on":1441041553,"ups":0,"permalink":"/r/MachineLearning/comments/2o12uv/how_to_get_a_job_at_deepmind_and_elon_musks/","link_flair_text":null}
{"from":null,"author":"rumpelstilzien","is_self":false,"author_flair_css_class":null,"num_comments":1,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2o1fq6","id":"2o1fq6","archived":true,"edited":false,"score":8,"quarantine":false,"from_id":null,"secure_media":{"oembed":{"thumbnail_height":360,"author_url":"http://www.youtube.com/user/interintelligence","url":"http://www.youtube.com/watch?v=YWQnzylhgHc","type":"video","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FYWQnzylhgHc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYWQnzylhgHc&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FYWQnzylhgHc%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"author_name":"Timothy Busbice","provider_name":"YouTube","provider_url":"http://www.youtube.com/","description":"Extending the connectome of C Elegans to a robot, displays behaviors we observe in the living organism and allows researchers to study the connectome from sensory input to motor output in real world environments.","thumbnail_width":480,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FYWQnzylhgHc%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"title":"CElegans Neurorobotics"},"type":"youtube.com"},"url":"https://www.youtube.com/watch?v=YWQnzylhgHc","subreddit_id":"t5_2r3gv","created":1417522405,"from_kind":null,"stickied":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/i5Fz4CL7LNr1h9N6AhyPz70-XQf46-BLPa3Cn88tlGk.jpg","distinguished":null,"title":"Lego robot with the \"brain\" of a real worm","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FYWQnzylhgHc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYWQnzylhgHc&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FYWQnzylhgHc%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":338,"scrolling":false},"created_utc":"1417522405","author_flair_text":null,"media_embed":{"scrolling":false,"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FYWQnzylhgHc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYWQnzylhgHc&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FYWQnzylhgHc%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"subreddit":"MachineLearning","domain":"youtube.com","hide_score":false,"selftext":"","media":{"type":"youtube.com","oembed":{"provider_name":"YouTube","provider_url":"http://www.youtube.com/","width":600,"thumbnail_url":"http://i.ytimg.com/vi/YWQnzylhgHc/hqdefault.jpg","title":"CElegans Neurorobotics","description":"Extending the connectome of C Elegans to a robot, displays behaviors we observe in the living organism and allows researchers to study the connectome from sensory input to motor output in real world environments.","thumbnail_width":480,"author_url":"http://www.youtube.com/user/interintelligence","type":"video","url":"http://www.youtube.com/watch?v=YWQnzylhgHc","thumbnail_height":360,"height":338,"author_name":"Timothy Busbice","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FYWQnzylhgHc%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYWQnzylhgHc&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FYWQnzylhgHc%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"}},"ups":8,"retrieved_on":1441041386,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o1fq6/lego_robot_with_the_brain_of_a_real_worm/"}
{"score":0,"edited":false,"quarantine":false,"from_id":null,"created":1417530614,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.bbc.co.uk/news/technology-30290540","secure_media":null,"stickied":false,"from":null,"author":"katchoovanski","is_self":false,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2o1puh","id":"2o1puh","archived":true,"selftext":"","media":null,"ups":0,"retrieved_on":1441041255,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o1puh/stephen_hawking_warns_artificial_intelligence/","thumbnail":"http://b.thumbs.redditmedia.com/W-7MfccXYcmdFYz76eRmazqHmsKxGc_eb0RZF1oDUOI.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Stephen Hawking warns artificial intelligence could end mankind","author_flair_text":null,"created_utc":"1417530614","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"bbc.co.uk"}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417533689","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Advice on self-study for my winter break?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2o1une/advice_on_selfstudy_for_my_winter_break/","link_flair_text":null,"media":null,"selftext":"Hello everyone,\n\nI'm a math major interested in getting into data science. I took a seminar on data science this semester (ending in 2 weeks) and it was entirely on data mining/python/data cleaning/visualization etc.\n\nI want to spend my winter break of 6 weeks studying machine learning and just getting a deeper feel of the subject. I don't like video lectures and I kinda poked my head into \"The Elements of Statistical Learning\" but I feel like it may be just a little bit over my head (only on page 18). \n\nI'm an experienced python programmer (with understanding of basic data structures/algorithms/complexity) and have taken the following math courses:{calc 3, intro linear algebra, intro probability, intro real analysis}. Is there a book or pdf that I can read and finish in 6 weeks that will give me the *theory* of machine learning in some sort of nutshell?\n\nShould I just put my face into The Elements of Statistical Learning for 6 weeks (it's not undoable, just seems kinda slow) or do you have a better book recommendation? \n\nOh and I think I can only manage 400-500 pages in 6 weeks (Elements is like 700+). \n\nI have the practical side covered with the book \"Building Machine Learning Systems with Python\" so only theory recommendations please and thank you!","retrieved_on":1441041193,"ups":9,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":6,"archived":true,"id":"2o1une","gilded":0,"name":"t3_2o1une","link_flair_css_class":null,"author":"Nixonite","from":null,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/2o1une/advice_on_selfstudy_for_my_winter_break/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417533689,"from_id":null,"stickied":false,"quarantine":false,"score":9,"edited":false}
{"score":0,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o1z5c/getting_the_index_of_a_leaf_where_an_example/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417536214,"is_self":true,"from":null,"author":"[deleted]","name":"t3_2o1z5c","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2o1z5c","num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"ups":0,"retrieved_on":1441041134,"selftext":"Hi everyone,\n\nIs there any way to get the leaf to which an example belongs in scikit-learn's trees ?\nActually I am trying out the method suggested in this article \n[practical lessons facebook from Predicting Clicks on Ads at Facebook](https://www.facebook.com/publications/329190253909587/)\n\nIf you guys have any suggestions, or have tried out the approach yourselves, I'll be glad to hear your advice.\n\nThank you !","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o1z5c/getting_the_index_of_a_leaf_where_an_example/","title":"Getting the index of a leaf where an example belongs in scikit learn","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417536214","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"is_self":true,"from":null,"author":"bkaz","name":"t3_2o22v3","gilded":0,"link_flair_css_class":null,"id":"2o22v3","archived":true,"author_flair_css_class":null,"num_comments":11,"over_18":false,"saved":false,"score":0,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417538121,"url":"http://www.reddit.com/r/MachineLearning/comments/2o22v3/cognitive_algortihm/","secure_media":null,"secure_media_embed":{},"title":"Cognitive algortihm","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417538121","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1441041086,"selftext":"Lots of people think that main problem in AI is a lack of funding. I disagree, Einstein didn't need a pay to work on his theory. GI is the most theoretical problem ever, &amp; such work must be driven by curiosity. Anyway, I offer prizes of up to ~$500K for relevant ideas or stimulating questions &amp; objections. $1600 is paid out so far, mostly for stimulation. This won\u2019t work unless my intro (below) rings a lot of bells for you.\n\nMy approach is unsupervised learning, vaguely similar to hierarchical fuzzy clustering. But no method that I know of is close enough. This field is largely experimental (especially ANNs), while I believe in theoretical integrity. So, I had to start from the scratch. I am not interested in combining disparate methods that somehow \u201cwork\u201d, any suggestion must fit into strictly incremental-complexity search hierarchy outlined in the intro. Unless you can find a flaw in my reasoning, which would be even more valuable:\n\nI define intelligence as ability to predict &amp; plan, which can only be done by discovering &amp; projecting patterns. This perspective is well established: pattern recognition is a core of any IQ test. A pattern is a set of matching inputs or lower-level patterns. Partial matches &amp; misses are produced by comparison among input patterns, over selectively extended range of search. I see no alternative to these definitions, &amp; expand them to derive operations on the inputs. This deduction is uniquely incremental, hence a singular in \u201ccognitive algorithm\u201c...:\n\nwww.cognitivealgorithm.info\n","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o22v3/cognitive_algortihm/"}
{"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417540773,"url":"http://aka.ms/fvm1op","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":1,"id":"2o280e","archived":true,"name":"t3_2o280e","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"is_self":false,"author":"MLBlogTeam","from":null,"permalink":"/r/MachineLearning/comments/2o280e/python_tools_for_visual_studio_now_integrates/","link_flair_text":null,"retrieved_on":1441041019,"ups":1,"media":null,"selftext":"","hide_score":false,"domain":"aka.ms","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417540773","secure_media_embed":{},"title":"Python Tools for Visual Studio now integrates with Azure Machine Learning","distinguished":null,"thumbnail":"default","downs":0}
{"is_self":true,"from":null,"author":"ighbal","name":"t3_2o28ct","link_flair_css_class":null,"gilded":0,"id":"2o28ct","archived":true,"author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417540957,"url":"http://www.reddit.com/r/MachineLearning/comments/2o28ct/libraries_to_get_calibrated_probabilities_for_ml/","secure_media":null,"secure_media_embed":{},"title":"Libraries to get calibrated probabilities for ML outlier detection algorithm ?","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417540957","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1441041015,"selftext":"I am currently developping a 2 class classification algorithm. However, as the dataset is at the moment really small (&lt;50 observations) and imbalanced (~1/10 ratio), I decided to rather first concentrate on developing a one class, i.e. outlier detection ML algorithm. Moreover, I'll need to get from this ML algorithm, an estimate of the conditional probability P(Outlier|X).\nAs I have read that conditional probabilites for strong imbalanced dataset, needs correct calibration, and that Platt scaling is not recommended in that case,\nI'd like to ask to the community here if it exist some libraries in R, Python, ... that would facilitate this calibration exercise.","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o28ct/libraries_to_get_calibrated_probabilities_for_ml/"}
{"title":"Deep Learning RNNaissance with Dr. Juergen Schmidhuber (NYC-ML Meetup)","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F113402131&amp;url=http%3A%2F%2Fvimeo.com%2F113402131&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F498716569_960.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=vimeo\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/zOiW_-aGqa74NbEf8xFt3kYeqfEnA4IyT5ca1Ean4mE.jpg","domain":"vimeo.com","hide_score":false,"media_embed":{"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F113402131&amp;src_secure=1&amp;url=http%3A%2F%2Fvimeo.com%2F113402131&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F498716569_960.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=vimeo\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"subreddit":"MachineLearning","created_utc":"1417547475","author_flair_text":null,"retrieved_on":1441040843,"ups":7,"media":{"type":"vimeo.com","oembed":{"title":"Deep Learning RNNaissance with Dr. Juergen Schmidhuber","width":600,"thumbnail_url":"http://i.vimeocdn.com/video/498716569_960.jpg","thumbnail_width":960,"description":"A great session of NYC-ML Meetup Hosted by ShutterStock in the glorious Empire State building.","provider_name":"Vimeo","provider_url":"https://vimeo.com/","author_name":"xamdam","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F113402131&amp;src_secure=1&amp;url=http%3A%2F%2Fvimeo.com%2F113402131&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F498716569_960.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=vimeo\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","type":"video","author_url":"http://vimeo.com/user5498254","thumbnail_height":540}},"selftext":"","permalink":"/r/MachineLearning/comments/2o2ll4/deep_learning_rnnaissance_with_dr_juergen/","link_flair_text":null,"is_self":false,"author":"xamdam","from":null,"archived":true,"id":"2o2ll4","name":"t3_2o2ll4","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":3,"quarantine":false,"edited":false,"score":7,"stickied":false,"url":"https://vimeo.com/113402131","secure_media":{"oembed":{"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F113402131&amp;url=http%3A%2F%2Fvimeo.com%2F113402131&amp;image=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F498716569_960.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=vimeo\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"xamdam","height":338,"thumbnail_height":540,"type":"video","author_url":"http://vimeo.com/user5498254","thumbnail_width":960,"description":"A great session of NYC-ML Meetup Hosted by ShutterStock in the glorious Empire State building.","title":"Deep Learning RNNaissance with Dr. Juergen Schmidhuber","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F498716569_960.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","provider_url":"https://vimeo.com/","provider_name":"Vimeo"},"type":"vimeo.com"},"from_kind":null,"created":1417547475,"subreddit_id":"t5_2r3gv","from_id":null}
{"retrieved_on":1441040757,"ups":0,"media":null,"selftext":"Hello! So, we've broken the greater enrollment problem into a smaller one that just looks at the past 5 years worth of transcripts and sees if it can use that information to predict whether or not a student will take a specific class in the next quarter. I have a function that converts a \"transcript\" into a bit vector - 1 if the class has been taken and 0 if the class has not been taken. Each vector is ordered exactly the same and each vector is followed by True or False indicating whether or not (in the next quarter) they took the class we're interested in (can easily be turned to 0 or 1). I'm just wondering how I would use logistic regression (scikit-learn) to approach this problem, I'm a little stuck/lost. \nAlso if there are any things you would change or want to clarify, ask away.\nThanks!","permalink":"/r/MachineLearning/comments/2o2s8a/need_help_with_approaching_enrollment_prediction/","link_flair_text":null,"title":"Need help with approaching enrollment prediction","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417550670","author_flair_text":null,"quarantine":false,"score":0,"edited":false,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2o2s8a/need_help_with_approaching_enrollment_prediction/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417550670,"from_id":null,"is_self":true,"author":"[deleted]","from":null,"id":"2o2s8a","archived":true,"name":"t3_2o2s8a","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1}
{"title":"How to use the \"little bag of bootstraps\" methodology to compute \"error bounds\" on machine learning tasks.","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/ZIzyt4n0GDuKUaYorKtN2jhWRkM7A7002ChBHJh77LY.jpg","domain":"blog.dominodatalab.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417551880","author_flair_text":null,"retrieved_on":1441040724,"ups":8,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2o2us0/how_to_use_the_little_bag_of_bootstraps/","link_flair_text":null,"is_self":false,"author":"AnnaOnTheWeb","from":null,"id":"2o2us0","archived":true,"name":"t3_2o2us0","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"edited":false,"score":8,"stickied":false,"url":"http://blog.dominodatalab.com/getting-error-bounds-on-classification-metrics/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417551880,"from_id":null}
{"score":2,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o31a5/helpresources_withfor_bayesian_optimization/","created":1417554889,"subreddit_id":"t5_2r3gv","from_kind":null,"is_self":true,"from":null,"author":"genix2011","gilded":0,"link_flair_css_class":null,"name":"t3_2o31a5","archived":true,"id":"2o31a5","author_flair_css_class":null,"num_comments":1,"saved":false,"over_18":false,"ups":2,"retrieved_on":1441040580,"selftext":"Hello all,\n\nI want to implement Bayesian Optimization like described in the paper \"Active Preference Learning with Discrete Choice Data\" - by Eric Brochu, Nando de Freitas and Abhijeet Ghosh (http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_902.pdf). \nUnfortunately it seems like it is too much for me right now, I seem to miss a lot of knowledge in GP and Statistics.\n\nDoes anyone have any resources that could help understanding Bayesian Optimization better? Or, perhaps even an implementation of the referenced paper?\n\nThank you very much.","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o31a5/helpresources_withfor_bayesian_optimization/","title":"Help/Resources with/for Bayesian Optimization","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417554889","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"media":null,"selftext":"I'm not sure if this is the best place to be asking this question, but I'm not sure where else I would ask it. I have made a model using support vector machines in scikit-learn and I would like to use it in an android app. Are there anyways to easily port this model? After doing some searching I was unable to find anything, so any references would be of help.","retrieved_on":1441040323,"ups":3,"permalink":"/r/MachineLearning/comments/2o3l2x/how_to_deploy_a_model_from_scikitlearn_to_android/","link_flair_text":null,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"How to deploy a model from scikit-learn to android?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417564460","hide_score":false,"domain":"self.MachineLearning","quarantine":false,"edited":false,"score":3,"created":1417564460,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o3l2x/how_to_deploy_a_model_from_scikitlearn_to_android/","from_id":null,"stickied":false,"author":"[deleted]","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":5,"author_flair_css_class":null,"id":"2o3l2x","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2o3l2x"}
{"permalink":"/r/MachineLearning/comments/2o3vu3/is_there_any_ml_technique_that_relies_on_long/","link_flair_text":null,"retrieved_on":1441040184,"ups":0,"media":null,"selftext":"I've been looking through ML techniques and most of them, if I get it right, are kinda \"flat\" in that they search through some space to solve some specific problem. After the training phase, it doesn't really change much. Is there any kind of technique that targets the long term? That is, teaching something many different things, and allowing it to improve its memory and gain new experiences with time, never stopping to learn (including when it is computing something)?\n\nI've also never heard of an ML algorithm that learns about the problem **while** it computes it.","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417569914","author_flair_text":null,"title":"Is there any ML technique that relies on long term memories?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2o3vu3/is_there_any_ml_technique_that_relies_on_long/","secure_media":null,"created":1417569914,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"quarantine":false,"score":0,"edited":false,"archived":true,"id":"2o3vu3","gilded":0,"link_flair_css_class":null,"name":"t3_2o3vu3","saved":false,"over_18":false,"num_comments":5,"author_flair_css_class":null,"is_self":true,"author":"SrPeixinho","from":null}
{"from_id":null,"secure_media":null,"url":"https://github.com/EducationalTestingService/skll","created":1417580752,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false,"score":45,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":10,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2o4fxx","archived":true,"id":"2o4fxx","from":null,"author":"danielblanchard","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o4fxx/scikitlearn_laboratory_skll/","selftext":"","media":null,"ups":45,"retrieved_on":1441039924,"created_utc":"1417580752","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"github.com","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/mvaLsQBFsxAJVuChutHZFkWgUew3fXf1fMLHyZiJ6PQ.jpg","distinguished":null,"title":"SciKit-Learn Laboratory (SKLL)","secure_media_embed":{}}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o4q0f/learning_algorithms_cheatsheet/","selftext":"Hello r/MachineLearning\n\nI have my finals coming up and I'm enrolled in a Machine Learning course at my university. The course is not very well taught and I had been doing the Andrew NG course on Coursera before the semester began. Due to certain other responsibilities, I was unable to complete the Coursera course. My finals are in 2 days and the finals has a complete section with Modelling problems.\n \nThe course has been taught in such a way that it is completely theoretical and less application focused. I messed up in this regards and didn't pay much attention to the application area of ML.\nThe professor has focused on Decision trees, ANN's, SVMs, Clustering, Bayesian Networks, Reinforcement Learning, Ensemble Classifiers, Active learning and Dimensionality reduction methodologies. \n\nIs there any resource available which gives a side by side comparison of the application of these algorithms to various problems or just a repository of Machine learning modelling problems with their solutions so that I can get into that particular mindset which would be helpful to me at this point of time.\n\nI know I messed up this semester, I would really appreciate it if you guys could help me out at this late hour.","media":null,"ups":0,"retrieved_on":1441039793,"author_flair_text":null,"created_utc":"1417587073","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Learning algorithms CheatSheet?","from_id":null,"created":1417587073,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o4q0f/learning_algorithms_cheatsheet/","secure_media":null,"stickied":false,"edited":false,"score":0,"quarantine":false,"num_comments":2,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2o4q0f","link_flair_css_class":null,"gilded":0,"id":"2o4q0f","archived":true,"from":null,"author":"deathstone","is_self":true}
{"retrieved_on":1441039706,"ups":2,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2o4wpv/deep_learning_face_representation_by_joint/","link_flair_text":null,"secure_media_embed":{},"title":"Deep Learning Face Representation by Joint Identification-Verification -- Paper that reports better accuracy than Deepface on LFW","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"arxiv.org","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417592323","quarantine":false,"edited":false,"score":2,"stickied":false,"from_kind":null,"created":1417592323,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://arxiv.org/abs/1406.4773","from_id":null,"is_self":false,"author":"rorschach122","from":null,"id":"2o4wpv","archived":true,"link_flair_css_class":null,"name":"t3_2o4wpv","gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null}
{"from":null,"author":"Sergiointelnics","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2o50kt","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2o50kt","edited":false,"score":0,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417596155,"secure_media":null,"url":"http://ai4g.com/","stickied":false,"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Artificial Intelligence for Games!!!","author_flair_text":null,"created_utc":"1417596155","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"ai4g.com","selftext":"","media":null,"ups":0,"retrieved_on":1441039657,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o50kt/artificial_intelligence_for_games/"}
{"from":null,"author":"iwansyahp","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2o554p","gilded":0,"archived":true,"id":"2o554p","score":0,"edited":false,"quarantine":false,"from_id":null,"url":"http://arxiv.org/abs/1308.4214","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417601080,"stickied":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Pylearn2: a machine learning research library","secure_media_embed":{},"created_utc":"1417601080","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"arxiv.org","hide_score":false,"selftext":"","media":null,"ups":0,"retrieved_on":1441039597,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o554p/pylearn2_a_machine_learning_research_library/"}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o59ud/understanding_big_data_what_is_unsupervised/","selftext":"","media":null,"ups":0,"retrieved_on":1441039537,"author_flair_text":null,"created_utc":"1417606240","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"blog.aureusanalytics.com","thumbnail":"http://b.thumbs.redditmedia.com/rKqGqOK4gCROvFb9scjq-9kTACRmD9MJRMgl5khrcWA.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Understanding Big Data: What is unsupervised learning","from_id":null,"created":1417606240,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://blog.aureusanalytics.com/unsupervised-learning","secure_media":null,"stickied":false,"score":0,"edited":false,"quarantine":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2o59ud","link_flair_css_class":null,"gilded":0,"id":"2o59ud","archived":true,"from":null,"author":"soniyaiyer","is_self":false}
{"retrieved_on":1441039371,"ups":4,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2o5mmw/14121058_effective_use_of_word_order_for_text/","link_flair_text":null,"title":"[1412.1058] Effective Use of Word Order for Text Categorization with Convolutional Neural Networks","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"arxiv.org","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417617151","author_flair_text":null,"quarantine":false,"edited":false,"score":4,"stickied":false,"url":"http://arxiv.org/abs/1412.1058","secure_media":null,"from_kind":null,"created":1417617151,"subreddit_id":"t5_2r3gv","from_id":null,"is_self":false,"author":"improbabble","from":null,"id":"2o5mmw","archived":true,"link_flair_css_class":null,"name":"t3_2o5mmw","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":7}
{"stickied":false,"url":"http://numenta.org/blog/2014/12/03/htm-on-the-jvm.html","secure_media":null,"subreddit_id":"t5_2r3gv","created":1417624737,"from_kind":null,"from_id":null,"quarantine":false,"score":17,"edited":false,"archived":true,"id":"2o5zp3","gilded":0,"link_flair_css_class":null,"name":"t3_2o5zp3","saved":false,"over_18":false,"num_comments":12,"author_flair_css_class":null,"is_self":false,"author":"numenta","from":null,"permalink":"/r/MachineLearning/comments/2o5zp3/htm_in_java/","link_flair_text":null,"retrieved_on":1441039202,"ups":17,"media":null,"selftext":"","domain":"numenta.org","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417624737","author_flair_text":null,"title":"HTM in Java!","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/z7mVZA1ucx8qGhtyuCsGjfNIf5xFaLwH6i_tn8c43ZM.jpg"}
{"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o682s/online_masters_in_ml/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1417629089,"from_id":null,"quarantine":false,"score":3,"edited":false,"archived":true,"id":"2o682s","link_flair_css_class":null,"name":"t3_2o682s","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":12,"is_self":true,"author":"ai_noob","from":null,"permalink":"/r/MachineLearning/comments/2o682s/online_masters_in_ml/","link_flair_text":null,"retrieved_on":1441039092,"ups":3,"media":null,"selftext":"Are there any valid all online options out there for getting a master's degree in machine learning?  I did a quick search myself and couldn't get past all the ads for random online programs.  I currently have a bachelor's in CS from MIT so I think I should be able to jump right in but there are no programs near where I live. Thanks.","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417629089","author_flair_text":null,"title":"Online masters in ML?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self"}
{"score":7,"edited":false,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o6d51/how_do_i_choose_the_right_or_optimal_random/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1417631604,"stickied":false,"from":null,"author":"TissueReligion","is_self":true,"author_flair_css_class":null,"num_comments":19,"saved":false,"over_18":false,"name":"t3_2o6d51","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2o6d51","selftext":"Hey guys. I have a practical concern about the stochasticity of random forest generation.\n\nSo lets say I have a dataset that I'm training a random forest model on, and I do 100 simulations, generating a new random forest each time, and I get a new model with a new associated classification accuracy each time.\n\nWhat is a rational approach to selecting the \"optimal\" random forest model? Just selecting the one with the highest accuracy on a particular set of training data doesn't seem like a good metric.\n\nWould this approach make sense?\n-Generate 100 random forests on 100 different partitions of the test data\n-Measure the classification accuracy of each of the random forests on 100 partitions of the test data (ie, 100 random forests * 100 tests each = 10,000 tests total)\n-Pick the forest with the highest total accuracy.\n\nEh?","media":null,"ups":7,"retrieved_on":1441039028,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o6d51/how_do_i_choose_the_right_or_optimal_random/","downs":0,"thumbnail":"self","distinguished":null,"title":"How do I choose the \"right\" or \"optimal\" random forest model?","secure_media_embed":{},"created_utc":"1417631604","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o6w9p/astronomical_data_mining/","ups":5,"retrieved_on":1441038779,"selftext":"I just heard about this field today, and it's got me really interested. I'm applying to graduate schools for a master's is statistics, and have a strong programming background. Does anybody know anything about this field? What jobs there are, and where? It seems like a great way to combine my love of space with programming and stats. ","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417640828","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Astronomical data mining?","thumbnail":"self","downs":0,"distinguished":null,"stickied":false,"from_id":null,"from_kind":null,"created":1417640828,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o6w9p/astronomical_data_mining/","score":5,"edited":false,"quarantine":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2o6w9p","id":"2o6w9p","archived":true,"num_comments":7,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"kinofpumps"}
{"hide_score":false,"domain":"deeplearning.cs.toronto.edu","author_flair_text":null,"created_utc":"1417642216","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Images to Text demo - Toronto Deep Learning","thumbnail":"http://b.thumbs.redditmedia.com/8cNexAUAmQPLRpheHwvLTm0sA35YUVKUvzuIO6K0-HQ.jpg","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o6z5n/images_to_text_demo_toronto_deep_learning/","ups":15,"retrieved_on":1441038741,"selftext":"","media":null,"gilded":0,"name":"t3_2o6z5n","link_flair_css_class":null,"archived":true,"id":"2o6z5n","author_flair_css_class":null,"num_comments":9,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"benanne","stickied":false,"from_id":null,"created":1417642216,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://deeplearning.cs.toronto.edu/i2t","secure_media":null,"score":15,"edited":false,"quarantine":false}
{"stickied":false,"subreddit_id":"t5_2r3gv","created":1417643971,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o72mk/taming_latency_variability_and_scaling_deep/","secure_media":null,"from_id":null,"quarantine":false,"score":1,"edited":false,"id":"2o72mk","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2o72mk","over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"is_self":true,"author":"oneAngrySonOfaBitch","from":null,"permalink":"/r/MachineLearning/comments/2o72mk/taming_latency_variability_and_scaling_deep/","link_flair_text":null,"retrieved_on":1441038696,"ups":1,"media":null,"selftext":"Great talk on how google is implementing Deep Learning to solve real problems in production.\n\nhttps://plus.google.com/+ResearchatGoogle/posts/C1dPhQhcDRv","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417643971","secure_media_embed":{},"title":"Taming Latency Variability and Scaling Deep Learning","distinguished":null,"thumbnail":"self","downs":0}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417647892","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Scikit vs R for random forest","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2o7a76/scikit_vs_r_for_random_forest/","link_flair_text":null,"media":null,"selftext":"I am looking into a random forest model that will use both continuous and categorical variables. I have used scikit for forests in the past, but I've never had to use categorical variables. Based on what I've read, these seem to be the immediately obvious pros and cons for the two...\n\n* R can handle categorical variables without any transformation\n* R can't handle categorical variables that have more than 32 levels (and I'm using state). The ability to use categorical variables might not matter\n* R can choose a multiple levels from a category at a specific branch. If I have a dummy variable for each label in a category, this is not possible\n* Creating a single numeric variable for categories in scikit implies ordinality. Creating a dummy variable for each level of each categorical variable could drastically increase data size and dimensionality\nIt seems like there are pros and cons to each.\n\nUnless my understanding of ordinality in integer labeled categories is wrong, it seems like a branch in a scikit can't accomplish the same level of interaction that one in R could because R could split on multiple values of a categorical variable instead of one value at a time. At the same time, I wouldn't even be able to pass a few of my categories into R because of the label limit. With sufficiently deep trees and sufficiently large forests, would this difference in implementation even matter?\n\nAny thoughts would be appreciated. Thanks.","retrieved_on":1441038598,"ups":10,"saved":false,"over_18":false,"num_comments":7,"author_flair_css_class":null,"id":"2o7a76","archived":true,"name":"t3_2o7a76","link_flair_css_class":null,"gilded":0,"author":"neelshiv","from":null,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/2o7a76/scikit_vs_r_for_random_forest/","secure_media":null,"created":1417647892,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"score":10,"edited":1417664638}
{"is_self":false,"author":"urinec","from":null,"id":"2o7xgp","archived":true,"name":"t3_2o7xgp","gilded":0,"link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"score":0,"edited":false,"stickied":false,"url":"http://www.datasciencecentral.com/profiles/blogs/new-selection-of-40-machine-learning-big-data-ressources-and","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417660213,"from_id":null,"title":"63 Machine Learning, Data Science, Big Data Resources and Articles","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"datasciencecentral.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417660213","author_flair_text":null,"retrieved_on":1441038297,"ups":0,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2o7xgp/63_machine_learning_data_science_big_data/","link_flair_text":null}
{"is_self":true,"author":"kaustest","from":null,"id":"2o8j56","archived":true,"link_flair_css_class":null,"name":"t3_2o8j56","gilded":0,"saved":false,"over_18":false,"num_comments":8,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":5,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o8j56/ways_to_improve_dynamic_time_warping_word/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1417673094,"from_id":null,"title":"Ways to improve dynamic time warping word recognition system?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417673094","author_flair_text":null,"retrieved_on":1441038016,"ups":5,"media":null,"selftext":"I recently got interested in speech recognition and have implemented a simple dynamic time warp system for word recognition for my own learning purpose. However after testing a bit I believe that I might made a mistake somewhere in the implementation, as the least distance value in traversing the dtw matrix does not accurately separate different word from each other. Here is the step I followed in the implementation.\n\n1. Compute mfcc for two wav samples using https://github.com/jameslyons/python_speech_features (I will eventually replace it with my own mfcc algorithm)\n\n2. Compute l2 norm for the top 13 mfcc in order\n\n3. Traverse through the dtw matrix and find the least distance.\n\n\nBelow are my results\n\nComparing 1 of the kiwi file to all other file I get the following average\n\n    kiwi\n    53.5627956541\n    apple\n    52.8226506157\n    banana\n    57.885524018\n    lime\n    48.5113003162\n    orange\n    63.9675030969\n\nHere is my code https://gist.github.com/anonymous/1323692feea2a2bcfba4\n\nI feel I am missing some crucial steps, any advice will be appreciated.\n\nThanks\n\nEdit:\n\nI am using audio file from\n\nhttps://dl.dropboxusercontent.com/u/15378192/audio.tar.gz\n\nand \n\nhttp://www.forvo.com/word/apple/#en","permalink":"/r/MachineLearning/comments/2o8j56/ways_to_improve_dynamic_time_warping_word/","link_flair_text":null}
{"is_self":true,"from":null,"author":"CompleteSkeptic","name":"t3_2o8s44","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2o8s44","author_flair_css_class":null,"num_comments":7,"over_18":false,"saved":false,"score":3,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"created":1417680741,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2o8s44/possible_commercialfriendly_caffe_pretrained_nets/","secure_media_embed":{},"title":"Possible Commercial-friendly Caffe Pre-trained Nets","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417680741","subreddit":"MachineLearning","media_embed":{},"ups":3,"retrieved_on":1441037899,"selftext":"A friend of mine told me that during an Nvidia webinar today (\"DIY Deep Learning for Vision: A Tutorial with Caffe\"), someone mentioned that the pre-trained nets will be released with a new commercial-friendly license. I know quite a few companies that would be pleased with the news, and I assume others would be as well. (:\n\nHas anyone else heard this though or have any news on it?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o8s44/possible_commercialfriendly_caffe_pretrained_nets/"}
{"archived":true,"id":"2o8wbt","gilded":0,"name":"t3_2o8wbt","link_flair_css_class":null,"saved":false,"over_18":false,"num_comments":1,"author_flair_css_class":null,"is_self":false,"author":"paralax77","from":null,"stickied":false,"url":"https://www.youtube.com/watch?v=zPVHH7ZJi9Q","secure_media":{"oembed":{"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FzPVHH7ZJi9Q%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"title":"Data Science @ ESIEE Paris - Yann LeCun","description":"Video of Yann LeCun's talk on June, 12th, 2014 @ ESIEE Paris http://laurentnajman.org/journee_datascience/","thumbnail_width":480,"provider_name":"YouTube","provider_url":"http://www.youtube.com/","height":450,"author_name":"Laurent Najman","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzPVHH7ZJi9Q%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzPVHH7ZJi9Q&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FzPVHH7ZJi9Q%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","author_url":"http://www.youtube.com/channel/UC4EsjBCAAD7Yv0pmdROJYgQ","url":"http://www.youtube.com/watch?v=zPVHH7ZJi9Q","type":"video","thumbnail_height":360},"type":"youtube.com"},"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417685330,"from_id":null,"quarantine":false,"edited":false,"score":2,"domain":"youtube.com","hide_score":false,"media_embed":{"width":600,"height":450,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FzPVHH7ZJi9Q%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzPVHH7ZJi9Q&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FzPVHH7ZJi9Q%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"subreddit":"MachineLearning","created_utc":"1417685330","author_flair_text":null,"title":"Data Science @ ESIEE Paris - Yann LeCun","secure_media_embed":{"scrolling":false,"width":600,"height":450,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzPVHH7ZJi9Q%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzPVHH7ZJi9Q&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FzPVHH7ZJi9Q%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/2eixhLc13qiOoXKiGHPzbovFNdcqPfV7_n334ZIGd1U.jpg","permalink":"/r/MachineLearning/comments/2o8wbt/data_science_esiee_paris_yann_lecun/","link_flair_text":null,"retrieved_on":1441037846,"ups":2,"media":{"oembed":{"author_url":"http://www.youtube.com/channel/UC4EsjBCAAD7Yv0pmdROJYgQ","type":"video","url":"http://www.youtube.com/watch?v=zPVHH7ZJi9Q","thumbnail_height":360,"height":450,"author_name":"Laurent Najman","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FzPVHH7ZJi9Q%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzPVHH7ZJi9Q&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FzPVHH7ZJi9Q%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","provider_url":"http://www.youtube.com/","provider_name":"YouTube","width":600,"thumbnail_url":"http://i.ytimg.com/vi/zPVHH7ZJi9Q/hqdefault.jpg","title":"Data Science @ ESIEE Paris - Yann LeCun","description":"Video of Yann LeCun's talk on June, 12th, 2014 @ ESIEE Paris http://laurentnajman.org/journee_datascience/","thumbnail_width":480},"type":"youtube.com"},"selftext":""}
{"is_self":false,"from":null,"author":"test3545","name":"t3_2o99r2","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2o99r2","author_flair_css_class":null,"num_comments":6,"over_18":false,"saved":false,"score":25,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1417699146,"from_kind":null,"secure_media":null,"url":"https://plus.google.com/100209651993563042175/posts/M5xrJQ3kEKy","secure_media_embed":{},"title":"From G+, Andrej Karpathy: \"I rendered the NIPS 2014 papers in pretty format again.\"","thumbnail":"http://b.thumbs.redditmedia.com/OIDFhu5yAP0JqdfBir1tz42FuTmqaA3ZDbKWO2v3DVs.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"plus.google.com","author_flair_text":null,"created_utc":"1417699146","subreddit":"MachineLearning","media_embed":{},"ups":25,"retrieved_on":1441037671,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2o99r2/from_g_andrej_karpathy_i_rendered_the_nips_2014/"}
{"quarantine":false,"edited":false,"score":0,"stickied":false,"created":1417712709,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2o9vmm/how_to_weka_the_living_daylights_outta_my_data/","secure_media":null,"from_id":null,"is_self":true,"author":"shugakayne","from":null,"id":"2o9vmm","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2o9vmm","over_18":false,"saved":false,"num_comments":1,"author_flair_css_class":null,"retrieved_on":1441037387,"ups":0,"media":null,"selftext":"I must say this is exciting, i absolutely have no foundation in computer science or programming and neither was i very good at mathematics but somehow i am in love with the idea of machine learning, probably because i have a real life scenario i want to experiment with.\n\nI have up to 20 weekends and more of historical data of matches played and i would like to see how weka can predict the outcome of matches played within that 20 week period.\n\nMy data is in tabular form and it is stored in microsoft word.\nIt is a forecast of football matches played in the past.\n\nPattern detection is the key, By poring over historical data of matches played in the past, patterns begin to emerge and i use this to forecast what the outcome of matches will be for the next game.\n\nI use the following attributes for detecting patterns and making predictions which on paper is always 80-100% accurate but when i make a bet, it fails.\n(results, team names, codes, week's color, row number) \n\nResults= Matches that result in DRAWS\n\nTeam names = Believe it or not, teams names are used as parameters to make predictions, HOW? They begin with Alphabets.\n\nCodes= These are 3-4 strings either digits or a combo of letters and digits, depending on where they are strategically placed in the table, they offer insight into detecting patterns.\n\nWeeks Color= In the football forecasting world, there are 4 colours used to represent each week in a month. RED, BLUE, BROWN and PURPLE. These also allows the forecaster to see emerging patterns.\n\nRow Number= Each week, the data is presented in a table form with two competing teams occupying a row and a number is associated with that row. These numbers are used to make preditions. \n\nSo i would like to TEACH WEKA how i detect these patterns so that my task can be automated and tweaked anyhow i like it.\n\nIn plain english, how do i write out my \"pattern detecting style\" for weka to understand and how do i get this information loaded into weka for processing into my desired results.\nGoing by my scenario, What will be my attributes? \nWhat will be my instances? \nWhat will be the claasifiers? \nWhat algorithms do i use to achieve my aim or will i need to write new algorithms?\n\nI sincerely hope someone will come to my rescue.\n\nThanks\n ","permalink":"/r/MachineLearning/comments/2o9vmm/how_to_weka_the_living_daylights_outta_my_data/","link_flair_text":null,"secure_media_embed":{},"title":"How to WEKA the living daylights outta my data with a real life scenario???","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417712709"}
{"ups":1,"retrieved_on":1441037222,"selftext":"A characteristic feature of biological neural networks is that the connections between neurons are directed. Backpropagation as well as Boltzmann machines rely on bidirectional connections that allow information to flow in either direction ~~unidirectional connections~~. In backpropagation the error signal is propagated backwards through the connections to the pre-synaptic neuron, while in Boltzmann machines the weights are assumed to be tied. However, I came across this slide in a talk by Geoffery Hinton, in which he claims that the weights in a Boltzmann machine do not necessarily need to be tied (see Reference). Can anyone refer me to a paper on this? Has someone derived the equations for a Boltzmann machine with purely directed connections?\n\nReference: http://www.cs.toronto.edu/~hinton/backpropincortex2007.pdf#page=12\n\nThanks!","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oa8ev/question_about_neural_networks_with_directed/","secure_media_embed":{},"title":"Question about Neural Networks with directed weights","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417719194","subreddit":"MachineLearning","media_embed":{},"score":1,"edited":1417751228,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417719194,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oa8ev/question_about_neural_networks_with_directed/","is_self":true,"from":null,"author":"jostmey","link_flair_css_class":null,"name":"t3_2oa8ev","gilded":0,"archived":true,"id":"2oa8ev","num_comments":6,"author_flair_css_class":null,"over_18":false,"saved":false}
{"is_self":false,"author":"rscottking73","from":null,"id":"2oaca6","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2oaca6","over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"quarantine":false,"score":0,"edited":false,"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417721078,"secure_media":null,"url":"http://www.randalscottking.com/2014/12/a-dozen-informative-videos-on-data-science/","from_id":null,"secure_media_embed":{},"title":"A Dozen Good Videos on Data Science","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"randalscottking.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417721078","retrieved_on":1441037172,"ups":0,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2oaca6/a_dozen_good_videos_on_data_science/","link_flair_text":null}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oawar/scalable_highquality_object_detection/","selftext":"","media":null,"ups":5,"retrieved_on":1441036853,"created_utc":"1417731022","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"arxiv.org","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Scalable, High-Quality Object Detection","secure_media_embed":{},"from_id":null,"secure_media":null,"url":"http://arxiv.org/abs/1412.1441","subreddit_id":"t5_2r3gv","from_kind":null,"created":1417731022,"stickied":false,"edited":false,"score":5,"quarantine":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"name":"t3_2oawar","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2oawar","from":null,"author":"vkhuc","is_self":false}
{"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Huge drop in quality of neural network when using tanh instead of sigmoid as an activation function?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417735804","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"I'm playing around with neural nets in Haskell, and I've got this code :\n\n    import AI.HNN.FF.Network\n    import Numeric.LinearAlgebra\n\n\n    main :: IO ()\n    main = do\n        net &lt;- createNetwork 2 [5] 1 :: IO (Network Double)\n        print samples\n        let betterNet = trainNTimes 100 0.8 tanh tanh' net samples\n        print (output betterNet tanh (fromList [11,10]))\n        print \"hi\"\n\n\n    samples :: Samples Double\n    samples = [(fromList [a,b], fromList [out])  | a &lt;- [1..25], b &lt;- [1..25], let out = if a &gt; b then 1 else 0]\n\n\nBasically I give it input of the form [a, b], and the output is [c], where c is 1 when a &gt; b, and 0 when a &lt;= b. Using tanh as the activation function gives terrible results, when b &gt;= a, the result is usually around -.9, and when a &gt; b, the result is ~.85, but varies wildly, sometimes as low as 0.3. Then, I just swap out every instance of 'tanh' with 'sigmoid', and suddenly it works flawlessly, giving .9999 when a &gt; b, and something like 3*10^-5 when b &gt;= a.\n\nI really don't have much of an idea what I'm doing, so I'm hoping someone can explain the difference in performance, since everything I've been reading on the difference between the two makes it seem like they're almost interchangeable.","retrieved_on":1441036733,"ups":4,"permalink":"/r/MachineLearning/comments/2ob5lg/huge_drop_in_quality_of_neural_network_when_using/","link_flair_text":null,"author":"mbuffett1","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":5,"archived":true,"id":"2ob5lg","gilded":0,"link_flair_css_class":null,"name":"t3_2ob5lg","quarantine":false,"edited":false,"score":4,"subreddit_id":"t5_2r3gv","created":1417735804,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ob5lg/huge_drop_in_quality_of_neural_network_when_using/","secure_media":null,"from_id":null,"stickied":false}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2obcxe/transforming_autoencoders/","selftext":"","media":null,"ups":7,"retrieved_on":1441036638,"author_flair_text":null,"created_utc":"1417739634","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"cs.toronto.edu","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Transforming Auto-encoders","from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417739634,"secure_media":null,"url":"https://www.cs.toronto.edu/~hinton/absps/transauto6.pdf","stickied":false,"edited":false,"score":7,"quarantine":false,"author_flair_css_class":null,"num_comments":2,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2obcxe","id":"2obcxe","archived":true,"from":null,"author":"madisonmay","is_self":false}
{"from":null,"author":"fawar","is_self":true,"num_comments":8,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2obd8a","id":"2obd8a","archived":true,"edited":false,"score":0,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417739813,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2obd8a/train_network_by_comparing_2_set_of_entries/","stickied":false,"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Train network by comparing 2 set of entries","author_flair_text":null,"created_utc":"1417739813","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"I'm wondering if there is an actual method to do the following :\n\n\nHaving 2 set of Identical features which are not the same training exemple as a single training exemple.\n\nA1,A2...A10, B1,B2....B10. =&gt; A is better/ B is better\n\n(Supervised learning)  would it be possible to train a NN pr anything else that way? \nAnd then use it to. Evaluate a single. Set of feature (once training is done)?","media":null,"ups":0,"retrieved_on":1441036634,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2obd8a/train_network_by_comparing_2_set_of_entries/"}
{"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":12,"archived":true,"id":"2obzgp","name":"t3_2obzgp","gilded":0,"link_flair_css_class":null,"author":"improbabble","from":null,"is_self":false,"url":"http://research.google.com/pubs/pub43146.html","secure_media":null,"created":1417752553,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":60,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417752553","author_flair_text":null,"domain":"research.google.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Machine Learning: The High Interest Credit Card of Technical Debt","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2obzgp/machine_learning_the_high_interest_credit_card_of/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441036345,"ups":60}
{"quarantine":false,"edited":false,"score":1,"stickied":false,"created":1417755065,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oc3gk/id_love_to_read_a_primer_on_breeding_in_machine/","from_id":null,"is_self":true,"author":"[deleted]","from":null,"id":"2oc3gk","archived":true,"name":"t3_2oc3gk","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"retrieved_on":1441036294,"ups":1,"media":null,"selftext":"test 1000 permutations of a program for desired characteristics     \nbreed the two highest scoring programs (into 1000 **randomized** permutations)       \nrepeat      \n\n","permalink":"/r/MachineLearning/comments/2oc3gk/id_love_to_read_a_primer_on_breeding_in_machine/","link_flair_text":null,"secure_media_embed":{},"title":"I'd love to read a primer on \"Breeding\" in machine learning","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417755065"}
{"secure_media_embed":{},"title":"Translating randomForest (R) back into actionable items","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417772150","retrieved_on":1441036032,"ups":2,"media":null,"selftext":"I'm having a hard time translating my random forest back into actionable results. I'm trying to predict a bit flip based on a number of variables using R's excellent randomForest package. Let's say one of my fields is an integer; 'duration' with a value from 0 to 5000. It turns out that in varImpPlot it is by far the best indication of the flip. How do I plot the chance that a bit will be flipped for duration x using the random forest I've made? partialPlot only gets me so far, the axes are basically unlabeled and I can't 'action' that graph. Is there something I'm overlooking?","permalink":"/r/MachineLearning/comments/2ocnnx/translating_randomforest_r_back_into_actionable/","link_flair_text":null,"is_self":true,"author":"kaaswagen","from":null,"id":"2ocnnx","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2ocnnx","over_18":false,"saved":false,"num_comments":1,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":2,"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417772150,"url":"http://www.reddit.com/r/MachineLearning/comments/2ocnnx/translating_randomforest_r_back_into_actionable/","secure_media":null,"from_id":null}
{"hide_score":false,"domain":"infoworld.com","author_flair_text":null,"created_utc":"1417781277","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"11 open source tools to make the most of machine learning","thumbnail":"http://b.thumbs.redditmedia.com/zO8ypH3uBGrFEhTm-iRaRXotV0NpFBjcRFDtpgxoVsg.jpg","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ocvoe/11_open_source_tools_to_make_the_most_of_machine/","ups":0,"retrieved_on":1441035928,"selftext":"","media":null,"gilded":0,"link_flair_css_class":null,"name":"t3_2ocvoe","archived":true,"id":"2ocvoe","author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"futureisdata","stickied":false,"from_id":null,"created":1417781277,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.infoworld.com/article/2853707/machine-learning/11-open-source-tools-machine-learning.html","secure_media":null,"edited":false,"score":0,"quarantine":false}
{"quarantine":false,"edited":false,"score":11,"secure_media":null,"url":"http://lisa.iro.umontreal.ca/mt-demo/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1417783230,"from_id":null,"stickied":false,"author":"benanne","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":6,"archived":true,"id":"2ocxp9","gilded":0,"link_flair_css_class":null,"name":"t3_2ocxp9","media":null,"selftext":"","retrieved_on":1441035902,"ups":11,"permalink":"/r/MachineLearning/comments/2ocxp9/neural_machine_translation_demo/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Neural Machine Translation demo","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417783230","author_flair_text":null,"domain":"lisa.iro.umontreal.ca","hide_score":false}
{"quarantine":false,"edited":false,"score":0,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2od10m/machine_learning_in_matlab/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417786216,"from_id":null,"is_self":true,"author":"[deleted]","from":null,"id":"2od10m","archived":true,"name":"t3_2od10m","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":6,"retrieved_on":1441035859,"ups":0,"media":null,"selftext":"Does anyone here do their machine learning in MATLAB? What do people think about various packages available in matlab? I'm a total beginner and would love to hear people's opinions","permalink":"/r/MachineLearning/comments/2od10m/machine_learning_in_matlab/","link_flair_text":null,"title":"Machine Learning in MATLAB?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417786216","author_flair_text":null}
{"secure_media_embed":{},"title":"Data science in Python: A practical tutorial","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"radimrehurek.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417798335","retrieved_on":1441035607,"ups":86,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2odkgt/data_science_in_python_a_practical_tutorial/","link_flair_text":null,"is_self":false,"author":"piskvorky","from":null,"archived":true,"id":"2odkgt","gilded":0,"name":"t3_2odkgt","link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":20,"author_flair_css_class":null,"quarantine":false,"score":86,"edited":false,"stickied":false,"created":1417798335,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://radimrehurek.com/data_science_python/","secure_media":null,"from_id":null}
{"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417800049,"url":"http://www.reddit.com/r/MachineLearning/comments/2odnq4/question_on_autoassociation_with_multilayer/","secure_media":null,"stickied":false,"edited":false,"score":1,"quarantine":false,"author_flair_css_class":null,"num_comments":7,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2odnq4","gilded":0,"archived":true,"id":"2odnq4","from":null,"author":"fascz","is_self":true,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2odnq4/question_on_autoassociation_with_multilayer/","selftext":"Hi!\n\nI am trying to teach a multilayer perceptron to do an auto-association task but I can't get it to work properly. I have little experience with neural networks so I thought that the solution to my problem could be obvious for you guys. I tried many sets of parameters (size of hidden layer and learning rate) on my relatively simple implementation of the backprop rule, but without success. My inputs are images of black and white fish. The problem that I have is that the cost always seems to plateau after a while, and instead of giving me the same output as my input after the training, I get a kind of prototype fish for all inputs. I tried to lower my learning rate but it doesn't seem to be helping.\n\nI have a sigmoid activation for my hidden and output units. My inputs are all vector with 8064 binary features. My cost function is the following:\n\n    def cost(pred,y,_lambda,w1,w2):\n        m = pred.shape[0]\n        J = np.sum(np.power(pred - y,2))/(2*m)\n    if _lambda != 0:\n        regularization = (_lambda/(2*m)) * (np.sum(np.power(w1[:,1:],2)) + np.sum(np.power(w2[:,1:],2)))\n        J = J + regularization\n    return J   \n\nThank you!","media":null,"ups":1,"retrieved_on":1441035565,"author_flair_text":null,"created_utc":"1417800049","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Question on auto-association with multilayer perceptrons"}
{"is_self":false,"from":null,"author":"notarowboat","gilded":0,"link_flair_css_class":null,"name":"t3_2oe2cn","id":"2oe2cn","archived":true,"author_flair_css_class":null,"num_comments":4,"over_18":false,"saved":false,"edited":false,"score":17,"quarantine":false,"stickied":false,"from_id":null,"created":1417807722,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.wired.com/2014/12/new-startup-sets-bring-google-style-ai-masses","secure_media_embed":{},"title":"Richard Socher's new startup has just been unveiled","thumbnail":"http://b.thumbs.redditmedia.com/I99huHJAvvKW2NPC2hj9jItei4cel3yNc_yEZDfoMkE.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"wired.com","author_flair_text":null,"created_utc":"1417807722","subreddit":"MachineLearning","media_embed":{},"ups":17,"retrieved_on":1441035375,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oe2cn/richard_sochers_new_startup_has_just_been_unveiled/"}
{"author_flair_css_class":null,"num_comments":5,"saved":false,"over_18":false,"name":"t3_2oe7rn","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2oe7rn","from":null,"author":"sayan5678","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oe7rn/homework_trouble_with_nltk_python/","secure_media":null,"created":1417810594,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"edited":1417811558,"score":0,"quarantine":false,"created_utc":"1417810594","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"[homework] trouble with nltk python NaiveBayesClassifier, I keep getting same probabilities","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oe7rn/homework_trouble_with_nltk_python/","selftext":"Hey /r/machinelearning--\n\n I don't see too many [homework] posts here, so I hope I'm not in the wrong sub. If so, please point me to a better option.\n\n so I'm working on a project in which i take in anime names and genres and if they are relevant or irrelevant \nI am trying to build a NaiveBayesClassifier with that and then I want to pass in genres and for it to tell me if it is relevant or irrelevant\nI currently have the following:\n\n    import nltk\n    trainingdata =[({'drama': True, 'mystery': True, 'horror': True, 'psychological': True}, 'relevant'), ({'drama': True, 'fantasy': True, 'romance': True, 'adventure': True, 'science fiction': True}, 'unrelevant')]\n    classifier = nltk.classify.naivebayes.NaiveBayesClassifier.train(trainingdata)\n    classifier.classify({'Fantasy': True, 'Comedy': True, 'Supernatural': True})\n    prob_dist = classifier.prob_classify(anime)\n    print \"relevant \" + str(prob_dist.prob(\"relevant\"))\n    print \"unrelevant \" + str(prob_dist.prob(\"unrelevant\"))\n\nI currently have :\n    \n    size of training array:110\n    the relevant length 57\n    the unrelevant length 53\n\nSome results I receive :\n\n    relevant Tantei Opera Milky Holmes TD\n    {'Mystery': True, 'Comedy': True, 'Super': True, 'Power': True}\n    relevant 0.518018018018\n    unrelevant 0.481981981982\n\n    relevant Juuou Mujin no Fafnir\n    {'Romance': True, 'Fantasy': True, 'School': True}\n    relevant 0.518018018018\n    unrelevant 0.481981981982\n\nI was wondering if that makes sense... Since I am getting the same relevant probability for each classification it makes.. \nFrom my understanding of Naive Bayes it shouldn't be doing that... \n\nusing python, nltk\nThanks!","media":null,"ups":0,"retrieved_on":1441035305}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oe98q/help_with_generative_adversarial_networks/","ups":3,"retrieved_on":1441035286,"selftext":"I just finished implementing Generative Adversarial Networks.  There are a few things that I'm confused about.  \n\n1.  In my experiments, training diverges if I use any type of distribution where regions with no density touch regions with high density.  An extreme example of this is discrete distributions where all of the mass is concentrated at single points.  \n\nI think that this happens because there's a strong penalty for the generator assigning probability density to places where there shouldn't be any mass.  \n\nIn the paper, the authors consider two different losses for the generator:  \n\na: -1.0 * log(D(G(z)))\n\nb: log(1.0 - D(G(z)))\n\nAs D(G(z)) -&gt; 0.0, the first loss approaches infinity and the second loss approaches 0.0.  \n\nAs D(G(z)) -&gt; 1.0, the first loss approaches 0.0 and the second loss approaches negative infinity.  \n\nI think that the second loss is much better if the true distribution has discontinuities, because it allows the network to put some amount of mass in places that shouldn't have any mass without facing a severe penalty.  I did a simple experiment where I trained the model to reproduce a mixture of a normal distribution and a gamma distribution.  The gamma distribution has a discontinuity which the first loss has a hard time handling: \n\nLoss a: \n\nhttp://imgur.com/bo7TYfJ\n\nLoss b: \n\nhttp://imgur.com/MoIZIx4\n\nIn both cases green is the true distribution, blue is the generated distributed, and the dots are D(G(z)) from the discriminator.  As you can see, the second loss performs a lot better.  Nonetheless, the paper advocates for the use of the first loss in section 3.  \n\nPerhaps there's another loss for the generator that gets the benefits of both?  \n\n2.  I've found that the generator is a lot harder to optimize than the discriminator.  \n\n3.  The experimental results in the paper are for real data.  Has anyone done experiments showing that the model is calibrated for synthetic data?  ","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417811374","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Help with Generative Adversarial Networks","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oe98q/help_with_generative_adversarial_networks/","from_kind":null,"created":1417811374,"subreddit_id":"t5_2r3gv","score":3,"edited":false,"quarantine":false,"name":"t3_2oe98q","gilded":0,"link_flair_css_class":null,"id":"2oe98q","archived":true,"num_comments":8,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"alexmlamb"}
{"archived":true,"id":"2oeawh","gilded":0,"name":"t3_2oeawh","link_flair_css_class":null,"saved":false,"over_18":false,"num_comments":8,"author_flair_css_class":null,"is_self":true,"author":"bthrill","from":null,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2oeawh/listing_kaggle_projects_on_resume/","secure_media":null,"from_kind":null,"created":1417812211,"subreddit_id":"t5_2r3gv","from_id":null,"quarantine":false,"edited":1417813872,"score":1,"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417812211","author_flair_text":null,"title":"Listing Kaggle projects on resume","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","permalink":"/r/MachineLearning/comments/2oeawh/listing_kaggle_projects_on_resume/","link_flair_text":null,"retrieved_on":1441035264,"ups":1,"media":null,"selftext":"I just completed a few tutorials on analyzing the Kaggle Titanic dataset using R and Python from various websites. Should I list these completed submissions on my resume?"}
{"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/vyI_ego45cjaO4bjIkCqG3dAHrec1ZOj2t9t7YwOcfg.jpg","downs":0,"secure_media_embed":{"scrolling":false,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FGlcnxUlrtek%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DGlcnxUlrtek&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FGlcnxUlrtek%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":338},"title":"Backpropagation as simple as possible, but no simpler","subreddit":"MachineLearning","media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FGlcnxUlrtek%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DGlcnxUlrtek&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FGlcnxUlrtek%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"author_flair_text":null,"created_utc":"1417815002","hide_score":false,"domain":"youtube.com","media":{"oembed":{"version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FGlcnxUlrtek%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DGlcnxUlrtek&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FGlcnxUlrtek%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"author_name":"Stephen Welch","thumbnail_height":360,"author_url":"http://www.youtube.com/user/Taylorns34","type":"video","url":"http://www.youtube.com/watch?v=GlcnxUlrtek","description":"Backpropagation as simple as possible, but no simpler. Perhaps the most misunderstood part of neural networks, Backpropagation of errors is the key step that allows ANNs to learn. In this video, I give the derivation and thought processes behind backpropagation using high school level calculus.","thumbnail_width":480,"width":600,"thumbnail_url":"http://i.ytimg.com/vi/GlcnxUlrtek/hqdefault.jpg","title":"Neural Networks Demystified [Part 4: Backpropagation]","provider_url":"http://www.youtube.com/","provider_name":"YouTube"},"type":"youtube.com"},"selftext":"","retrieved_on":1441035196,"ups":24,"permalink":"/r/MachineLearning/comments/2oeg5t/backpropagation_as_simple_as_possible_but_no/","link_flair_text":null,"author":"stephencwelch","from":null,"is_self":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":15,"id":"2oeg5t","archived":true,"name":"t3_2oeg5t","link_flair_css_class":null,"gilded":0,"quarantine":false,"edited":false,"score":24,"created":1417815002,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":{"type":"youtube.com","oembed":{"provider_name":"YouTube","provider_url":"http://www.youtube.com/","thumbnail_width":480,"description":"Backpropagation as simple as possible, but no simpler. Perhaps the most misunderstood part of neural networks, Backpropagation of errors is the key step that allows ANNs to learn. In this video, I give the derivation and thought processes behind backpropagation using high school level calculus.","title":"Neural Networks Demystified [Part 4: Backpropagation]","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FGlcnxUlrtek%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","thumbnail_height":360,"url":"http://www.youtube.com/watch?v=GlcnxUlrtek","type":"video","author_url":"http://www.youtube.com/user/Taylorns34","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FGlcnxUlrtek%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DGlcnxUlrtek&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FGlcnxUlrtek%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"Stephen Welch","height":338}},"url":"https://www.youtube.com/watch?v=GlcnxUlrtek","from_id":null,"stickied":false}
{"author_flair_text":null,"created_utc":"1417819013","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"on-demand-gtc.gputechconf.com","thumbnail":"http://b.thumbs.redditmedia.com/VeYqsFQBPIW0T7SmrbIXKo5MDJyp8KoZynzMtQzGu3Y.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"DIY Deep Learning for Vision: A Tutorial with Caffe","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oensf/diy_deep_learning_for_vision_a_tutorial_with_caffe/","selftext":"","media":null,"ups":4,"retrieved_on":1441035097,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2oensf","archived":true,"id":"2oensf","from":null,"author":"CompleteSkeptic","is_self":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1417819013,"from_kind":null,"secure_media":null,"url":"http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+","stickied":false,"score":4,"edited":false,"quarantine":false}
{"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"id":"2oesw5","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2oesw5","author":"MLBlogTeam","from":null,"is_self":false,"subreddit_id":"t5_2r3gv","created":1417821779,"from_kind":null,"secure_media":null,"url":"http://aka.ms/w506wc","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417821779","hide_score":false,"domain":"aka.ms","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"Weekend reading - 3 recent stories about Microsoft Machine Learning and Advanced Analytics","permalink":"/r/MachineLearning/comments/2oesw5/weekend_reading_3_recent_stories_about_microsoft/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441035032,"ups":0}
{"stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1417840817,"url":"http://www.reddit.com/r/MachineLearning/comments/2ofncf/what_do_you_want_to_hear_from_a_prospect_how_to/","secure_media":null,"edited":false,"score":2,"quarantine":false,"gilded":0,"name":"t3_2ofncf","link_flair_css_class":null,"id":"2ofncf","archived":true,"num_comments":12,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"poundcakejumpsuit","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ofncf/what_do_you_want_to_hear_from_a_prospect_how_to/","ups":2,"retrieved_on":1441034636,"selftext":"I got a technical interview at a machine learning company, and have no idea how to prepare. I'm skilled at C++ and MATLAB, and I've fooled around in Python, but is this gonna be your standard \"balance this node, explain a hash table\" interview? I have a little experience in machine learning but expect to be picking up skills on the fly.","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417840817","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"What do you want to hear from a prospect? How to interview for machine learning?","thumbnail":"self","downs":0,"distinguished":null}
{"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417861522","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Can machine learning be used to translate complicated concepts into simple terms?","permalink":"/r/MachineLearning/comments/2og95b/can_machine_learning_be_used_to_translate/","link_flair_text":null,"media":null,"selftext":"For example lets say I typed the word trabecilae would machine learning be used to retrieve pictures of the eiffel tower and simplify the physics?","retrieved_on":1441034354,"ups":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"archived":true,"id":"2og95b","gilded":0,"name":"t3_2og95b","link_flair_css_class":null,"author":"uptnapishtim","from":null,"is_self":true,"created":1417861522,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2og95b/can_machine_learning_be_used_to_translate/","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417883657,"url":"http://www.reddit.com/r/MachineLearning/comments/2ogvut/namely_nyc_is_looking_for_a_data_science_intern/","secure_media":null,"score":0,"edited":false,"quarantine":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2ogvut","id":"2ogvut","archived":true,"num_comments":14,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"manueslapera","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ogvut/namely_nyc_is_looking_for_a_data_science_intern/","ups":0,"retrieved_on":1441034059,"selftext":"Come! It will be fun :)\n\nhttp://namely.workable.com/jobs/27793","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417883657","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Namely [NYC] is looking for a Data Science Intern","thumbnail":"self","downs":0,"distinguished":null}
{"title":"Experts divided on Stephen Hawking\u2019s claim that Artificial Intelligence could end Humanity","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/gk6jRQknqbs3XDGZKqI2BjlEizTz0RnOU3I_gl7k6m8.jpg","domain":"zapmylife.ca","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417889869","author_flair_text":null,"retrieved_on":1441033929,"ups":0,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2oh5wg/experts_divided_on_stephen_hawkings_claim_that/","link_flair_text":null,"is_self":false,"author":"sheriner","from":null,"id":"2oh5wg","archived":true,"name":"t3_2oh5wg","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"num_comments":10,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":0,"stickied":false,"url":"http://zapmylife.ca/experts-divided-on-stephen-hawkings-claim-that-artificial-intelligence-could-end-humanity/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417889869,"from_id":null}
{"is_self":true,"from":null,"author":"bubaonaruba","gilded":0,"link_flair_css_class":null,"name":"t3_2ohdq2","archived":true,"id":"2ohdq2","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"score":0,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1417894372,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ohdq2/linear_combination_of_weak_estimators_over_fuzzy/","secure_media_embed":{},"title":"Linear combination of weak estimators over fuzzy classifiers?","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417894372","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1441033828,"selftext":"Having:\n\n* set of soft fuzzy classifiers (classification onto overlapping sets) C_i(x) -&gt; [0,1]\n* corresponding set of weak estimators R_i(z) of the form R_i(z) = EX(y|z).\n\n\nThe estimators R_i are just some kind of regression, kalman or particle filters. The classifiers C_i are fixed and static.\n\nHow to make a strong estimator out of a weighted combination of the form L(x, z) = SUM_i: C_i(x)*R_i(z)*Q_i ?\n\n\nIn other words how to choose the weights Q_i?\n\n\nIs there some kind of online approach to this problem?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ohdq2/linear_combination_of_weak_estimators_over_fuzzy/"}
{"created_utc":"1417894841","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"databricks.com","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/upS_00a5xVe48v-64Q4q67tjPEDuotuZu0AjEMcXlTI.jpg","distinguished":null,"title":"Databricks to run free Scalable Machine Learning MOOC","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ohei0/databricks_to_run_free_scalable_machine_learning/","selftext":"","media":null,"ups":54,"retrieved_on":1441033818,"num_comments":8,"author_flair_css_class":null,"saved":false,"over_18":false,"gilded":0,"name":"t3_2ohei0","link_flair_css_class":null,"id":"2ohei0","archived":true,"from":null,"author":"edXbecky","is_self":false,"from_id":null,"secure_media":null,"url":"http://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html","created":1417894841,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"score":54,"edited":false,"quarantine":false}
{"media":null,"selftext":"Why should I choose to fully link?\n\nInput to first Hidden layer?\nHidden Layer to Hidden Layer?\nHidden Layer to ouput?\n\nIf I don't fully link those, what are my options and how do I choose it?\n\nSince the knowledge in those units is \"hidden\", machine representation, how should I expect to know which linkage would fit the most? \n\nI'm asking because, I normally see Fully linked everywhere, but Convolutional just use subsets\n","retrieved_on":1441033692,"ups":0,"permalink":"/r/MachineLearning/comments/2oho7j/how_to_choose_linkage_in_hidden_layers_fully/","link_flair_text":null,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"How to choose linkage in hidden layers (Fully, cluster .. etc)","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417900246","hide_score":false,"domain":"self.MachineLearning","quarantine":false,"score":0,"edited":false,"from_kind":null,"created":1417900246,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oho7j/how_to_choose_linkage_in_hidden_layers_fully/","from_id":null,"stickied":false,"author":"fawar","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":7,"author_flair_css_class":null,"archived":true,"id":"2oho7j","name":"t3_2oho7j","gilded":0,"link_flair_css_class":null}
{"permalink":"/r/MachineLearning/comments/2ohoe1/neural_representation_of_language_learning_tom/","link_flair_text":null,"retrieved_on":1441033690,"ups":21,"media":{"type":"youtube.com","oembed":{"version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FC5WzCRBfObs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DC5WzCRBfObs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FC5WzCRBfObs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"author_name":"Computational Social Science Laboratory USC","thumbnail_height":360,"author_url":"http://www.youtube.com/channel/UC1GyZUz73suKSnuMWu1EfTQ","type":"video","url":"http://www.youtube.com/watch?v=C5WzCRBfObs","description":"Big Data and Human Behavior Speaker Series at USC, Cammilleri Hall, organized by the Computational Social Science Laboratory (http://dornsife.usc.edu/labs/cssl) Talk 2: Tom Mitchell - Carnegie Mellon University - Dec 3, 2014 Neural Representation of Language Learning How does the human brain use neural activity to create and represent meanings of words, sentences and stories?One way to study this question is to have people read text while scanning their brain, then develop machine learning methods to discover the mapping between language features and observed neural activity.","thumbnail_width":480,"width":600,"thumbnail_url":"http://i.ytimg.com/vi/C5WzCRBfObs/hqdefault.jpg","title":"Neural Representation of Language Learning - Tom Mitchell","provider_name":"YouTube","provider_url":"http://www.youtube.com/"}},"selftext":"","hide_score":false,"domain":"youtube.com","subreddit":"MachineLearning","media_embed":{"scrolling":false,"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FC5WzCRBfObs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DC5WzCRBfObs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FC5WzCRBfObs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"author_flair_text":null,"created_utc":"1417900352","secure_media_embed":{"scrolling":false,"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FC5WzCRBfObs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DC5WzCRBfObs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FC5WzCRBfObs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"title":"Neural Representation of Language Learning - Tom Mitchell","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/D33a7xds_ehrvq4sCjbpR_ynioQIaruGysBVuZ1OTdg.jpg","downs":0,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417900352,"secure_media":{"type":"youtube.com","oembed":{"thumbnail_width":480,"description":"Big Data and Human Behavior Speaker Series at USC, Cammilleri Hall, organized by the Computational Social Science Laboratory (http://dornsife.usc.edu/labs/cssl) Talk 2: Tom Mitchell - Carnegie Mellon University - Dec 3, 2014 Neural Representation of Language Learning How does the human brain use neural activity to create and represent meanings of words, sentences and stories?One way to study this question is to have people read text while scanning their brain, then develop machine learning methods to discover the mapping between language features and observed neural activity.","title":"Neural Representation of Language Learning - Tom Mitchell","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FC5WzCRBfObs%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","provider_url":"http://www.youtube.com/","provider_name":"YouTube","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FC5WzCRBfObs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DC5WzCRBfObs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FC5WzCRBfObs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"Computational Social Science Laboratory USC","height":338,"thumbnail_height":360,"type":"video","url":"http://www.youtube.com/watch?v=C5WzCRBfObs","author_url":"http://www.youtube.com/channel/UC1GyZUz73suKSnuMWu1EfTQ"}},"url":"https://www.youtube.com/watch?v=C5WzCRBfObs","from_id":null,"quarantine":false,"edited":false,"score":21,"id":"2ohoe1","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2ohoe1","over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"is_self":false,"author":"alexleavitt","from":null}
{"created_utc":"1417930060","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"beyondthebox.aibs.org","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/kW8JENdXaOkaRozeikS8sUGD3Em8oOEwVGsVjd7qn3A.jpg","distinguished":null,"title":"$1M prize for digitization and identification of insect collections","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oiznd/1m_prize_for_digitization_and_identification_of/","selftext":"","media":null,"ups":9,"retrieved_on":1441033017,"num_comments":13,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2oiznd","gilded":0,"archived":true,"id":"2oiznd","from":null,"author":"aihardin","is_self":false,"from_id":null,"secure_media":null,"url":"https://beyondthebox.aibs.org","created":1417930060,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"score":9,"edited":false,"quarantine":false}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1417931398,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oj1jb/what_are_the_benefits_and_disadvantages_to_lasso/","secure_media":null,"edited":false,"score":0,"quarantine":false,"name":"t3_2oj1jb","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2oj1jb","author_flair_css_class":null,"num_comments":3,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"TrashQuestion","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oj1jb/what_are_the_benefits_and_disadvantages_to_lasso/","ups":0,"retrieved_on":1441032993,"selftext":"I am implementing these four regularization techniques for linear regression of stock data in MATLAB but i noticed elastic net is just the sum of Ridge and Lasso, and i dont full understand how exactly Non Negative Garrotte Works as a regularization technique.\n\nHow does Garrotte work and why wouldnt you just always use elastic net over lasso and ridge? (Aside from computation complexity)\n","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417931398","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"What are the benefits and disadvantages to Lasso, Ridge, Elastic Net, and Non Negative Garrotte Regularization techniques?","thumbnail":"self","downs":0,"distinguished":null}
{"quarantine":false,"edited":false,"score":0,"url":"http://www.reddit.com/r/MachineLearning/comments/2oj47j/questiontraining_rbf_with_lms/","secure_media":null,"created":1417933399,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"[deleted]","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"archived":true,"id":"2oj47j","gilded":0,"name":"t3_2oj47j","link_flair_css_class":null,"media":null,"selftext":"I am training my RBF with LMS and wanted to know if I am on the right track. Are my weights updated with the product of the learning rate parameter, the error between the desired and actual response, and the output of the respective activation function? I'm unsure if I'm getting the right values. How would you plot a learning curve for this? It's MSE vs training points used from what I understand. If having trained with n number of epochs, is my MSE at that point based on the error within epoch n or based on epoch n and all the epochs beforehand?\n\nI've been debugging my code all day and have probably confused myself in the process. Any insight will be much appreciated!","retrieved_on":1441032958,"ups":0,"permalink":"/r/MachineLearning/comments/2oj47j/questiontraining_rbf_with_lms/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"[Question]Training RBF with LMS","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417933399","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false}
{"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ojain/best_applied_ml_course_tutorial/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1417939012,"score":0,"edited":false,"quarantine":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2ojain","id":"2ojain","archived":true,"num_comments":4,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"benkitty","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ojain/best_applied_ml_course_tutorial/","ups":0,"retrieved_on":1441032876,"selftext":"I'd like to learn more modern machine learning techniques in the context of an actual data set and code base. For example, there are theoretical books on convolutional neural networks, but I don't want to hunt down small/medium training data sets that I can try in under a week. I'm mostly language agnostic, but it would be nice if the language was Java-like, Python-like, or R-like.\n\nBasically I'm looking for is a collection of \"Hello World\" programs demonstrating modern neural nets, SVMs, and other classification algos.","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1417939012","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Best Applied ML Course / Tutorial?","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null}
{"media":null,"selftext":"What algorithms do other people use for hyperparameter optimisation and how long do you spend on it?\n\nI quite like random sampling of hyperparams as it is so simple to implement and will typically get a good spread of samples.\n\nWith Gaussian Process I find it will often beat random samples; but they can be sensitive to parameters being specified in the wrong space (e.g. learning rate vs. log(learning rate)), among other difficulties.\n\nI haven't tried TPE (http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) but it looks respectable.\n \nI haven't experimented much with manual tuning.\n\nI am currently working on a problem where my models just aren't training nicely and would be grateful to hear any advice or opinions.","retrieved_on":1441032762,"ups":1,"permalink":"/r/MachineLearning/comments/2ojjbr/hyperparameter_optimization_routines/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"self","title":"Hyperparameter Optimization Routines","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417949683","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"quarantine":false,"score":1,"edited":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2ojjbr/hyperparameter_optimization_routines/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417949683,"from_id":null,"stickied":false,"author":"ml_man","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":7,"author_flair_css_class":null,"archived":true,"id":"2ojjbr","name":"t3_2ojjbr","link_flair_css_class":null,"gilded":0}
{"is_self":false,"from":null,"author":"[deleted]","gilded":0,"link_flair_css_class":null,"name":"t3_2ojp01","id":"2ojp01","archived":true,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"edited":false,"score":3,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"created":1417956756,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://arxiv.org/abs/1410.3831","secure_media_embed":{},"title":"An exact mapping between the Variational Renormalization Group and Deep Learning","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"arxiv.org","author_flair_text":null,"created_utc":"1417956756","subreddit":"MachineLearning","media_embed":{},"ups":3,"retrieved_on":1441032689,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ojp01/an_exact_mapping_between_the_variational/"}
{"stickied":false,"secure_media":null,"url":"http://stackoverflow.com/questions/27343379/rnn-in-generative-mode-with-theano-scan-op","subreddit_id":"t5_2r3gv","created":1417960724,"from_kind":null,"from_id":null,"quarantine":false,"edited":false,"score":2,"id":"2ojsrq","archived":true,"link_flair_css_class":null,"name":"t3_2ojsrq","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":4,"is_self":false,"author":"TheAlienDude","from":null,"permalink":"/r/MachineLearning/comments/2ojsrq/rnn_in_generative_mode_with_theano_scan_op/","link_flair_text":null,"retrieved_on":1441032640,"ups":2,"media":null,"selftext":"","domain":"stackoverflow.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417960724","author_flair_text":null,"title":"RNN in generative mode with Theano (Scan op)","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/aXYrRkeCvV2XLxkjJ0IlzqDQKfJmSxIod_GIKvDSNl0.jpg"}
{"quarantine":false,"score":38,"edited":false,"stickied":false,"created":1417961348,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"https://www.youtube.com/watch?v=xx310zM3tLs","secure_media":{"oembed":{"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fxx310zM3tLs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dxx310zM3tLs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Fxx310zM3tLs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"TEDx Talks","height":338,"thumbnail_height":360,"url":"http://www.youtube.com/watch?v=xx310zM3tLs","type":"video","author_url":"http://www.youtube.com/user/TEDxTalks","thumbnail_width":480,"description":"This talk was given at a local TEDx event, produced independently of the TED Conferences. The extraordinary, wonderful, and terrifying implications of computers that can learn Jeremy is the CEO of Enlitic, which uses recent advances in machine learning to make medical diagnostics faster, more accurate, and more accessible.","title":"The wonderful and terrifying implications of computers that can learn | Jeremy Howard | TEDxBrussels","thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2Fxx310zM3tLs%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"provider_name":"YouTube","provider_url":"http://www.youtube.com/"},"type":"youtube.com"},"from_id":null,"is_self":false,"author":"cybrbeast","from":null,"archived":true,"id":"2ojtfd","gilded":0,"name":"t3_2ojtfd","link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":26,"retrieved_on":1441032631,"ups":38,"media":{"type":"youtube.com","oembed":{"author_name":"TEDx Talks","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Fxx310zM3tLs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dxx310zM3tLs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Fxx310zM3tLs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=xx310zM3tLs","type":"video","author_url":"http://www.youtube.com/user/TEDxTalks","thumbnail_height":360,"title":"The wonderful and terrifying implications of computers that can learn | Jeremy Howard | TEDxBrussels","width":600,"thumbnail_url":"http://i.ytimg.com/vi/xx310zM3tLs/hqdefault.jpg","thumbnail_width":480,"description":"This talk was given at a local TEDx event, produced independently of the TED Conferences. The extraordinary, wonderful, and terrifying implications of computers that can learn Jeremy is the CEO of Enlitic, which uses recent advances in machine learning to make medical diagnostics faster, more accurate, and more accessible.","provider_name":"YouTube","provider_url":"http://www.youtube.com/"}},"selftext":"","permalink":"/r/MachineLearning/comments/2ojtfd/jeremy_howard_the_wonderful_and_terrifying/","link_flair_text":null,"secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fxx310zM3tLs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dxx310zM3tLs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Fxx310zM3tLs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"title":"Jeremy Howard - The wonderful and terrifying implications of computers that can learn","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/37sWSxAwIBcAzZlkSBG3HKNl8xhKzVthEOD7cpWrUpA.jpg","downs":0,"hide_score":false,"domain":"youtube.com","subreddit":"MachineLearning","media_embed":{"scrolling":false,"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Fxx310zM3tLs%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dxx310zM3tLs&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Fxx310zM3tLs%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"author_flair_text":null,"created_utc":"1417961348"}
{"media":null,"selftext":"","retrieved_on":1441032603,"ups":0,"permalink":"/r/MachineLearning/comments/2ojvmu/stephen_hawking_right_about_dangers_of_ai_but_for/","link_flair_text":null,"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/GQwh1yModPtbOlVnf19doXPYk4j_c1FK7izlvFqR7Rw.jpg","downs":0,"secure_media_embed":{},"title":"Stephen Hawking right about dangers of AI... but for the wrong reasons, says eminent computer expert","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417963308","hide_score":false,"domain":"independent.co.uk","quarantine":false,"score":0,"edited":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417963308,"secure_media":null,"url":"http://www.independent.co.uk/news/science/stephen-hawking-right-about-dangers-of-ai-but-for-the-wrong-reasons-says-eminent-computer-expert-9908450.html","from_id":null,"stickied":false,"author":"netkrow","from":null,"is_self":false,"over_18":false,"saved":false,"num_comments":6,"author_flair_css_class":null,"archived":true,"id":"2ojvmu","link_flair_css_class":null,"gilded":0,"name":"t3_2ojvmu"}
{"is_self":true,"from":null,"author":"Chobeat","link_flair_css_class":null,"gilded":0,"name":"t3_2oksyb","archived":true,"id":"2oksyb","num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"created":1417983774,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oksyb/noobquestion_how_to_calculate_the_angle_between/","secure_media":null,"secure_media_embed":{},"title":"[NoobQuestion] How to calculate the angle between an SVM solution with kernel and a vector?","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1417983774","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1441032171,"selftext":"As the title say i have my svm solution with kernel in the form of a few vector of lagrangian variables (alpha,gamma,delta) specific for the svm i'm working with, i have a vector (generated by a regressor but that's not important) and i need to calculate the angle between them.\n\nObviously i don't have the explicit vector of weights because i would need to know the mapping phi(x) but i don't know it due to the kernel trick. \n\nHow should i proceed?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oksyb/noobquestion_how_to_calculate_the_angle_between/"}
{"ups":1,"retrieved_on":1441032140,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2okvcs/lightlda_big_topic_models_on_modest_compute/","secure_media_embed":{},"title":"LightLDA: Big Topic Models on Modest Compute Clusters","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"arxiv.org","author_flair_text":null,"created_utc":"1417985011","subreddit":"MachineLearning","media_embed":{},"edited":false,"score":1,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1417985011,"url":"http://arxiv.org/abs/1412.1576","secure_media":null,"is_self":false,"from":null,"author":"[deleted]","link_flair_css_class":null,"gilded":0,"name":"t3_2okvcs","archived":true,"id":"2okvcs","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false}
{"permalink":"/r/MachineLearning/comments/2okwij/implement_naive_bayes_from_scratch_in_python/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441032125,"ups":26,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1417985642","hide_score":false,"domain":"machinelearningmastery.com","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/vEOpC7K_4WIB44tTy8dmjjiU54N_YiLtA7Z1llo1pXU.jpg","downs":0,"secure_media_embed":{},"title":"Implement Naive Bayes From Scratch in Python","from_kind":null,"created":1417985642,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":26,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2okwij","gilded":0,"link_flair_css_class":null,"name":"t3_2okwij","author":"jasonb","from":null,"is_self":false}
{"url":"http://www.reddit.com/r/MachineLearning/comments/2olaym/why_isnt_there_a_machine_learning_stack_exchange/","secure_media":null,"created":1417993459,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":2,"saved":false,"over_18":false,"num_comments":5,"author_flair_css_class":null,"archived":true,"id":"2olaym","gilded":0,"link_flair_css_class":null,"name":"t3_2olaym","author":"[deleted]","from":null,"is_self":true,"permalink":"/r/MachineLearning/comments/2olaym/why_isnt_there_a_machine_learning_stack_exchange/","link_flair_text":null,"media":null,"selftext":"It seems like there's enough interest (even technical interest) in the field to warrant one.","retrieved_on":1441031937,"ups":2,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1417993459","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Why isn't there a \"Machine Learning\" Stack Exchange site?","secure_media_embed":{}}
{"permalink":"/r/MachineLearning/comments/2ols0y/finding_good_parameters_and_interpreting_the/","link_flair_text":null,"retrieved_on":1441031716,"ups":1,"media":null,"selftext":"I have a data set that I'm playing with, but I'm not sure if there is a pattern to be found so I'm being really hesitant with my results. As such I also don't have a good feel for tweaking my parameters so I decided to generate a product of parameters, calculate an error rate against the test data and add the abs sum of values across the training set and see what kind of parameters give me a good fit to my error rate.\n\nSo far it appears that as my [iterations](http://pastebin.com/RSx7wU7D) go up the over fitting gets really bad and the best configurations tend to land pretty close to the defaults.\n\nHere are all the parameters that I am calculating a product for:\n\n        l_l1_ratio = (0, 0.05, 0.1, 0.15, 0.25, 0.5, 0.75, 1)\n        l_penalty = ('l1', 'l2', 'elasticnet')\n        l_alpha = (0.00001, 0.0001, 0.001, 0.01, 0.1)\n        l_loss = ('squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive')\n        l_n_iter = (5, 50, 500, 5000)\n        l_eta0 = (0.01, 0.001, 0.0001) # 0.1 crashes the fit!\n\n\n\nI'm using [sklearn.linear_model.SGDRegressor](http://scikit-learn.sourceforge.net/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)\n\nI also have plotted a scatter of expected values X against actual values Y then in a second graph performed a fill of expected against actual. [1752 error](http://imgur.com/7oeubki) vs [15360 error](http://imgur.com/6iC9dNz).\n\nThere are 30 attributes and ~400k samples broken up 80/20 but after rejecting bad data the useable rows end up being around ~40k.\n\nA low score seems to be highly accurate but higher scores tend to jump around quite a bit. But my main question is about strange gap centered around 0,0 are these caused by NaN rows or are they artifacts of the SGDRegressor and am I correct in assuming that high error rates with large n_iter counts are a sign of strong over fitting?","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418004156","secure_media_embed":{},"title":"Finding good parameters and interpreting the results (in python)","distinguished":null,"thumbnail":"self","downs":0,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418004156,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ols0y/finding_good_parameters_and_interpreting_the/","secure_media":null,"from_id":null,"quarantine":false,"score":1,"edited":false,"id":"2ols0y","archived":true,"gilded":0,"name":"t3_2ols0y","link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":5,"is_self":true,"author":"Azsu","from":null}
{"stickied":false,"from_id":null,"from_kind":null,"created":1418016726,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://static.googleusercontent.com/media/research.google.com/en/us/people/jeff/CIKM-keynote-Nov2014.pdf","edited":false,"score":12,"quarantine":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2omcre","id":"2omcre","archived":true,"num_comments":3,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"benkitty","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2omcre/large_scale_deep_learning_jeff_dean/","ups":12,"retrieved_on":1441031448,"selftext":"","media":null,"hide_score":false,"domain":"static.googleusercontent.com","author_flair_text":null,"created_utc":"1418016726","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Large Scale Deep Learning \u2013 Jeff Dean","thumbnail":"default","downs":0,"distinguished":null}
{"quarantine":false,"score":1,"edited":false,"stickied":false,"from_kind":null,"created":1418017453,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2omdt8/looking_for_a_good_book_on_nonparametric_methods/","from_id":null,"is_self":true,"author":"[deleted]","from":null,"id":"2omdt8","archived":true,"name":"t3_2omdt8","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"retrieved_on":1441031434,"ups":1,"media":null,"selftext":"Just wondering if anyone here had come across any good resources for this sort of stuff at an intermediate level. I couldn't see any in the subreddit FAQ, but maybe there just aren't a great deal of textbooks that are completely dedicated to the topic. Any help would be much appreciated.\n\ne: Also the more application-centred the better, but this is not a major concern.","permalink":"/r/MachineLearning/comments/2omdt8/looking_for_a_good_book_on_nonparametric_methods/","link_flair_text":null,"secure_media_embed":{},"title":"Looking for a good book on nonparametric methods for machine learning.","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418017453"}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Please suggest resources for learning about Generalized Linear models.","author_flair_text":null,"created_utc":"1418020275","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"The best youtube videos, courses, books, articles etc. Could you please suggest a few. I am overwhelmed on where to start.","media":null,"ups":2,"retrieved_on":1441031383,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2omhql/please_suggest_resources_for_learning_about/","from":null,"author":"[deleted]","is_self":true,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2omhql","link_flair_css_class":null,"gilded":0,"id":"2omhql","archived":true,"score":2,"edited":false,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418020275,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2omhql/please_suggest_resources_for_learning_about/","stickied":false}
{"domain":"slideshare.net","hide_score":false,"created_utc":"1418028601","author_flair_text":null,"media_embed":{"width":425,"height":355,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.slideshare.net%2Fslideshow%2Fembed_code%2F42347616&amp;src_secure=1&amp;url=http%3A%2F%2Fwww.slideshare.net%2Fpolykraftmachines%2Fdifferent-types-of-radial-drilling-machine-from-polykraftmachinescom&amp;image=http%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdifferenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg%3Fcb%3D1417694915&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=slideshare\" width=\"425\" height=\"355\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"subreddit":"MachineLearning","title":"Different types of Radial Drilling Machine from polykraftmachines.com","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.slideshare.net%2Fslideshow%2Fembed_code%2F42347616&amp;url=http%3A%2F%2Fwww.slideshare.net%2Fpolykraftmachines%2Fdifferent-types-of-radial-drilling-machine-from-polykraftmachinescom&amp;image=http%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdifferenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg%3Fcb%3D1417694915&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=slideshare\" width=\"425\" height=\"355\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":425,"height":355,"scrolling":false},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2omqpn/different_types_of_radial_drilling_machine_from/","ups":1,"retrieved_on":1441031267,"selftext":"","media":{"type":"slideshare.net","oembed":{"description":"Poly Kraft Machines is different types of radial drilling machine manufacturers and Suppliers of All Geared Radial Drilling Machine (RD - 40/1000), Geared Radi...","thumbnail_width":768,"width":425,"thumbnail_url":"http://cdn.slidesharecdn.com/ss_thumbnails/differenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg?cb=1417694915","title":"Different types of Radial Drilling Machine from polykraftmachines.com","provider_name":"SlideShare","provider_url":"http://www.slideshare.net","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.slideshare.net%2Fslideshow%2Fembed_code%2F42347616&amp;src_secure=1&amp;url=http%3A%2F%2Fwww.slideshare.net%2Fpolykraftmachines%2Fdifferent-types-of-radial-drilling-machine-from-polykraftmachinescom&amp;image=http%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdifferenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg%3Fcb%3D1417694915&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=slideshare\" width=\"425\" height=\"355\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":355,"author_name":"polykraftmachines","thumbnail_height":432,"author_url":"http://www.slideshare.net/polykraftmachines","type":"rich"}},"link_flair_css_class":null,"gilded":0,"name":"t3_2omqpn","id":"2omqpn","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"polykraftmachines","stickied":false,"from_id":null,"url":"http://www.slideshare.net/polykraftmachines/different-types-of-radial-drilling-machine-from-polykraftmachinescom","secure_media":{"type":"slideshare.net","oembed":{"height":355,"author_name":"polykraftmachines","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.slideshare.net%2Fslideshow%2Fembed_code%2F42347616&amp;url=http%3A%2F%2Fwww.slideshare.net%2Fpolykraftmachines%2Fdifferent-types-of-radial-drilling-machine-from-polykraftmachinescom&amp;image=http%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdifferenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg%3Fcb%3D1417694915&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=slideshare\" width=\"425\" height=\"355\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","author_url":"http://www.slideshare.net/polykraftmachines","type":"rich","thumbnail_height":432,"width":425,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdifferenttypesofradialdrillingmachinefrompolykraftmachines-141204060805-conversion-gate02-thumbnail-4.jpg%3Fcb%3D1417694915&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","title":"Different types of Radial Drilling Machine from polykraftmachines.com","description":"Poly Kraft Machines is different types of radial drilling machine manufacturers and Suppliers of All Geared Radial Drilling Machine (RD - 40/1000), Geared Radi...","thumbnail_width":768,"provider_name":"SlideShare","provider_url":"http://www.slideshare.net"}},"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418028601,"score":1,"edited":false,"quarantine":false}
{"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/8WLFBS2slSBbIxo8FNf89kMqOgOCgGcNxLs-qZVV6QI.jpg","downs":0,"secure_media_embed":{},"title":"Response to Yann LeCun's Questions on the Brain","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418043655","hide_score":false,"domain":"inbits.com","media":null,"selftext":"","retrieved_on":1441031078,"ups":0,"permalink":"/r/MachineLearning/comments/2on58p/response_to_yann_lecuns_questions_on_the_brain/","link_flair_text":null,"author":"fergbyrne","from":null,"is_self":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"id":"2on58p","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2on58p","quarantine":false,"edited":false,"score":0,"subreddit_id":"t5_2r3gv","created":1418043655,"from_kind":null,"secure_media":null,"url":"http://inbits.com/2014/12/response-to-yann-lecuns-questions-on-the-brain/","from_id":null,"stickied":false}
{"secure_media_embed":{},"title":"MusicMood - Classify Music by Mood Based on Song Lyrics","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"sebastianraschka.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418044708","retrieved_on":1441031061,"ups":0,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2on6k2/musicmood_classify_music_by_mood_based_on_song/","link_flair_text":null,"is_self":false,"author":"[deleted]","from":null,"archived":true,"id":"2on6k2","link_flair_css_class":null,"name":"t3_2on6k2","gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"quarantine":false,"score":0,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418044708,"from_kind":null,"url":"http://sebastianraschka.com/Articles/2014_musicmood.html","secure_media":null,"from_id":null}
{"url":"http://sebastianraschka.com/Articles/2014_musicmood.html","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418045700,"from_id":null,"stickied":false,"quarantine":false,"score":1,"edited":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2on7rw","name":"t3_2on7rw","gilded":0,"link_flair_css_class":null,"author":"[deleted]","from":null,"is_self":false,"permalink":"/r/MachineLearning/comments/2on7rw/musicmood_my_experiences_with_a_building_a/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441031046,"ups":1,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418045700","author_flair_text":null,"domain":"sebastianraschka.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"MusicMood. My experiences with a building a classification model based on song lyrics","secure_media_embed":{}}
{"over_18":false,"saved":false,"num_comments":11,"author_flair_css_class":null,"id":"2on7y7","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2on7y7","author":"rasbt","from":null,"is_self":false,"subreddit_id":"t5_2r3gv","created":1418045808,"from_kind":null,"url":"http://sebastianraschka.com/Articles/2014_musicmood.html","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"score":25,"edited":false,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418045808","hide_score":false,"domain":"sebastianraschka.com","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/XOtuUUI8OEO2ncBt4O10tcYrFQmnI0cTYb4R2xfOHmA.jpg","downs":0,"secure_media_embed":{},"title":"My experiences with training a classifier based on song lyrics and turning it into a webapp","permalink":"/r/MachineLearning/comments/2on7y7/my_experiences_with_training_a_classifier_based/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441031043,"ups":25}
{"stickied":false,"from_id":null,"url":"http://googleresearch.blogspot.com/2014/12/high-quality-object-detection-at-scale.html","secure_media":null,"from_kind":null,"created":1418048834,"subreddit_id":"t5_2r3gv","score":10,"edited":false,"quarantine":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2onc08","id":"2onc08","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"[deleted]","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2onc08/high_quality_object_detection_at_scale/","ups":10,"retrieved_on":1441030991,"selftext":"","media":null,"domain":"googleresearch.blogspot.com","hide_score":false,"created_utc":"1418048834","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"High Quality Object Detection at Scale","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/OoaB5Z2xDnMvnSad7hzMMFIGenYKqyZOeRKQPGPuzwk.jpg","distinguished":null}
{"quarantine":false,"edited":false,"score":12,"secure_media":null,"url":"http://redditmetrics.com/r/MachineLearning","created":1418052239,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"author":"TrendingBot","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":3,"archived":true,"id":"2onh8b","name":"t3_2onh8b","link_flair_css_class":null,"gilded":0,"media":null,"selftext":"","retrieved_on":1441030923,"ups":12,"permalink":"/r/MachineLearning/comments/2onh8b/rmachinelearning_hits_30k_subscribers/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"/r/MachineLearning hits 30K subscribers","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418052239","author_flair_text":null,"domain":"redditmetrics.com","hide_score":false}
{"from_id":null,"url":"http://www.randalscottking.com/2014/12/common-problems-with-data/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418053062,"stickied":false,"score":0,"edited":false,"quarantine":false,"num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2oniop","gilded":0,"id":"2oniop","archived":true,"from":null,"author":"rscottking73","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oniop/common_problems_with_data/","selftext":"","media":null,"ups":0,"retrieved_on":1441030904,"created_utc":"1418053062","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"randalscottking.com","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/YmcYvQDg2JLJSB5H_PBRmOw8nZ76sDXMbcoyyZiq6SI.jpg","distinguished":null,"title":"Common Problems with Data","secure_media_embed":{}}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2onnq0/whos_in_montreal_right_now_nips/","ups":11,"retrieved_on":1441030839,"selftext":"Which posters are you excited about? Which workshops are you going to?","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418055807","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Who's in Montreal right now (@ NIPS)?","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2onnq0/whos_in_montreal_right_now_nips/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418055807,"score":11,"edited":false,"quarantine":false,"link_flair_css_class":null,"name":"t3_2onnq0","gilded":0,"id":"2onnq0","archived":true,"author_flair_css_class":null,"num_comments":4,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"cypherx"}
{"ups":83,"retrieved_on":1441030685,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2onzmd/deep_neural_networks_are_easily_fooled_high/","title":"Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"arxiv.org","hide_score":false,"created_utc":"1418061889","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","score":83,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"url":"http://arxiv.org/abs/1412.1897","secure_media":null,"created":1418061889,"from_kind":null,"subreddit_id":"t5_2r3gv","is_self":false,"from":null,"author":"downtownslim","gilded":0,"name":"t3_2onzmd","link_flair_css_class":null,"id":"2onzmd","archived":true,"num_comments":48,"author_flair_css_class":null,"saved":false,"over_18":false}
{"is_self":false,"author":"xamdam","from":null,"id":"2oo6tq","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2oo6tq","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"edited":false,"score":3,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418065362,"url":"https://sites.google.com/site/software4ml/accepted-papers","secure_media":null,"from_id":null,"secure_media_embed":{},"title":"Accepted Papers - Software Engineering for Machine Learning","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"sites.google.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418065362","retrieved_on":1441030591,"ups":3,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2oo6tq/accepted_papers_software_engineering_for_machine/","link_flair_text":null}
{"saved":false,"over_18":false,"num_comments":13,"author_flair_css_class":null,"archived":true,"id":"2op7uz","link_flair_css_class":null,"gilded":0,"name":"t3_2op7uz","author":"ankitsablok89","from":null,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/2op7uz/who_are_some_of_the_leading_researchers_working/","secure_media":null,"from_kind":null,"created":1418083308,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":3,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418083308","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Who are some of the leading researchers working in the field of machine learning and working at Google or Facebook or other well known companies?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2op7uz/who_are_some_of_the_leading_researchers_working/","link_flair_text":null,"media":null,"selftext":"I am quite curious to know who are some of the machine learning experts working at some of the cream companies in US mainly like Google, Facebook, Palantir etc and startups if any.","retrieved_on":1441030111,"ups":3}
{"title":"Time Series Question","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418095151","author_flair_text":null,"retrieved_on":1441029830,"ups":3,"media":null,"selftext":"So I am playing with a pet project.\n\nI have collected several time series (dates) which can take one of four different categorical values on a given day (A,B,C,D).  I would like to train a model to forecast what future what the next week's outcomes will be. Could you please suggest any appropriate approaches?  Thanks in advance.   ","permalink":"/r/MachineLearning/comments/2optld/time_series_question/","link_flair_text":null,"is_self":true,"author":"DinosaurChemist","from":null,"archived":true,"id":"2optld","gilded":0,"name":"t3_2optld","link_flair_css_class":null,"saved":false,"over_18":false,"num_comments":3,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":3,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2optld/time_series_question/","from_kind":null,"created":1418095151,"subreddit_id":"t5_2r3gv","from_id":null}
{"stickied":false,"created":1418103913,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oq8rj/anyone_worked_with_sonar_data/","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":0,"archived":true,"id":"2oq8rj","link_flair_css_class":null,"gilded":0,"name":"t3_2oq8rj","over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"is_self":true,"author":"soulslicer0","from":null,"permalink":"/r/MachineLearning/comments/2oq8rj/anyone_worked_with_sonar_data/","link_flair_text":null,"retrieved_on":1441029614,"ups":0,"media":null,"selftext":"Has anyone one here worked with sonar data? I am hoping to do some kind of object recognition on sonar data, and am curious to know some of the pre-processing techniques, and/or features extracted from the final image. References to papers would be great too","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418103913","secure_media_embed":{},"title":"Anyone worked with Sonar Data?","distinguished":null,"thumbnail":"self","downs":0}
{"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2oq9gu","gilded":0,"archived":true,"id":"2oq9gu","from":null,"author":"[deleted]","is_self":true,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oq9gu/how_to_get_contribute_to_ongoing_developments_in/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418104384,"stickied":false,"score":1,"edited":false,"quarantine":false,"created_utc":"1418104384","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"How to get contribute to ongoing developments in AI?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oq9gu/how_to_get_contribute_to_ongoing_developments_in/","selftext":"Hey everyone, I'm about to finish a course in AI and I wanted to know:\n1.  What are some cool new open-source softwares around that I can contribute to? 2. What are some hot areas in AI now? 3. What are some skills that can get me into AI-related companies?","media":null,"ups":1,"retrieved_on":1441029605}
{"is_self":false,"author":"larsga","from":null,"id":"2oqdjo","archived":true,"name":"t3_2oqdjo","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"quarantine":false,"score":2,"edited":false,"stickied":false,"secure_media":null,"url":"http://googleresearch.blogspot.no/2014/09/building-deeper-understanding-of-images.html","subreddit_id":"t5_2r3gv","created":1418107243,"from_kind":null,"from_id":null,"title":"Building a deeper understanding of images","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/wiEQ9neB_Kc6A1LNFeDr4pzGQmiUIOrUGizuDkWaQ3M.jpg","domain":"googleresearch.blogspot.no","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418107243","author_flair_text":null,"retrieved_on":1441029552,"ups":2,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2oqdjo/building_a_deeper_understanding_of_images/","link_flair_text":null}
{"author":"n0uveau","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":13,"author_flair_css_class":null,"archived":true,"id":"2oqgn8","name":"t3_2oqgn8","link_flair_css_class":null,"gilded":0,"quarantine":false,"score":0,"edited":false,"created":1418109709,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oqgn8/took_a_course_in_ai_next/","from_id":null,"stickied":false,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Took a course in AI. Next?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418109709","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hey everyone, I'm about to finish a course in AI and I wanted to know: 1. What are some cool new open-source softwares around that I can contribute to? 2. What are some hot areas in AI now? 3. What are some skills that can get me into AI-related companies?","retrieved_on":1441029512,"ups":0,"permalink":"/r/MachineLearning/comments/2oqgn8/took_a_course_in_ai_next/","link_flair_text":null}
{"edited":false,"score":24,"quarantine":false,"stickied":false,"from_id":null,"created":1418115336,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.dlworkshop.org/accepted-papers","is_self":false,"from":null,"author":"MohamedO","gilded":0,"name":"t3_2oqmjm","link_flair_css_class":null,"id":"2oqmjm","archived":true,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"ups":24,"retrieved_on":1441029436,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oqmjm/nips_14_deep_learning_workshop_accepted_papers/","secure_media_embed":{},"title":"NIPS '14 Deep Learning Workshop - Accepted Papers","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"dlworkshop.org","author_flair_text":null,"created_utc":"1418115336","subreddit":"MachineLearning","media_embed":{}}
{"stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oqoit/course_recommendations/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418117289,"edited":false,"score":3,"quarantine":false,"name":"t3_2oqoit","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2oqoit","num_comments":14,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"nsaul","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oqoit/course_recommendations/","ups":3,"retrieved_on":1441029410,"selftext":"I'm currently figuring out which courses to take next semester, and I was hoping some internet wisdom from this relevant subreddit could help me decide.  \n\nThis will be my final semester of an undergraduate math degree at a midtear state school.  I have to decide between taking Analysis II, or a grad level Machine Learning course.  \n\nMy goals are to go to grad school studying statistics and ML and eventually work in industry.  I will probably take a pitstop in industry before going to grad school.  \n\nThe main points I've been debating with myself are:\n\n* *Analysis is a critical course for future math study and having more will help with grad school applications.*\n\n* *ML is my fix and I want to study it in grad school.*\n\n* *Studying ML seriously will require more Analysis, and the sooner the better.*\n\n* *MLwill help land an interesting job if I never make it to grad school.*\n\nDo you have any thoughts?  \n\n**tl;dr** more analysis courses, or machine learning courses?  ","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418117289","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"course recommendations","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oqrnp/jeff_hawkins_on_why_his_approach_to_ai_will_be/","selftext":"","media":null,"ups":0,"retrieved_on":1441029369,"created_utc":"1418120479","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"gigaom.com","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Jeff Hawkins on why his Approach to AI will be the approach to AI","secure_media_embed":{},"from_id":null,"secure_media":null,"url":"https://gigaom.com/2014/09/24/the-gigaom-interview-jeff-hawkins-on-why-his-approach-to-ai-will-become-the-approach-to-ai/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418120479,"stickied":false,"score":0,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":4,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2oqrnp","gilded":0,"archived":true,"id":"2oqrnp","from":null,"author":"[deleted]","is_self":false}
{"secure_media_embed":{},"title":"The fastest convolutions in Theano with meta-optimization","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/RWP8WAA3thJFVRSSbqEMdaDC4DDNWqtu_DmrGRssHuM.jpg","downs":0,"hide_score":false,"domain":"benanne.github.io","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418129129","retrieved_on":1441029252,"ups":31,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2or0or/the_fastest_convolutions_in_theano_with/","link_flair_text":null,"is_self":false,"author":"benanne","from":null,"archived":true,"id":"2or0or","gilded":0,"name":"t3_2or0or","link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":13,"quarantine":false,"edited":false,"score":31,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418129129,"secure_media":null,"url":"http://benanne.github.io/2014/12/09/theano-metaopt.html","from_id":null}
{"title":"Do we know enough about the brains to build intelligent machines?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418132288","author_flair_text":null,"retrieved_on":1441029202,"ups":4,"media":null,"selftext":"Should we prioritize learning about how brains work first, or do you think that we already know enough and the rest is just about implementing algorithms etc,","permalink":"/r/MachineLearning/comments/2or4j0/do_we_know_enough_about_the_brains_to_build/","link_flair_text":null,"is_self":true,"author":"[deleted]","from":null,"id":"2or4j0","archived":true,"name":"t3_2or4j0","gilded":0,"link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":4,"quarantine":false,"score":4,"edited":false,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2or4j0/do_we_know_enough_about_the_brains_to_build/","secure_media":null,"from_kind":null,"created":1418132288,"subreddit_id":"t5_2r3gv","from_id":null}
{"from":null,"author":"AlexDiru","is_self":true,"author_flair_css_class":null,"num_comments":6,"over_18":false,"saved":false,"name":"t3_2orgr8","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2orgr8","score":0,"edited":false,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418139804,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2orgr8/what_svm_does_the_r_package_caret_use_alternative/","stickied":false,"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"What SVM does the R package 'caret' use? / Alternative SVM suggestions wanted","author_flair_text":null,"created_utc":"1418139804","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"I need to use the SVM (used by the caret package [1] in R) in Java. One way would be to use rJava [2] and call R from Java but I will need to do this over 100,000 times so I can imagine that will take a while. Thus if I can access the SVM it uses without the overhead of R, everything will be much quicker!\n\nOr an alternative would be to use an SVM which has a similar parameter training method to the one used in caret [3] as that seems to work relatively well. Other SVMs seem to favour one class on the prediction. The data used is class balanced, min-max normalised, though I haven't done too much investigation into parameter searching - RBF kernel, I tend to set the cost to between 1 and 100, and gamma as 1/num_features.\n\nCheers for any help :)\n\n[1] http://topepo.github.io/caret/index.html\n\n[2] http://rforge.net/JRI/\n\n[3] http://topepo.github.io/caret/training.html#control","media":null,"ups":0,"retrieved_on":1441029044,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2orgr8/what_svm_does_the_r_package_caret_use_alternative/"}
{"from_id":null,"subreddit_id":"t5_2r3gv","created":1418143177,"from_kind":null,"url":"http://www.gizmag.com/learn-immersive-language-virtual-reality/35128/","secure_media":null,"stickied":false,"score":1,"edited":false,"quarantine":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2orn58","archived":true,"id":"2orn58","from":null,"author":"tonydiv","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2orn58/learn_immersive_teaches_languages_in_virtual/","selftext":"","media":null,"ups":1,"retrieved_on":1441028961,"author_flair_text":null,"created_utc":"1418143177","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"gizmag.com","thumbnail":"http://b.thumbs.redditmedia.com/q8_VFrUmDCq7QfTftOypUkrb9v85SsVrdCVZSrh1lcM.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Learn Immersive Teaches Languages in Virtual Reality"}
{"archived":true,"id":"2orpil","name":"t3_2orpil","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":7,"is_self":true,"author":"coreybenny","from":null,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2orpil/new_to_ml/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418144382,"from_id":null,"quarantine":false,"score":2,"edited":false,"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418144382","author_flair_text":null,"title":"New to ML","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","permalink":"/r/MachineLearning/comments/2orpil/new_to_ml/","link_flair_text":null,"retrieved_on":1441028930,"ups":2,"media":null,"selftext":"Hi Everyone, \nI'm new to machine learning but am looking to integrate it into some of my research (public health). Anyway I have already found some potentially good resources (books, online classes etc) to help me learn about it but was hoping to hear some recommendations from the community as well. Two resources that I am very interested in finding would be:\n1. A machine learning cookbook style book\n2. A top down approach to learning \n\nThanks for your help!"}
{"from":null,"author":"MLBlogTeam","is_self":false,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2orr4g","gilded":0,"archived":true,"id":"2orr4g","score":7,"edited":false,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://aka.ms/nuy2oy","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418145203,"stickied":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Machine Learning \u2013 Hype or Reality? Microsoft ML Experts Weigh In","secure_media_embed":{},"created_utc":"1418145203","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"aka.ms","hide_score":false,"selftext":"","media":null,"ups":7,"retrieved_on":1441028910,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2orr4g/machine_learning_hype_or_reality_microsoft_ml/"}
{"archived":true,"id":"2oru2f","link_flair_css_class":null,"name":"t3_2oru2f","gilded":0,"over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"is_self":true,"author":"pnambiar","from":null,"stickied":false,"created":1418146595,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oru2f/training_data_size_in_deep_learning/","from_id":null,"quarantine":false,"edited":false,"score":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418146595","secure_media_embed":{},"title":"Training data size in deep learning","distinguished":null,"thumbnail":"self","downs":0,"permalink":"/r/MachineLearning/comments/2oru2f/training_data_size_in_deep_learning/","link_flair_text":null,"retrieved_on":1441028871,"ups":0,"media":null,"selftext":"Hello, I am trying to find out the effect of training dataset size on the success rate of overfeat classifier. I would like to deremine the effect of pose of the objects in the training on the classification results,\n\nAny published work?"}
{"permalink":"/r/MachineLearning/comments/2orzb5/status_andor_thoughts_on_elms_extreme_learning/","link_flair_text":null,"retrieved_on":1441028804,"ups":1,"media":null,"selftext":"I've been looking into various NN training options and stumbled across Extreme Learning Machines which, the creators purport, supposedly train much faster than deep networks using more traditional back prop training methods. \n\nI notice this paper from 2004: http://www.ntu.edu.sg/home/egbhuang/pdf/ELM_IJCNN2004.PDF\n \nAnd an article in IEEE IntelligentSystems from 2013:\nhttp://www.ntu.edu.sg/home/egbhuang/pdf/IEEE-IS-ELM.pdf\n\nDoes anyone have any experience with ELMs? Impressions, opinions?","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418149165","author_flair_text":null,"title":"Status and/or thoughts on ELMs? (Extreme Learning Machines)","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2orzb5/status_andor_thoughts_on_elms_extreme_learning/","secure_media":null,"from_kind":null,"created":1418149165,"subreddit_id":"t5_2r3gv","from_id":null,"quarantine":false,"score":1,"edited":false,"archived":true,"id":"2orzb5","gilded":0,"name":"t3_2orzb5","link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":5,"is_self":true,"author":"cafedude","from":null}
{"id":"2osf7q","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2osf7q","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"is_self":true,"author":"duckandcover","from":null,"stickied":false,"created":1418156746,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2osf7q/is_there_a_good_free_paper_available_which/","from_id":null,"quarantine":false,"score":1,"edited":false,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418156746","secure_media_embed":{},"title":"Is there a good free paper available which compares the performance of classifiers given a relatively small sample size (give the FV dimensionality)?","distinguished":null,"thumbnail":"self","downs":0,"permalink":"/r/MachineLearning/comments/2osf7q/is_there_a_good_free_paper_available_which/","link_flair_text":null,"retrieved_on":1441028597,"ups":1,"media":null,"selftext":"I gather large margin classifiers are the way to go which would suggest SVM or perhaps some tree boosting methods which, as I understand it, are also large margin.  I thought I saw a helpful paper but it's not free....damn it."}
{"hide_score":false,"domain":"machinelearningmastery.com","author_flair_text":null,"created_utc":"1418158442","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Tips To Get The Most From The Naive Bayes","thumbnail":"http://b.thumbs.redditmedia.com/qY0z3-L3kpPa96fukF39b2AjEL8vnSr1qeyOGLRin4Q.jpg","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2osimj/tips_to_get_the_most_from_the_naive_bayes/","ups":3,"retrieved_on":1441028553,"selftext":"","media":null,"gilded":0,"name":"t3_2osimj","link_flair_css_class":null,"id":"2osimj","archived":true,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"jasonb","stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418158442,"secure_media":null,"url":"http://machinelearningmastery.com/better-naive-bayes/","score":3,"edited":false,"quarantine":false}
{"from":null,"author":"y05f","is_self":true,"author_flair_css_class":null,"num_comments":5,"saved":false,"over_18":false,"name":"t3_2oss6l","gilded":0,"link_flair_css_class":null,"id":"2oss6l","archived":true,"score":1,"edited":1418185410,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oss6l/need_benchmark_data_sets_to_test_my_evolutionary/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1418162892,"stickied":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"Need benchmark data sets to test my Evolutionary RNN.","secure_media_embed":{},"created_utc":"1418162892","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"I am working on Evolutionary recurrent neural networks and i want to test and compare the performance of my method. For this, i need recent benchmark data sets which are used by the most researchers. What are your suggestions?","media":null,"ups":1,"retrieved_on":1441028429,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oss6l/need_benchmark_data_sets_to_test_my_evolutionary/"}
{"permalink":"/r/MachineLearning/comments/2ot5ke/question_metrics_to_compare_between_two_cities/","link_flair_text":null,"media":null,"selftext":"I made a histogram-based visualization to compare between two states (collected data from different sources like: census .. etc).\n\nWhat is the best way to compare between two states, with metrics, using demographic data?. Is there any recommended metrics that I need to use.\n\nIs there any similar project/paper/website/book that I need to check ? ","retrieved_on":1441028256,"ups":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418169552","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"[Question] Metrics to compare between two cities, using their demographic data ?","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418169552,"url":"http://www.reddit.com/r/MachineLearning/comments/2ot5ke/question_metrics_to_compare_between_two_cities/","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"over_18":false,"saved":false,"num_comments":1,"author_flair_css_class":null,"archived":true,"id":"2ot5ke","name":"t3_2ot5ke","link_flair_css_class":null,"gilded":0,"author":"[deleted]","from":null,"is_self":true}
{"from_id":null,"url":"http://videolectures.net/sikdd2014_hruschka_labeling_facts/","secure_media":null,"created":1418171534,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"edited":false,"score":16,"quarantine":false,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"name":"t3_2ot9c4","gilded":0,"link_flair_css_class":null,"id":"2ot9c4","archived":true,"from":null,"author":"wordsnerd","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ot9c4/automatically_labeling_facts_in_a_neverending/","selftext":"","media":null,"ups":16,"retrieved_on":1441028207,"created_utc":"1418171534","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"videolectures.net","hide_score":false,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/axwz1O5Mk5NVz4W76Yw8yBNuU19Bozzgb_oSiinqVN4.jpg","distinguished":null,"title":"Automatically labeling facts in a never-ending language learning system","secure_media_embed":{}}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418202908","author_flair_text":null,"domain":"lightreading.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/KoAGDuvOr6NL_PCef3IEvnVEjW57s5kJkzjNZu4PhGk.jpg","title":"Real-Time Predictive Analytics: Are Companies Ready to Take Full Advantage?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2oum56/realtime_predictive_analytics_are_companies_ready/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441027514,"ups":0,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"archived":true,"id":"2oum56","gilded":0,"link_flair_css_class":null,"name":"t3_2oum56","author":"Sergiointelnics","from":null,"is_self":false,"url":"http://www.lightreading.com/analytics/real-time-predictive-analytics-are-companies-ready-to-take-full-advantage/v/d-id/712476","secure_media":null,"subreddit_id":"t5_2r3gv","created":1418202908,"from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0}
{"domain":"jmlr.csail.mit.edu","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418227487","author_flair_text":null,"title":"Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? [pdf]","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","permalink":"/r/MachineLearning/comments/2ovgea/do_we_need_hundreds_of_classifiers_to_solve_real/","link_flair_text":null,"retrieved_on":1441027122,"ups":31,"media":null,"selftext":"","archived":true,"id":"2ovgea","link_flair_css_class":null,"name":"t3_2ovgea","gilded":0,"saved":false,"over_18":false,"num_comments":12,"author_flair_css_class":null,"is_self":false,"author":"improbabble","from":null,"stickied":false,"secure_media":null,"url":"http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf","created":1418227487,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"quarantine":false,"edited":false,"score":31}
{"thumbnail":"http://b.thumbs.redditmedia.com/oVfUYPhNi4JsINDL13BPC9em5acyln5Od3oMRevJCJA.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"L_1 regularization in Machine Learning: Deep Convolutional Networks and Kernel Machines (x-post r/CompressiveSensing)","author_flair_text":null,"created_utc":"1418227750","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"nuit-blanche.blogspot.fr","selftext":"","media":null,"ups":2,"retrieved_on":1441027116,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ovgvp/l_1_regularization_in_machine_learning_deep/","from":null,"author":"compsens","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2ovgvp","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2ovgvp","edited":false,"score":2,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418227750,"url":"http://nuit-blanche.blogspot.fr/2014/12/l1-regularization-in-machine-learning.html","secure_media":null,"stickied":false}
{"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418229392","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"[Question] Formula for number of parameters in an undirected graphical model","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ovk24/question_formula_for_number_of_parameters_in_an/","ups":0,"retrieved_on":1441027075,"selftext":"Disclaimer: crossposted [here](http://cs.stackexchange.com/questions/34109/formula-for-number-of-parameters-in-an-undirected-graphical-probability-model).\n\nI have googled endlessly, and I cannot find it. Can anyone point me to a reference that gives a way to calculate the number of parameters in an undirected Graphical Model?\n\nAdapting from the similar formula for Bayes nets, this is my guess: if we have variables $1 \\ldots k$ where variable $i$ has $d_i$ possible values, and $n(i)$ is the set of vertices adjacent to $i$ in the independence graph, then the number of parameters would be\n\n$\\Sigma_{i=1}^{k}(d_i - 1) \\Pi_{j \\in n(i)}d_j$.\n\nCan anyone verify this, and if it's wrong, point me to a correct solution?\n\nI'm trying to calculate the AIC and BIC scores for graphical models, but to do that, I need the number of parameters each has.","media":null,"link_flair_css_class":null,"name":"t3_2ovk24","gilded":0,"id":"2ovk24","archived":true,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"jmite","stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ovk24/question_formula_for_number_of_parameters_in_an/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418229392,"edited":false,"score":0,"quarantine":false}
{"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418236165,"secure_media":null,"url":"http://www.npr.org/blogs/health/2014/12/10/369654830/a-crowd-of-scientists-finds-a-better-way-to-predict-epileptic-seizures","stickied":false,"edited":false,"score":15,"quarantine":false,"author_flair_css_class":null,"num_comments":9,"over_18":false,"saved":false,"name":"t3_2ovxnu","link_flair_css_class":null,"gilded":0,"id":"2ovxnu","archived":true,"from":null,"author":"cavedave","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ovxnu/a_crowd_of_scientists_finds_a_better_way_to/","selftext":"","media":null,"ups":15,"retrieved_on":1441026900,"author_flair_text":"naive","created_utc":"1418236165","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"npr.org","thumbnail":"http://b.thumbs.redditmedia.com/IKoEL_CuBRE1OCu99Ki_AZ4KnRBwWimcNyn75juVbPc.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"A Crowd Of Scientists Finds A Better Way To Predict Seizures"}
{"is_self":true,"author":"SunnyJapan","from":null,"archived":true,"id":"2ow60h","link_flair_css_class":null,"gilded":0,"name":"t3_2ow60h","over_18":false,"saved":false,"num_comments":15,"author_flair_css_class":null,"quarantine":false,"score":3,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418240074,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ow60h/what_kind_of_computer_setup_do_i_need_in_order_to/","secure_media":null,"from_id":null,"secure_media_embed":{},"title":"What kind of computer setup do I need in order to be able to train deep neural network on imagenet database in reasonable time (say couple of hours)?","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418240074","retrieved_on":1441026790,"ups":3,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2ow60h/what_kind_of_computer_setup_do_i_need_in_order_to/","link_flair_text":null}
{"distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"Jeremy Howard (Recent TEDxBrussels on ML) AMA @ /r/futurology - 1PM EST Dec 13th","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418246625","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Jeremy Howard discussed ML and its implications at the recent TEDxBrussels and will be hosting an AMA over at /r/futurology this coming Saturday at 1PM EST.\n\nFeel free to join us!","retrieved_on":1441026610,"ups":0,"permalink":"/r/MachineLearning/comments/2owk09/jeremy_howard_recent_tedxbrussels_on_ml_ama/","link_flair_text":null,"author":"[deleted]","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2owk09","name":"t3_2owk09","gilded":0,"link_flair_css_class":null,"quarantine":false,"score":0,"edited":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418246625,"url":"http://www.reddit.com/r/MachineLearning/comments/2owk09/jeremy_howard_recent_tedxbrussels_on_ml_ama/","secure_media":null,"from_id":null,"stickied":false}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2owk8k/darpa_offers_free_watsonlike_artificial/","selftext":"","media":null,"ups":2,"retrieved_on":1441026606,"created_utc":"1418246732","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"eetimes.com","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/P5ss9hiR13V7olAM3Ipe3s51sQyYxur6AqtSkNOz5kA.jpg","distinguished":null,"title":"DARPA Offers Free Watson-Like Artificial Intelligence","secure_media_embed":{},"from_id":null,"secure_media":null,"url":"http://www.eetimes.com/document.asp?doc_id=1324936&amp;_mc=sm_eet","created":1418246732,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"score":2,"edited":false,"quarantine":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2owk8k","gilded":0,"archived":true,"id":"2owk8k","from":null,"author":"mrprint","is_self":false}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418249284","author_flair_text":null,"domain":"stackoverflow.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Request: Clever Things to do with Naive Bayes","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2owpri/request_clever_things_to_do_with_naive_bayes/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441026535,"ups":1,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"id":"2owpri","archived":true,"name":"t3_2owpri","link_flair_css_class":null,"gilded":0,"author":"[deleted]","from":null,"is_self":false,"url":"http://stackoverflow.com/questions/27411743/request-clever-things-to-do-with-naive-bayes","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418249284,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":1}
{"quarantine":false,"score":0,"edited":false,"secure_media":null,"url":"http://stats.stackexchange.com/questions/127575/request-clever-things-to-do-with-naive-bayes","created":1418250034,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"PlexiglassPelican","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"id":"2owrdf","archived":true,"link_flair_css_class":null,"name":"t3_2owrdf","gilded":0,"media":null,"selftext":"","retrieved_on":1441026515,"ups":0,"permalink":"/r/MachineLearning/comments/2owrdf/request_clever_things_to_do_with_naive_bayes/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Request: Clever Things to do with Naive Bayes","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418250034","author_flair_text":null,"domain":"stats.stackexchange.com","hide_score":false}
{"thumbnail":"http://b.thumbs.redditmedia.com/U92N7eMAM5cjJcKqhD8_AJ1fBHozV-nrxB5muj-9apI.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Kaggle introduces a new deep learning tutorial for sentiment analysis","author_flair_text":null,"created_utc":"1418253404","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"kaggle.com","selftext":"","media":null,"ups":65,"retrieved_on":1441026426,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2owy6j/kaggle_introduces_a_new_deep_learning_tutorial/","from":null,"author":"notarowboat","is_self":false,"author_flair_css_class":null,"num_comments":15,"over_18":false,"saved":false,"name":"t3_2owy6j","link_flair_css_class":null,"gilded":0,"id":"2owy6j","archived":true,"score":65,"edited":false,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418253404,"url":"http://www.kaggle.com/c/word2vec-nlp-tutorial","secure_media":null,"stickied":false}
{"retrieved_on":1441026211,"ups":1,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2oxeor/good_news_from_the_hawkman/","link_flair_text":null,"secure_media_embed":{},"title":"Good news from the Hawkman","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"gospelherald.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418262191","quarantine":false,"edited":false,"score":1,"stickied":false,"created":1418262191,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.gospelherald.com/articles/53515/20141209/palm-computing-and-numenta-founder-jeff-hawkins-says-true-machine-intelligence-now-less-than-five-years-away.htm","secure_media":null,"from_id":null,"is_self":false,"author":"[deleted]","from":null,"id":"2oxeor","archived":true,"link_flair_css_class":null,"name":"t3_2oxeor","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0}
{"is_self":true,"author":"maemre","from":null,"id":"2oxfhb","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2oxfhb","over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"quarantine":false,"score":4,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418262642,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2oxfhb/question_how_can_i_combine_multiple_neural/","from_id":null,"secure_media_embed":{},"title":"[Question] How can I combine multiple neural networks into one?","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418262642","retrieved_on":1441026201,"ups":4,"media":null,"selftext":"Hi!\n\nI'm implementing Cooperative Q-Learning with multiple agents [as described by Ahmadabadi](http://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;citation_for_view=QlwWxmoAAAAJ:u5HHmVD_uO8C). However, there is a communication overhead (agents communicate over a network and trying to be energy efficient, so the data sent over network is costly) and I'm planning to implement Q function using a neural network instead of a matrix to reduce communication overhead.\n\nBut, I could not find a method to combine the neural networks of several agents into one to send them back to agents. Is there a method that does that efficiently? Do you have any ideas?\n\nOr, alternatively, do you have any ideas to minimize the overhead?\n\nIn the worst case, I'm planning to reconstruct Q function as a matrix and train a neural network with all of the the extracted Q functions.","permalink":"/r/MachineLearning/comments/2oxfhb/question_how_can_i_combine_multiple_neural/","link_flair_text":null}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oyga8/word2vec_is_an_implicit_shifted_pmi_matrix/","selftext":"","media":null,"ups":11,"retrieved_on":1441025725,"created_utc":"1418286839","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"papers.nips.cc","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"word2vec is an implicit shifted PMI matrix factorization","secure_media_embed":{},"from_id":null,"url":"http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418286839,"stickied":false,"score":11,"edited":false,"quarantine":false,"num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2oyga8","id":"2oyga8","archived":true,"from":null,"author":"fasterthanlite","is_self":false}
{"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418300046,"url":"http://www.wired.com/2014/01/how-to-hack-okcupid/all/","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":0,"id":"2oysym","archived":true,"name":"t3_2oysym","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"is_self":false,"author":"ArchieIndian","from":null,"permalink":"/r/MachineLearning/comments/2oysym/how_a_math_genius_hacked_okcupid_to_find_true/","link_flair_text":null,"retrieved_on":1441025560,"ups":0,"media":null,"selftext":"","hide_score":false,"domain":"wired.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418300046","secure_media_embed":{},"title":"How a Math Genius Hacked OkCupid to Find True Love | WIRED","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/2tV6V8daevJy7-nJ9IWicoHF-MAV1hqk2QGOH1OJ9wE.jpg","downs":0}
{"quarantine":false,"score":94,"edited":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418301477,"secure_media":null,"url":"http://www.shivonzilis.com/machineintelligence","from_id":null,"stickied":false,"author":"digitron","from":null,"is_self":false,"over_18":false,"saved":false,"num_comments":44,"author_flair_css_class":null,"archived":true,"id":"2oyug8","link_flair_css_class":null,"gilded":0,"name":"t3_2oyug8","media":null,"selftext":"","retrieved_on":1441025541,"ups":94,"permalink":"/r/MachineLearning/comments/2oyug8/the_current_state_of_machine_intellignece/","link_flair_text":null,"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/54E7qMPEZagXbFb97DAwWx0CC05aVrV2fXlFMOHcU2o.jpg","downs":0,"secure_media_embed":{},"title":"The Current State of Machine Intellignece","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418301477","hide_score":false,"domain":"shivonzilis.com"}
{"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2oyymj","id":"2oyymj","archived":true,"from":null,"author":"[deleted]","is_self":false,"from_id":null,"url":"http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418304925,"stickied":false,"score":1,"edited":false,"quarantine":false,"created_utc":"1418304925","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"newyorker.com","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Is \u201cDeep Learning\u201d a Revolution in Artificial Intelligence?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2oyymj/is_deep_learning_a_revolution_in_artificial/","selftext":"","media":null,"ups":1,"retrieved_on":1441025486}
{"selftext":"Does anyone have experience/tips they'd be willing to share on label propagation?\n\nIn particular I'm interested if you've used it successfully for regression problems particularly over large-ish graphs. I have 90MM nodes and 1.2B edges so I'm also curious if anyone has pointers for data of that size and larger.\n\nI've looked at these packages:\n* https://github.com/parthatalukdar/junto\n* https://github.com/smly/label-propagation\n* Sklearn (http://scikit-learn.org/stable/modules/label_propagation.html)\n* Graphchi (http://bickson.blogspot.com/2013/02/label-propagation-in-graphchi.html)\n\n","media":null,"ups":1,"retrieved_on":1441025298,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ozd7p/label_propagation_experience/","downs":0,"thumbnail":"self","distinguished":null,"title":"Label Propagation experience","secure_media_embed":{},"created_utc":"1418314125","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"edited":false,"score":1,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ozd7p/label_propagation_experience/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418314125,"stickied":false,"from":null,"author":"improbabble","is_self":true,"num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2ozd7p","archived":true,"id":"2ozd7p"}
{"quarantine":false,"score":2,"edited":false,"secure_media":null,"url":"http://aka.ms/v7ovq4","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418318655,"from_id":null,"stickied":false,"author":"MLBlogTeam","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2ozly7","link_flair_css_class":null,"gilded":0,"name":"t3_2ozly7","media":null,"selftext":"","retrieved_on":1441025184,"ups":2,"permalink":"/r/MachineLearning/comments/2ozly7/microsoft_working_with_research_community_to/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Microsoft working with research community to advance sign language recognition using Xbox Kinect","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418318655","author_flair_text":null,"domain":"aka.ms","hide_score":false}
{"name":"t3_2ozxw1","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2ozxw1","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"apmTech","stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418324571,"url":"http://www.reddit.com/r/MachineLearning/comments/2ozxw1/the_long_foggy_race_to_generalpurpose/","secure_media":null,"edited":false,"score":0,"quarantine":false,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418324571","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"The long foggy race to General-Purpose Superintelligence-An explosion will take place in AI investments. Deep learning is the first step.","thumbnail":"self","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ozxw1/the_long_foggy_race_to_generalpurpose/","ups":0,"retrieved_on":1441025030,"selftext":"short article on the future of AI, deep learning and funding- do let me know your thoughts =)\n\nhttp://www.planettechnews.com/reviews/climbing-the-mountain-the-long-foggy-race-to-general-purpose-superintelligence\n\nwww.planettechnews.com for your daily dose of innovation news","media":null}
{"retrieved_on":1441024993,"ups":1,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2p00t3/is_master_in_applied_statistics_a_good_program_to/","link_flair_text":null,"title":"Is Master in Applied Statistics a good program to get myself into Machine Learning field? If this is the wrong subreddit sorry!","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418325951","author_flair_text":null,"quarantine":false,"score":1,"edited":false,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p00t3/is_master_in_applied_statistics_a_good_program_to/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418325951,"from_id":null,"is_self":true,"author":"urmyheartBeatStopR","from":null,"archived":true,"id":"2p00t3","gilded":0,"link_flair_css_class":null,"name":"t3_2p00t3","saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":2}
{"archived":true,"id":"2p04rq","gilded":0,"link_flair_css_class":null,"name":"t3_2p04rq","saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"is_self":true,"author":"[deleted]","from":null,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p04rq/greedy_bfs_multivariate_optimization/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418327829,"from_id":null,"quarantine":false,"score":1,"edited":false,"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418327829","author_flair_text":null,"title":"Greedy BFS multi-variate optimization?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","permalink":"/r/MachineLearning/comments/2p04rq/greedy_bfs_multivariate_optimization/","link_flair_text":null,"retrieved_on":1441024940,"ups":1,"media":null,"selftext":"I'm trying to come up with a fast way to optimize a function which takes a large number of variables, each of which can take a finite set of values. \n\nI have an idea which seems like, if polished, would give a good greedy solution, but I know something like this is sure to have been done already or there's a better way to do it; so what I'm looking for is some suggestion on papers to look at to accomplish what I'm trying to do. \n\nThe algorithm I have in mind goes as follows:\n\n&amp;nbsp;\n\nThe objective is to maximize function f(a,b,c), where a,b,c can take on values in some finite sets A,B,C. \n\nBase solution = f(a0,b0,c0)\n\nDepth one branches are f(a1,b0,c0), f(a0,b1,c0), f(a0,b0,c1)\n\nDepth two branches are f(a2,b0,c0), f(a0,b2,c0), f(a0,b0,c2)\n\nDepth n branches are f(an, b0, c0), f(a0, bn, c0), f(a0, b0, cn)  \n&amp;nbsp;\n\nSo you have a tree like:\n\n                       f(a0,b0,c0)\n            /                |                \\\n    f(a1,b0,c0)         f(a0,b1,c0)        f(a0,b0,c1)    \n          |                  |                  |\n    f(a2,b0,c0)         f(a0,b2,c0)       f(a0,b0,c2)\n          |                  |                  |\n         ...                ...                ...\n\nIf the first depth branches has a better solution than the base, that branch becomes the new base; next round.\n\nOtherwise, move to the next depth and search the branches.\n\nEach depth's branches are just the previous root's parameter index + 1. So for example if f(a1,b0,c0) is a better solution, it becomes the base and the next tree is\n\n\n                       f(a1,b0,c0)\n            /                |                \\\n    f(a2,b0,c0)         f(a1,b1,c0)        f(a1,b0,c1)    \n          |                  |                  |\n    f(a3,b0,c0)         f(a1,b2,c0)       f(a1,b0,c2)\n          |                  |                  |\n         ...                ...                ...\n\n\nThe algorithm ends when reaching a leaf node, at which point the root is the returned solution."}
{"is_self":true,"author":"Vystril","from":null,"archived":true,"id":"2p1hc6","gilded":0,"link_flair_css_class":null,"name":"t3_2p1hc6","saved":false,"over_18":false,"num_comments":11,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":3,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p1hc6/methods_for_training_convolutional_neural/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418352793,"from_id":null,"title":"Methods for training convolutional neural networks on varying size test images?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418352793","author_flair_text":null,"retrieved_on":1441024311,"ups":3,"media":null,"selftext":"It seems like all the literature I come across has uniform sizes for the training images.  Are there any techniques for training a CNN on varying size input images?","permalink":"/r/MachineLearning/comments/2p1hc6/methods_for_training_convolutional_neural/","link_flair_text":null}
{"created_utc":"1418368634","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"Can anyone suggest a Java ML lib that supports Logistic Regression + L2 reg?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p25gt/can_anyone_suggest_a_java_ml_lib_that_supports/","selftext":"So far I've been playing with [LAML](https://sites.google.com/site/qianmingjie/home/toolkits/laml) which has been really nice as far as efficiency goes but doesn't seem to support any sort of regularization.\n\nWorst case I can just export my data and use a different language/lib in my pipeline but I figured I should see what's out there.\n\nCheers!","media":null,"ups":1,"retrieved_on":1441023938,"author_flair_css_class":null,"num_comments":11,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2p25gt","gilded":0,"archived":true,"id":"2p25gt","from":null,"author":"isep","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p25gt/can_anyone_suggest_a_java_ml_lib_that_supports/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418368634,"stickied":false,"score":1,"edited":false,"quarantine":false}
{"hide_score":false,"domain":"aka.ms","author_flair_text":null,"created_utc":"1418404910","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"ML in Bing brings the world\u2019s knowledge into your Office docs - take \"Insights for Office\" for a free spin","thumbnail":"default","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p3ebm/ml_in_bing_brings_the_worlds_knowledge_into_your/","ups":0,"retrieved_on":1441023356,"selftext":"","media":null,"name":"t3_2p3ebm","gilded":0,"link_flair_css_class":null,"id":"2p3ebm","archived":true,"author_flair_css_class":null,"num_comments":8,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"MLBlogTeam","stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418404910,"url":"http://aka.ms/i3p819","secure_media":null,"score":0,"edited":false,"quarantine":false}
{"quarantine":false,"score":14,"edited":false,"url":"https://github.com/MrChrisJohnson/logistic-mf","secure_media":null,"from_kind":null,"created":1418423627,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"improbabble","from":null,"is_self":false,"saved":false,"over_18":false,"num_comments":7,"author_flair_css_class":null,"id":"2p4cuq","archived":true,"gilded":0,"name":"t3_2p4cuq","link_flair_css_class":null,"media":null,"selftext":"","retrieved_on":1441022909,"ups":14,"permalink":"/r/MachineLearning/comments/2p4cuq/github_logistic_matrix_factorization_for_implicit/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/h6ifu7J3mVJVtMl5qCQ-LUZ9EYGHefGbcoVpO-BFNv8.jpg","title":"[github] Logistic Matrix Factorization for Implicit Feedback Data","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418423627","author_flair_text":null,"domain":"github.com","hide_score":false}
{"permalink":"/r/MachineLearning/comments/2p4m7m/creating_a_universal_standardschema_for_text/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441022788,"ups":1,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418428981","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Creating a universal standard/schema for text resources rather than using natural launguage processing.","secure_media_embed":{},"url":"http://www.reddit.com/r/MachineLearning/comments/2p4m7m/creating_a_universal_standardschema_for_text/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418428981,"from_id":null,"stickied":false,"quarantine":false,"score":1,"edited":1418433585,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":2,"id":"2p4m7m","archived":true,"name":"t3_2p4m7m","link_flair_css_class":null,"gilded":0,"author":"mrapogee","from":null,"is_self":true}
{"edited":false,"score":1,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.dataelixir.com/issues/13","subreddit_id":"t5_2r3gv","created":1418434045,"from_kind":null,"is_self":false,"from":null,"author":"lonriesberg","name":"t3_2p4u9a","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2p4u9a","author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"ups":1,"retrieved_on":1441022684,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p4u9a/data_elixir_issue_13_machine_learning_data/","title":"Data Elixir, Issue 13 - machine learning, data wrangling, D3, gift list for data geeks","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"dataelixir.com","hide_score":false,"created_utc":"1418434045","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"media_embed":{"scrolling":false,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FcVYShuTNw9A%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcVYShuTNw9A&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcVYShuTNw9A%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600},"subreddit":"MachineLearning","created_utc":"1418447275","author_flair_text":null,"domain":"youtube.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/3zEvQ5HXqGt4Etk-mdKavuZsm_PVQubRgJiREPTvCpU.jpg","title":"Data Scientists Alessandro Gagliardi and Amit Kapoor talk about their learnings in the field.","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcVYShuTNw9A%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcVYShuTNw9A&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcVYShuTNw9A%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"permalink":"/r/MachineLearning/comments/2p5dko/data_scientists_alessandro_gagliardi_and_amit/","link_flair_text":null,"media":{"oembed":{"thumbnail_height":360,"author_url":"http://www.youtube.com/channel/UCIyGhcNhpJSriArcCsvF5ag","type":"video","url":"http://www.youtube.com/watch?v=cVYShuTNw9A","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FcVYShuTNw9A%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcVYShuTNw9A&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcVYShuTNw9A%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"author_name":"SlideRule","provider_url":"http://www.youtube.com/","provider_name":"YouTube","description":"Our awesome mentors Alessando Gagliardi and Amit Kapoor kick off Office Hours with the most important lessons they've learnt while becoming Data Scientists. Looking to get into Data Science as a Career? If so, head here [https://www.mysliderule.com/workshops/data-science], leave your email and you'll be the first to know when our next session opens!","thumbnail_width":480,"width":600,"thumbnail_url":"http://i.ytimg.com/vi/cVYShuTNw9A/hqdefault.jpg","title":"Data Science School - Skills You Need to Get Ahead"},"type":"youtube.com"},"selftext":"","retrieved_on":1441022432,"ups":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2p5dko","name":"t3_2p5dko","link_flair_css_class":null,"gilded":0,"author":"barmalade","from":null,"is_self":false,"secure_media":{"type":"youtube.com","oembed":{"provider_url":"http://www.youtube.com/","provider_name":"YouTube","title":"Data Science School - Skills You Need to Get Ahead","thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcVYShuTNw9A%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"thumbnail_width":480,"description":"Our awesome mentors Alessando Gagliardi and Amit Kapoor kick off Office Hours with the most important lessons they've learnt while becoming Data Scientists. Looking to get into Data Science as a Career? If so, head here [https://www.mysliderule.com/workshops/data-science], leave your email and you'll be the first to know when our next session opens!","type":"video","url":"http://www.youtube.com/watch?v=cVYShuTNw9A","author_url":"http://www.youtube.com/channel/UCIyGhcNhpJSriArcCsvF5ag","thumbnail_height":360,"author_name":"SlideRule","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcVYShuTNw9A%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcVYShuTNw9A&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcVYShuTNw9A%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0"}},"url":"https://www.youtube.com/watch?v=cVYShuTNw9A","subreddit_id":"t5_2r3gv","created":1418447275,"from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false}
{"ups":61,"retrieved_on":1441022132,"selftext":"I am going to start a comment threads on each nomination category. The categories at the moment will be\n\nMost influential Variable (Best commenter/contributor to the community)\n\nBest discussion\n\nHighest AUC (Best submission)\n\n\nTo nominate something, find the top comment of mine that is the category, and reply to it with a link and a brief description of why you think it is Best Of material. If you want another category PM the mods with a suggestion.\nTake a moment to think back and see what sticks out in your own memory as good AMA's, threads and submissions","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p60ty/machine_learning_best_of_2014_nomination_thread/","secure_media_embed":{},"title":"Machine Learning Best of 2014 Nomination thread","thumbnail":"self","downs":0,"distinguished":"moderator","hide_score":false,"domain":"self.MachineLearning","author_flair_text":"naive","created_utc":"1418471417","subreddit":"MachineLearning","media_embed":{},"score":61,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"created":1418471417,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p60ty/machine_learning_best_of_2014_nomination_thread/","is_self":true,"from":null,"author":"cavedave","link_flair_css_class":null,"name":"t3_2p60ty","gilded":0,"id":"2p60ty","archived":true,"author_flair_css_class":null,"num_comments":22,"over_18":false,"saved":false}
{"permalink":"/r/MachineLearning/comments/2p6518/ml_problem_will_this_compile/","link_flair_text":null,"retrieved_on":1441022076,"ups":4,"media":null,"selftext":"*Given ample training data of compiling and non-compiling strings of source code in a single language (consider the language to be fully unknown, albeit consistent), construct a self configuring solution for classifying new strings of source code from that same language as either compiling or non-compiling.*\n\nThe solution to this problem could theoretically comprehend (or tractably index) the foundational structure of language (programming, spoken, written, mathematical, scientific, genetic, viral, chemical, or otherwise).\n\n**If anyone else is actively working in this problem domain, I'm looking for you.**\n","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418476530","secure_media_embed":{},"title":"ML Problem: \"Will this compile?\"","distinguished":null,"thumbnail":"self","downs":0,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418476530,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p6518/ml_problem_will_this_compile/","from_id":null,"quarantine":false,"edited":false,"score":4,"archived":true,"id":"2p6518","name":"t3_2p6518","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":10,"is_self":true,"author":"noel___","from":null}
{"ups":2,"retrieved_on":1441021867,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p6l99/nips2014_poster_papers_xpost_rcompressivesesning/","title":"NIPS2014 Poster papers (x-post r/CompressiveSesning )","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/e_E03PqTQmC5jRrsnV1DMesqm4FhxhM4DgW-4TzBv_s.jpg","distinguished":null,"domain":"nuit-blanche.blogspot.com","hide_score":false,"created_utc":"1418489713","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","edited":false,"score":2,"quarantine":false,"stickied":false,"from_id":null,"url":"http://nuit-blanche.blogspot.com/2014/12/nips2014-poster-papers.html","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418489713,"is_self":false,"from":null,"author":"compsens","link_flair_css_class":null,"name":"t3_2p6l99","gilded":0,"id":"2p6l99","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false}
{"is_self":true,"from":null,"author":"nytalmx","link_flair_css_class":null,"gilded":0,"name":"t3_2p6oc5","archived":true,"id":"2p6oc5","author_flair_css_class":null,"num_comments":3,"over_18":false,"saved":false,"score":1,"edited":1418544301,"quarantine":false,"stickied":false,"from_id":null,"created":1418491654,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p6oc5/can_anyone_give_some_more_insight_into_this_quote/","secure_media":null,"secure_media_embed":{},"title":"Can anyone give some more insight into this quote?","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418491654","subreddit":"MachineLearning","media_embed":{},"ups":1,"retrieved_on":1441021827,"selftext":"http://youtu.be/LZnAFO5gkOQ?t=37m12s\n\nSpecifically the the stuff he says about it being well understood that there are high dimensional functions which cannot be approximated, and about neural nets approximating a \"class\" of function and that we know nothing about the functional spaces they can approximate.\n\nAnd then the panelist to his right says that we do know, but he gets cut off.\n\nCan anyone can give some more insight about these questions?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p6oc5/can_anyone_give_some_more_insight_into_this_quote/"}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Link to Futurology -&gt;AMA I'm Jeremy Howard, Enlitic CEO, Kaggle Past President, Singularity U Faculty. Ask me anything about machine learning, future of medicine, technological unemployment, startups, VC, or programming","author_flair_text":null,"created_utc":"1418507609","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"reddit.com","selftext":"","media":null,"ups":6,"retrieved_on":1441021476,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p7fgr/link_to_futurology_ama_im_jeremy_howard_enlitic/","from":null,"author":"hajshin","is_self":false,"author_flair_css_class":null,"num_comments":3,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2p7fgr","archived":true,"id":"2p7fgr","edited":false,"score":6,"quarantine":false,"from_id":null,"from_kind":null,"created":1418507609,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/Futurology/comments/2p6k20/im_jeremy_howard_enlitic_ceo_kaggle_past/","secure_media":null,"stickied":false}
{"selftext":"I've been working on this project for a while and am still struggling to  get my automatic detection algorithms to work every time.\n\nGiven  a set of images like [this](http://imgur.com/4xBwedk) I need to extract the coloured region inside of the border of each square. \n\nIt seemed relatively simple at first but I can't get my code to work on every image. \n\nThe problem is the images may have been taken under different lighting conditions or slightly rotated, etc, so some times some of the squares are missed. \n\nI currently apply a Gaussian blur to the grey-scale image, then use otzu threshold to biniarize the image for contour detection. The contours are then sorted by size, shape ,and centroid to get the positions of the regions of interest which is used to extract the colour information.\n\nThe problem is sometimes contour detection fails to get the outline for each square. I can play with the parameters of the blur and threshold step to get it to work, but I would like it to automatically determine optimal parameters to get a 10x10 grid of \"perfect\" square contours.\n\nIs there a machine learning or computer vision method to automatically determine the proper parameters for this type of image processing?  ","media":null,"ups":0,"retrieved_on":1441021086,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p89gb/is_it_possible_to_automatically_determine_the/","downs":0,"thumbnail":"self","distinguished":null,"title":"Is it possible to automatically determine the value of parameters for this type of image processing?","secure_media_embed":{},"created_utc":"1418526431","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"score":0,"edited":false,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p89gb/is_it_possible_to_automatically_determine_the/","subreddit_id":"t5_2r3gv","created":1418526431,"from_kind":null,"stickied":false,"from":null,"author":"cvnovice","is_self":true,"num_comments":5,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2p89gb","archived":true,"id":"2p89gb"}
{"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p8epv/is_machine_learning_closely_related_to_artificial/","subreddit_id":"t5_2r3gv","created":1418529986,"from_kind":null,"is_self":true,"from":null,"author":"[deleted]","name":"t3_2p8epv","link_flair_css_class":null,"gilded":0,"id":"2p8epv","archived":true,"author_flair_css_class":null,"num_comments":6,"saved":false,"over_18":false,"ups":0,"retrieved_on":1441021019,"selftext":"Both of these fields of study interest me greatly, but I'm honestly not sure if there is much of a difference between the two. Would machine learning also be 'artificial intelligence'?\n\nOr are they different because in the case of machine learning, we can technically have unsupervised learning in which intelligence is gained without any support. Whereas in artificial intelligence, I suppose we would support our programs with a variety of rules and decision making abilities which were defined by us.\n\nWhat's the difference?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p8epv/is_machine_learning_closely_related_to_artificial/","title":"Is machine learning closely related to artificial intelligence?","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418529986","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418545132","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"True art is samples from function spaces which can only be reached via a generative process which involves intelligence.","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p8wv2/true_art_is_samples_from_function_spaces_which/","ups":0,"retrieved_on":1441020784,"selftext":"What is art? Maybe machine learning can provide an answer.  Sorry to veer from hardcore machine learning, this is more of a philosophical question.  I think that the statement in the title is true.  For example, a non-intelligent program couldn't create a cartoon that satrizes something, that would require intelligence.  Cartoons are a type of art.  It applies to other art as well, I just chose cartoons because it seemed easier to explain by starting from there because some paintings do appear that they could be created by non-intelligent generative processes, (jackson pollack) which brings the question of whether that is true art.\n\nIt could be that \"function spaces\" of things/images/audio that can only be generated by human level intelligence, may be of interest to our brain and make us feel awe or curiousity precisely because they are from a different class of functions which look completely different from the patterns we see in nature. So since our visual system and auditory system is so attuened to compressing and processing the information in natural systems, when we see something which breaks many rules of structure that we are used to, it is very exciting for our perception.\n\nedit: also, I dont want to get into an argument on whether science and math can be art as well.  I personally think that science and math is more art than art, however, if you asked a wide survey, I think most people would define art as needing to be created by humans.  On the other hand, now that I think about it, science and math WERE created by humans, so maybe they are in fact art even by my prior defintion.","media":null,"link_flair_css_class":null,"gilded":0,"name":"t3_2p8wv2","archived":true,"id":"2p8wv2","author_flair_css_class":null,"num_comments":6,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"nytalmx","stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p8wv2/true_art_is_samples_from_function_spaces_which/","secure_media":null,"created":1418545132,"subreddit_id":"t5_2r3gv","from_kind":null,"edited":1418546018,"score":0,"quarantine":false}
{"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Who went to NIPS 2014 and what were the best sessions you attended?","author_flair_text":null,"created_utc":"1418546900","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"What were the best sessions you attended at NIPS 2014?","media":null,"ups":26,"retrieved_on":1441020762,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p8yhb/who_went_to_nips_2014_and_what_were_the_best/","from":null,"author":"evc123","is_self":true,"author_flair_css_class":null,"num_comments":15,"over_18":false,"saved":false,"name":"t3_2p8yhb","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2p8yhb","score":26,"edited":false,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418546900,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2p8yhb/who_went_to_nips_2014_and_what_were_the_best/","stickied":false}
{"from":null,"author":"umeedu","is_self":true,"num_comments":8,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2p8z3e","gilded":0,"id":"2p8z3e","archived":true,"score":4,"edited":false,"quarantine":false,"from_id":null,"from_kind":null,"created":1418547641,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2p8z3e/applying_machine_learning_for_identifying_driver/","secure_media":null,"stickied":false,"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Applying machine learning for identifying driver behavior","author_flair_text":null,"created_utc":"1418547641","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"I am a grad student in transportation engineering in Canada. I wrote some code to identify different stages of driving (uninfluenced driving, approaching, following, braking) as described in Wiedemann car following model. Because driving behavior varies for the same driver, among different vehicle types and driving styles, my code was a crude approximation of reality. \nI want to repeat this exercise using machine learning (ML)/ data science in R but have no background in ML so far. \nWhat do you think I should start learning first in ML in context of the research problem?","media":null,"ups":4,"retrieved_on":1441020755,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p8z3e/applying_machine_learning_for_identifying_driver/"}
{"media":{"type":"imgur.com","oembed":{"provider_name":"Imgur","provider_url":"http://imgur.com","thumbnail_url":"http://i.imgur.com/2dKCQHh.jpg","width":550,"title":"Visualizing Optimization Algos","description":"Due to the large initial gradient, velocity based techniques shoot off and bounce around - adagrad almost goes unstable for the same reason. Algos that scale gradients/step sizes like adadelta and RMSProp proceed more like accelerated SGD and handle large gradients with more stability.","thumbnail_width":620,"type":"rich","thumbnail_height":480,"height":550,"version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fimgur.com%2Fa%2FHqolp%2Fembed&amp;url=http%3A%2F%2Fimgur.com%2Fa%2FHqolp&amp;image=http%3A%2F%2Fi.imgur.com%2F2dKCQHh.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"550\" height=\"550\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"}},"selftext":"","retrieved_on":1441020694,"ups":228,"permalink":"/r/MachineLearning/comments/2p93s9/nice_visualization_of_stochastic_optimizers_by/","link_flair_text":null,"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/PMMD3-bU9h_YGl8urD9Wg8wDlSIAK5tqFEUIRqzCCJA.jpg","downs":0,"secure_media_embed":{},"title":"Nice visualization of stochastic optimizers by Alec Radford: (SGD, momentum, Nesterov, AdaGrad, AdaDelta, RMSProp)","subreddit":"MachineLearning","media_embed":{"width":550,"height":550,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fimgur.com%2Fa%2FHqolp%2Fembed&amp;url=http%3A%2F%2Fimgur.com%2Fa%2FHqolp&amp;image=http%3A%2F%2Fi.imgur.com%2F2dKCQHh.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=imgur\" width=\"550\" height=\"550\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"author_flair_text":null,"created_utc":"1418553676","hide_score":false,"domain":"imgur.com","quarantine":false,"edited":false,"score":228,"created":1418553676,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://imgur.com/a/Hqolp","secure_media":null,"from_id":null,"stickied":false,"author":"exellentpossum","from":null,"is_self":false,"over_18":false,"saved":false,"num_comments":24,"author_flair_css_class":null,"id":"2p93s9","archived":true,"name":"t3_2p93s9","link_flair_css_class":null,"gilded":0}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2p9lz0/suggestions_for_a_beginner/","ups":0,"retrieved_on":1441020398,"selftext":"Hello r/machinelearning !\n\nI am a beginner in the wonderful field of Machine Learning. I took Prof. Andrew Ng's class in coursera a year ago,\nso I know somewhat about regressions, optimization algorithms, basic neural nets, SVMs, and the likes.\n\nHowever, I have not yet used any of those knowledge in any practical areas. With the new era of Deep Learning and the \nexciting prospects it entails, I am extremely eager to dive my hands into it, only I feel that I am not ready. My knowledge\nof ML is a limited one that I gathered from Prof Andrew's course and my mathematics isn't at the level where I could be called an expert.\n\nThus, I'd like to inquire of good resources which could fill the gaps in my knowledge of ML and prepare me for a dive into the world of Deep Learning.\nI'd really appreciate if you guys could point me in the right direction.\n\nRegards.","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418572294","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Suggestions for a beginner..","thumbnail":"self","downs":0,"distinguished":null,"stickied":false,"from_id":null,"from_kind":null,"created":1418572294,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2p9lz0/suggestions_for_a_beginner/","secure_media":null,"edited":false,"score":0,"quarantine":false,"name":"t3_2p9lz0","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2p9lz0","num_comments":6,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"pddpro"}
{"url":"http://www.reddit.com/r/MachineLearning/comments/2p9mn0/how_to_get_into_a_phd_program_in_machine_learning/","secure_media":null,"from_kind":null,"created":1418572752,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0,"saved":false,"over_18":false,"num_comments":4,"author_flair_css_class":null,"id":"2p9mn0","archived":true,"name":"t3_2p9mn0","link_flair_css_class":null,"gilded":0,"author":"ankitsablok89","from":null,"is_self":true,"permalink":"/r/MachineLearning/comments/2p9mn0/how_to_get_into_a_phd_program_in_machine_learning/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441020390,"ups":0,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418572752","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"How to get into a Ph.D program in Machine Learning at institutions like MIT, Stanford, Caltech, UCB etc..","secure_media_embed":{}}
{"created_utc":"1418583130","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"linkedin.com","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"HR article, why your data scientist sucks","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pa3nl/hr_article_why_your_data_scientist_sucks/","selftext":"","media":null,"ups":0,"retrieved_on":1441020169,"author_flair_css_class":null,"num_comments":3,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pa3nl","archived":true,"id":"2pa3nl","from":null,"author":"[deleted]","is_self":false,"from_id":null,"url":"https://www.linkedin.com/pulse/why-your-data-scientist-sucks-benjamin?trk=prof-post","secure_media":null,"subreddit_id":"t5_2r3gv","created":1418583130,"from_kind":null,"stickied":false,"score":0,"edited":false,"quarantine":false}
{"retrieved_on":1441020009,"ups":15,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2pag0q/great_discussion_about_initializing_neural/","link_flair_text":null,"secure_media_embed":{},"title":"Great discussion about initializing neural networks on G+","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/d0ZhyD9pmT4G3JbGZtHoeR96UzT111XE5Fcffuutzzo.jpg","downs":0,"hide_score":false,"domain":"plus.google.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418589752","quarantine":false,"score":15,"edited":false,"stickied":false,"from_kind":null,"created":1418589752,"subreddit_id":"t5_2r3gv","url":"https://plus.google.com/+SoumithChintala/posts/RZfdrRQWL6u","secure_media":null,"from_id":null,"is_self":false,"author":"benanne","from":null,"archived":true,"id":"2pag0q","name":"t3_2pag0q","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":7,"author_flair_css_class":null}
{"permalink":"/r/MachineLearning/comments/2paowz/identification_of_long_running_patterns/","link_flair_text":null,"media":null,"selftext":"Hello guys,\n\nI have changed my job into the security niche and I am currently researching the question about \"long running attacks\". Currently, I haven`t found much. However, I believe it is possible to use machine learning to extract such long running patterns on log files from different sources.\n\n* Is there any research done on \"long term anomaly detection\"? Any ideas how to realize such a method? \n* Are there still some open problem in this niche?\n\nI really appreciate your answer!\n","retrieved_on":1441019894,"ups":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418594452","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Identification of \"long running\" patterns","created":1418594452,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2paowz/identification_of_long_running_patterns/","from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"archived":true,"id":"2paowz","name":"t3_2paowz","link_flair_css_class":null,"gilded":0,"author":"piscoster","from":null,"is_self":true}
{"distinguished":null,"downs":0,"thumbnail":"default","title":"How should I deal with categorical feature vector with feature groups that have growing number of categories","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418598213","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"So, what the title says: How should I deal with categorical feature vector with feature groups that have growing number of categories\n\nBackground:\nAs an exercise, I would like to do a binary classification on URLs. To figure out if they are spam or benign.  \n(Some people say this may not be the best way to detect spam... but thats another issue. This is an exercise more than anything)\n\n\nSo, given a URIs like this:\n\nFtp://foobarhost:123/some/path\n\nHttp://1234Host.com/some/other/path?some=query&amp;string=true\n\n\nOne of the feature groups will be the  Scheme (Protocol)  i.e  FTP | HTTP | HTTPS  etc..\n\n\nI am representing this as a binary categorical feature vector where the value is either 0 or 1:\n\n&lt;FTP, HTTP, HTTPS&gt;\n\n\nthen we have other feature vectors based on the other parts of the URI  (host, path, query etc)\n\n\nNote: The reason we keep separate feature vectors for all of the feature groups is that for example the word Foo means completely different things depending on where in the URI we see it\n\n.\n.\n.\nWhen we have fully built all of these feature group vectors, we would like to combine all of them to become our input. This works well\n\n\nBUT, what can we do if for example we come across a new Scheme/Protocol  FooBar:// and grow the  Scheme feature group?  If we grow that feature vector, then when we combine  the index where the next feature group started would mean something completely different.\n\n\nExample: Before growing this is our categorical input vector:\n\n [Scheme][Host bag of words   ]\n\n&lt;Http, Ftp, foo, bar, moo, woof,....&gt;\n\n\nAfter growing our Scheme feature group this is what it will look like:\n\n [       Scheme      ][Host bag of words   ]\n\n&lt;Http, Ftp,**HTTPS** foo, bar, moo, woof,....&gt;\n\nSo everything after index 2 means something completely different than it used to before growing the feature vector\n\n\n\nYou might say, just add it to the end of the end of the combined feature vector... but how do you keep track of that particular feature being at that index?\n\n\nSorry for the wall of text.. trying to explain the issue the best I can\n","retrieved_on":1441019803,"ups":1,"permalink":"/r/MachineLearning/comments/2pavv8/how_should_i_deal_with_categorical_feature_vector/","link_flair_text":null,"author":"[deleted]","from":null,"is_self":true,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"id":"2pavv8","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2pavv8","quarantine":false,"score":1,"edited":1418602487,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pavv8/how_should_i_deal_with_categorical_feature_vector/","created":1418598213,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false}
{"permalink":"/r/MachineLearning/comments/2pb1a7/train_time_layer_resizing/","link_flair_text":null,"media":null,"selftext":"When designing a neural net, depth and sizes of each layer matter a lot.   All papers I've seen so far seem to manually choose sizes.\n\nI'm considering a system which (imagine all layers are fully connected for now) can dynamically add and remove nodes and layers at training time to try to put parameters only where they are valuable.\n\nEvery epoch, the \"least useful\" 1% of nodes in a fully connected neural net are removed, and a new 1% of nodes are added randomly and randomly initialised.\n\n\"Least useful\" nodes are determined by looking at d(Node output value)/d(Loss Function) averaged across a large number of training examples.\n\nAdding layers can be done by inserting a new \"unity\" layer between two existing layers, which has all parameters set to just pass through data unmodified (and no nonlinear elements), but it can then have new non-unity nodes added through the above mechanism.\n\nOne can imagine there are (sometimes non-trivial) extensions to this idea to apply to other layer types too.\n\nHas this been done before, and is there any literature on the topic?","retrieved_on":1441019733,"ups":4,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418601151","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Train time layer resizing","created":1418601151,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2pb1a7/train_time_layer_resizing/","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"score":4,"edited":false,"over_18":false,"saved":false,"num_comments":4,"author_flair_css_class":null,"archived":true,"id":"2pb1a7","gilded":0,"link_flair_css_class":null,"name":"t3_2pb1a7","author":"londons_explorer","from":null,"is_self":true}
{"hide_score":false,"domain":"quantamagazine.org","author_flair_text":null,"created_utc":"1418604216","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"A Common Logic to Seeing Cats and Cosmos | Quanta Magazine","thumbnail":"http://b.thumbs.redditmedia.com/5D-0I0l0TakSNbdmKXanr_FNvlsZ2NvZSs6Vnv01SrU.jpg","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pb6wm/a_common_logic_to_seeing_cats_and_cosmos_quanta/","ups":1,"retrieved_on":1441019660,"selftext":"","media":null,"name":"t3_2pb6wm","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2pb6wm","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"masharpe","stickied":false,"from_id":null,"created":1418604216,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos/","secure_media":null,"score":1,"edited":false,"quarantine":false}
{"is_self":true,"from":null,"author":"[deleted]","gilded":0,"name":"t3_2pbfyf","link_flair_css_class":null,"archived":true,"id":"2pbfyf","num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pbfyf/my_experience_with_using_a_linear_model_for_tic/","from_kind":null,"created":1418609332,"subreddit_id":"t5_2r3gv","title":"My experience with using a linear model for Tic Tac Toe learning","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418609332","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":0,"retrieved_on":1441019543,"selftext":"I have put up my results here:\nhttp://skothawala.com/2014/12/15/my-experience-with-using-a-linear-model-for-tic-tac-toe-learning/\n\nThe results, in a nutshell, were that given a set of features, the learned linear model failed to show any significant improvement over one using random weights. It did, however, show a significant improvement over a player making moves at random. The credit, I conclude, is due to the algorithm used by the learning player to make a move.\n\nI don't have much formal training in ML and am trying to learn on my own. I would very much like to learn about any bugs or flaws in my approach, and any suggestions from more experienced practitioners will be much appreciated. \n\nThanks for checking it out. ","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pbfyf/my_experience_with_using_a_linear_model_for_tic/"}
{"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418614696","author_flair_text":null,"title":"How do I get started with Abstractive Text Summarization?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","permalink":"/r/MachineLearning/comments/2pbpe9/how_do_i_get_started_with_abstractive_text/","link_flair_text":null,"retrieved_on":1441019421,"ups":7,"media":null,"selftext":"Hello, I've been working on automatic extractive text summarization where the program picks the most important sentences of a text and forms its summary. \nWhat I would like to do now is more challenging. I would like to generate summaries in natural language, and not just pick important sentences. The computer should generate abstracts that are grammatically correct and content wise coherent. \nHow do I go about this problem? What machine learning algorithms would help? What kind of features am I supposed to identify?\nSorry for the bad formatting, I'm on my phone. \nThanks ","id":"2pbpe9","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2pbpe9","saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":4,"is_self":true,"author":"fiveblinks","from":null,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2pbpe9/how_do_i_get_started_with_abstractive_text/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1418614696,"from_kind":null,"from_id":null,"quarantine":false,"score":7,"edited":false}
{"author":"[deleted]","from":null,"is_self":true,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"id":"2pcfzc","archived":true,"name":"t3_2pcfzc","link_flair_css_class":null,"gilded":0,"quarantine":false,"score":1,"edited":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2pcfzc/where_do_you_think_machine_learningai_will_be_in/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418634742,"from_id":null,"stickied":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Where do you think machine learning/AI will be in 5 years?","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418634742","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"","retrieved_on":1441019076,"ups":1,"permalink":"/r/MachineLearning/comments/2pcfzc/where_do_you_think_machine_learningai_will_be_in/","link_flair_text":null}
{"stickied":false,"created":1418639177,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pck7y/autotuning_pid_controller/","from_id":null,"quarantine":false,"edited":false,"score":15,"archived":true,"id":"2pck7y","name":"t3_2pck7y","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":12,"is_self":true,"author":"joeflux","from":null,"permalink":"/r/MachineLearning/comments/2pck7y/autotuning_pid_controller/","link_flair_text":null,"retrieved_on":1441019021,"ups":15,"media":null,"selftext":"I've been looking for weeks, and can't find out how to autotune a PID algorithm in c++.  All I find is abstract mathematical equations that I struggle to interpret.\n\nIt's for a quad-copter.  I want to autotune the PID parameters for a quadcopter.","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418639177","secure_media_embed":{},"title":"Autotuning PID controller?","distinguished":null,"thumbnail":"self","downs":0}
{"from_kind":null,"created":1418646329,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2pcr52/ml_algorithm_parameters_in_python/","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"edited":1418653530,"score":2,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":5,"archived":true,"id":"2pcr52","link_flair_css_class":null,"name":"t3_2pcr52","gilded":0,"author":"InfinityCoffee","from":null,"is_self":true,"permalink":"/r/MachineLearning/comments/2pcr52/ml_algorithm_parameters_in_python/","link_flair_text":null,"media":null,"selftext":"I was wondering what the preferred way to handle (often large-dimensional) parameter sets in Python is, as you'd find in neural networks and Bayesian variational methods (particularly the latter). I am especially wondering how you make your code interface well with gradient methods.\n\nNumpy functionality seems a must, but storing parameters in separate named arrays has two weaknesses as I see it.\n\n* Operating on the entire parameter set, e.g. adding a gradient, is impossible, so you have to write the operation for each array of parameters explicitly.\n* If you have variations of the model with only a subset of the parameters inheritance becomes difficult to work with.\n\nStoring everything in a single array/vector seems like a terrible solution as the code would become unintelligible. So what is a better way to do it? A dictionary so you can loop over the parameters? A Pandas Panel frame? A parameter class with its own update method?\n\nWould there be any significant overhead on using a dictionary of a few large arrays and then looping over the keys, as opposed to writing each operation explicitly? ","retrieved_on":1441018932,"ups":2,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418646329","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"ML algorithm parameters in Python"}
{"is_self":true,"from":null,"author":"acrossthepond925","link_flair_css_class":null,"name":"t3_2pcvs6","gilded":0,"id":"2pcvs6","archived":true,"num_comments":2,"author_flair_css_class":null,"saved":false,"over_18":false,"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pcvs6/i_need_someone_to_run_a_few_topic_modeling/","created":1418650383,"subreddit_id":"t5_2r3gv","from_kind":null,"title":"I need someone to run a few topic modeling algorithms for me","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418650383","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":0,"retrieved_on":1441018871,"selftext":"Relatively new to machine learning, but decently well read on the particular applications I'm interested in for my research (topic modeling applications mostly).\n\nI'm learning slowly on the technical side (languages) but really need to start making just a little basic headway. Does anyone have any suggestions of where I might go to hire someone to do some basic work topic modeling a couple of corpora for me? ","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pcvs6/i_need_someone_to_run_a_few_topic_modeling/"}
{"selftext":"","media":null,"ups":0,"retrieved_on":1441018807,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pd0rv/the_four_vs_of_big_data/","thumbnail":"http://b.thumbs.redditmedia.com/FKvbUOv2ZLOphLx9rUCc8Fv9P5ZN0B5Qi90jRKV1lwY.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"The Four V's of Big Data","author_flair_text":null,"created_utc":"1418654081","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"ibmbigdatahub.com","edited":false,"score":0,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418654081,"secure_media":null,"url":"http://www.ibmbigdatahub.com/infographic/four-vs-big-data","stickied":false,"from":null,"author":"Quantizedz","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2pd0rv","gilded":0,"id":"2pd0rv","archived":true}
{"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Questions about Q-Learning using Neural Networks","author_flair_text":null,"created_utc":"1418655452","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"I have implemented Q-Learning as described in,\n\nhttp://web.cs.swarthmore.edu/~meeden/cs81/s12/papers/MarkStevePaper.pdf\n\nIn order to approx. Q(S,A) I use a neural network structure like the following,\n\n - Activation sigmoid\n - Inputs, number of inputs + 1 extra for the Action (All Inputs Scaled 0-1)\n - Outputs, single output. Q-Value\n - N number of M Hidden Layers.\n - Exploration method random 0 &lt; rand() &lt; propExplore\n\nAt each learning iteration I run all action/state pairs through the neural network either pick one at random or choose the one with the highest Q-Value then using the following formula,\n\nhttp://i.stack.imgur.com/e3hgc.png\n\nI calculate a Q-Target value then calculate an error using,\n\n    error = QTarget - LastQValueReturnedFromNN\n\nand the neural network using this error.\n\nQ1, Am I on the right track? I have seen some papers that implement a NN with one output neuron for each action.\n\nQ2, My reward function returns a number between -1 and 1. Is it ok to return a number between -1 and 1 when the activation function is sigmoid (0 1)\n\nQ3, From my understanding of this method given enough training instances it should be quarantined to find an optimal policy wight? When training for XOR sometimes it learns it after 2k iterations sometimes it won't learn even after 40k 50k iterations. Is this because random is random or am I missing something. (I've tried using boltzmann exploration instead of pure random but still iteration count needed for xor fluctuates.)","media":null,"ups":2,"retrieved_on":1441018778,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pd2zk/questions_about_qlearning_using_neural_networks/","from":null,"author":"yerlikayahamza","is_self":true,"num_comments":6,"author_flair_css_class":null,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2pd2zk","id":"2pd2zk","archived":true,"edited":false,"score":2,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418655452,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pd2zk/questions_about_qlearning_using_neural_networks/","stickied":false}
{"score":0,"edited":false,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://skothawala.com/2014/12/15/my-experience-with-using-a-linear-model-for-tic-tac-toe-learning/","subreddit_id":"t5_2r3gv","created":1418658942,"from_kind":null,"stickied":false,"from":null,"author":"[deleted]","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2pd8t9","id":"2pd8t9","archived":true,"selftext":"","media":null,"ups":0,"retrieved_on":1441018703,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pd8t9/using_a_linear_model_for_tic_tac_toe_learning/","downs":0,"thumbnail":"default","distinguished":null,"title":"Using a linear model for tic tac toe learning","secure_media_embed":{},"created_utc":"1418658942","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"skothawala.com","hide_score":false}
{"title":"Why Neural Networks Look Set To Thrash The Best Human Go Players For The First Time | MIT Technology Review","secure_media_embed":{},"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/LanyzL5bX8X3PAXfJFhkeHPor8LN1gmcqeskM4nRfE4.jpg","distinguished":null,"domain":"technologyreview.com","hide_score":false,"created_utc":"1418661887","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":64,"retrieved_on":1441018632,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pde8q/why_neural_networks_look_set_to_thrash_the_best/","is_self":false,"from":null,"author":"ezetter","link_flair_css_class":null,"name":"t3_2pde8q","gilded":0,"archived":true,"id":"2pde8q","num_comments":41,"author_flair_css_class":null,"saved":false,"over_18":false,"edited":false,"score":64,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.technologyreview.com/view/533496/why-neural-networks-look-set-to-thrash-the-best-human-go-players-for-the-first-time/","created":1418661887,"from_kind":null,"subreddit_id":"t5_2r3gv"}
{"secure_media_embed":{},"title":"A Parallel and Efficient Algorithm for Learning to Match [github]","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/QPePEx5TxFYV1gj0BVOJubhChd1fkZddPM1bFsimQsM.jpg","downs":0,"hide_score":false,"domain":"github.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418665373","retrieved_on":1441018543,"ups":2,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2pdl43/a_parallel_and_efficient_algorithm_for_learning/","link_flair_text":null,"is_self":false,"author":"improbabble","from":null,"archived":true,"id":"2pdl43","link_flair_css_class":null,"gilded":0,"name":"t3_2pdl43","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"quarantine":false,"score":2,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418665373,"url":"https://github.com/shangjingbo1226/PL2M","secure_media":null,"from_id":null}
{"link_flair_css_class":null,"name":"t3_2pdz9n","gilded":0,"archived":true,"id":"2pdz9n","num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"Foxtr0t","stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pdz9n/discovering_structure_in_highdimensional_data/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418672330,"edited":false,"score":16,"quarantine":false,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418672330","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Discovering Structure in High-Dimensional Data Through Correlation Explanation","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pdz9n/discovering_structure_in_highdimensional_data/","ups":16,"retrieved_on":1441018360,"selftext":"The paper:\nhttp://papers.nips.cc/paper/5580-discovering-structure-in-high-dimensional-data-through-correlation-explanation\n\nCode on GitHub:\nhttps://github.com/gregversteeg/CorEx\n\nSome demos:\nhttp://www.isi.edu/people/gregv/preliminary_corex_visualizations","media":null}
{"author_flair_text":null,"created_utc":"1418673184","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"mrtz.org","thumbnail":"http://b.thumbs.redditmedia.com/SwJCO4w1_gkZr5pno66aH4QBLpLmc2c2-RfLEp-o-vw.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"The NIPS Experiment: Half the papers appearing at NIPS would be rejected if the review process were rerun","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pe154/the_nips_experiment_half_the_papers_appearing_at/","selftext":"","media":null,"ups":75,"retrieved_on":1441018335,"num_comments":20,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2pe154","link_flair_css_class":null,"gilded":0,"id":"2pe154","archived":true,"from":null,"author":"urish","is_self":false,"from_id":null,"from_kind":null,"created":1418673184,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://mrtz.org/blog/the-nips-experiment/","stickied":false,"score":75,"edited":false,"quarantine":false}
{"distinguished":null,"downs":0,"thumbnail":"default","title":"Machine Learning introduction question","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418676149","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"I've only just started learning Python but it's going to be some time before I can really produce anything with it.  I was wondering if anyone has a downloadable program already that maybe I could feed it some images or something see something like Deeplearning in action on our own systems?  I'm just super excited about eventually learning how to teach algorithms like this.  So something I can use to spark ideas would be really cool! ","retrieved_on":1441018254,"ups":1,"permalink":"/r/MachineLearning/comments/2pe7f2/machine_learning_introduction_question/","link_flair_text":null,"author":"[deleted]","from":null,"is_self":true,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2pe7f2","link_flair_css_class":null,"name":"t3_2pe7f2","gilded":0,"quarantine":false,"edited":false,"score":1,"url":"http://www.reddit.com/r/MachineLearning/comments/2pe7f2/machine_learning_introduction_question/","secure_media":null,"created":1418676149,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false}
{"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pedw5/any_expertise_in_adaptive_resonance_theory_art/","from_kind":null,"created":1418679261,"subreddit_id":"t5_2r3gv","is_self":true,"from":null,"author":"ownallogist","name":"t3_2pedw5","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2pedw5","num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"ups":0,"retrieved_on":1441018170,"selftext":"Hi all,\n\nBeginning to work on a project with a professor regarding the Adaptive Resonance Theory, particularly ART 3. My role is to develop a model using python as the main language.\n\nAside from wikipedia and a few research papers here and there, I am having difficulty:\n\n1) Completely understanding the concept\n2) Finding anything code related\n\nWould anyone happen to have some expertise in this area? \n\n\nThanks in advance!\n","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pedw5/any_expertise_in_adaptive_resonance_theory_art/","title":"Any expertise in adaptive resonance theory (ART)?","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418679261","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"secure_media_embed":{},"title":"Text Analytics Sandbox - Like JSfiddlefor Text Analytics","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"blog.aylien.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418684089","retrieved_on":1441018039,"ups":1,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2penzj/text_analytics_sandbox_like_jsfiddlefor_text/","link_flair_text":null,"is_self":false,"author":"[deleted]","from":null,"archived":true,"id":"2penzj","link_flair_css_class":null,"name":"t3_2penzj","gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":1,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418684089,"from_kind":null,"secure_media":null,"url":"http://blog.aylien.com/post/105284602153/aylien-text-analysis-developer-sandbox-is-live?utm_source=Social&amp;utm_medium=Reddit&amp;utm_campaign=Sandbox%20Promo","from_id":null}
{"score":0,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"https://developer.aylien.com/sandbox?utm_source=Social&amp;utm_medium=Reddit&amp;utm_campaign=Sandbox%20Promo","from_kind":null,"created":1418684477,"subreddit_id":"t5_2r3gv","is_self":false,"from":null,"author":"MikeWally","link_flair_css_class":null,"gilded":0,"name":"t3_2peovb","id":"2peovb","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"ups":0,"retrieved_on":1441018028,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2peovb/aylien_text_analytics_sandbox_like_jsfiddle_for/","title":"AYLIEN Text Analytics Sandbox - Like JSFiddle for Text Analytics","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"developer.aylien.com","hide_score":false,"created_utc":"1418684477","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pevfe","archived":true,"id":"2pevfe","from":null,"author":"jubalince","is_self":false,"from_id":null,"from_kind":null,"created":1418687778,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://blogs.workday.com/profile_mohammad_sabah_director_of_data_science_at_workday.html","stickied":false,"edited":false,"score":0,"quarantine":false,"author_flair_text":null,"created_utc":"1418687778","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"blogs.workday.com","thumbnail":"http://b.thumbs.redditmedia.com/r-D9Yd5W-P3ExqRM_bzuVc0KyTi1IHkuML3iUdSzkiM.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Profile: Mohammad Sabah - Director of Data Science at Workday","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pevfe/profile_mohammad_sabah_director_of_data_science/","selftext":"","media":null,"ups":0,"retrieved_on":1441017943}
{"secure_media_embed":{},"title":"Where can I find implementation/tutorial of Knowledge based recommender system?","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418698439","retrieved_on":1441017681,"ups":2,"media":null,"selftext":"Hey,\nI'm trying to find an implementation of a knowledge based or demographic recommender but all I have found so far are descriptions of what they are, not an example or tutorial which is what I'm looking for.  Any help?  Thanks.","permalink":"/r/MachineLearning/comments/2pffna/where_can_i_find_implementationtutorial_of/","link_flair_text":null,"is_self":true,"author":"schmity","from":null,"archived":true,"id":"2pffna","name":"t3_2pffna","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":2,"stickied":false,"subreddit_id":"t5_2r3gv","created":1418698439,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pffna/where_can_i_find_implementationtutorial_of/","secure_media":null,"from_id":null}
{"media":null,"selftext":"Hi, I would like to learn wether there is an overlap between the fileds of complex event processing and machine learning","retrieved_on":1441017324,"ups":1,"permalink":"/r/MachineLearning/comments/2pg77r/complex_event_processing_and_machine_learning/","link_flair_text":null,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Complex event processing and Machine Learning...","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418715771","hide_score":false,"domain":"self.MachineLearning","quarantine":false,"edited":false,"score":1,"subreddit_id":"t5_2r3gv","created":1418715771,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pg77r/complex_event_processing_and_machine_learning/","secure_media":null,"from_id":null,"stickied":false,"author":"__null__","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":3,"archived":true,"id":"2pg77r","link_flair_css_class":null,"gilded":0,"name":"t3_2pg77r"}
{"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pgb8k/do_you_think_that_presentday_machine_learning/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418719573,"stickied":false,"edited":1418721774,"score":4,"quarantine":false,"num_comments":42,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2pgb8k","gilded":0,"id":"2pgb8k","archived":true,"from":null,"author":"[deleted]","is_self":true,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pgb8k/do_you_think_that_presentday_machine_learning/","selftext":"What I mean is that even if say Elon Musk develops liquid rockets rest of his life, that paradigm will never lead to faster than light travel. To accomplish that you need something completely different like warp drive or wormhole technology.\n\nSo do you think that just developing better deep learning algorithms etc will lead to Strong AI or do we need something completely new like from going from liquid rockets to warp drive?","media":null,"ups":4,"retrieved_on":1441017271,"created_utc":"1418719573","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Do you think that present-day machine learning will lead to Strong AI or is completely new paradigm needed?","secure_media_embed":{}}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pgbl9/error_when_using_multiple_taps_using_scan_in/","selftext":"","media":null,"ups":0,"retrieved_on":1441017267,"created_utc":"1418719959","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"stackoverflow.com","hide_score":false,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/aXYrRkeCvV2XLxkjJ0IlzqDQKfJmSxIod_GIKvDSNl0.jpg","distinguished":null,"title":"Error when using multiple taps using Scan in Theano","secure_media_embed":{},"from_id":null,"url":"http://stackoverflow.com/questions/27484484/error-when-using-multiple-taps-using-scan-in-theano","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418719959,"stickied":false,"score":0,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":1,"saved":false,"over_18":false,"name":"t3_2pgbl9","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2pgbl9","from":null,"author":"TheAlienDude","is_self":false}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418737796","author_flair_text":null,"domain":"machinedlearnings.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/9lSxjLpFl6kATbYGzyrVzUJ088b7m4yvFXeZ39zUsbE.jpg","title":"Conference report NIPS'14","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2pgurl/conference_report_nips14/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441017018,"ups":16,"saved":false,"over_18":false,"num_comments":3,"author_flair_css_class":null,"id":"2pgurl","archived":true,"name":"t3_2pgurl","link_flair_css_class":null,"gilded":0,"author":"cast42","from":null,"is_self":false,"url":"http://www.machinedlearnings.com/2014/12/nips-2014.html","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418737796,"from_id":null,"stickied":false,"quarantine":false,"score":16,"edited":false}
{"score":12,"edited":false,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1418744706,"from_kind":null,"url":"http://www.startup.ml/blog/2014/9/21/what-every-machine-learning-package-can-learn-from-vowpal-wabbit","secure_media":null,"stickied":false,"from":null,"author":"gwulfs","is_self":false,"num_comments":8,"author_flair_css_class":null,"over_18":false,"saved":false,"gilded":0,"name":"t3_2ph5tm","link_flair_css_class":null,"archived":true,"id":"2ph5tm","selftext":"","media":null,"ups":12,"retrieved_on":1441016815,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ph5tm/what_every_machine_learning_package_can_learn/","thumbnail":"http://b.thumbs.redditmedia.com/0ZZ9GcltpAmXm38KwJoCV7KSoAxWLFs-lXrP198D6NQ.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"What every machine learning package can learn from Vowpal Wabbit","author_flair_text":null,"created_utc":"1418744706","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"startup.ml"}
{"is_self":true,"from":null,"author":"[deleted]","gilded":0,"link_flair_css_class":null,"name":"t3_2phjfq","id":"2phjfq","archived":true,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"score":1,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"created":1418751631,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2phjfq/video_for_paper_deep_neural_networks_are_easily/","secure_media":null,"secure_media_embed":{},"title":"Video for paper: Deep Neural Networks are Easily Fooled.","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418751631","subreddit":"MachineLearning","media_embed":{},"ups":1,"retrieved_on":1441016639,"selftext":"Anh Nguyen made a video introducing a paper that was [discussed](https://www.reddit.com/r/MachineLearning/comments/2onzmd/deep_neural_networks_are_easily_fooled_high/) on r/MachineLearning a few days ago.\n\nHere's the paper link in case anyone is interested:\nhttp://arxiv.org/abs/1412.1897","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2phjfq/video_for_paper_deep_neural_networks_are_easily/"}
{"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/B8olZbwBhapoYZoN_T10iI4gfXfhDa3jPEhE12m3q0o.jpg","distinguished":null,"title":"Video explaining paper: Deep Neural Networks are Easily Fooled","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FM2IebCN9Ht4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DM2IebCN9Ht4&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FM2IebCN9Ht4%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"created_utc":"1418752259","author_flair_text":null,"media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FM2IebCN9Ht4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DM2IebCN9Ht4&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FM2IebCN9Ht4%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":338,"scrolling":false},"subreddit":"MachineLearning","domain":"youtube.com","hide_score":false,"selftext":"","media":{"oembed":{"provider_url":"http://www.youtube.com/","provider_name":"YouTube","thumbnail_width":480,"description":"A video summary of the paper: Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images.\" arXiv preprint arXiv:1412.1897 (2014). The paper is available here: http://EvolvingAI.org/fooling Special thanks to those who created the music, images, videos and software that were used to create this video.","title":"Deep Neural Networks are Easily Fooled","width":600,"thumbnail_url":"http://i.ytimg.com/vi/M2IebCN9Ht4/hqdefault.jpg","thumbnail_height":360,"type":"video","url":"http://www.youtube.com/watch?v=M2IebCN9Ht4","author_url":"http://www.youtube.com/user/JeffCluneResearch","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FM2IebCN9Ht4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DM2IebCN9Ht4&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FM2IebCN9Ht4%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"Evolving AI Lab","height":338},"type":"youtube.com"},"ups":93,"retrieved_on":1441016621,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2phkrd/video_explaining_paper_deep_neural_networks_are/","from":null,"author":"JasonYosinski","is_self":false,"author_flair_css_class":null,"num_comments":36,"saved":false,"over_18":false,"name":"t3_2phkrd","link_flair_css_class":null,"gilded":0,"id":"2phkrd","archived":true,"score":93,"edited":false,"quarantine":false,"from_id":null,"url":"https://www.youtube.com/watch?v=M2IebCN9Ht4","secure_media":{"oembed":{"author_url":"http://www.youtube.com/user/JeffCluneResearch","url":"http://www.youtube.com/watch?v=M2IebCN9Ht4","type":"video","thumbnail_height":360,"height":338,"author_name":"Evolving AI Lab","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FM2IebCN9Ht4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DM2IebCN9Ht4&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FM2IebCN9Ht4%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","provider_url":"http://www.youtube.com/","provider_name":"YouTube","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FM2IebCN9Ht4%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","title":"Deep Neural Networks are Easily Fooled","description":"A video summary of the paper: Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images.\" arXiv preprint arXiv:1412.1897 (2014). The paper is available here: http://EvolvingAI.org/fooling Special thanks to those who created the music, images, videos and software that were used to create this video.","thumbnail_width":480},"type":"youtube.com"},"created":1418752259,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false}
{"created_utc":"1418752641","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"aka.ms","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg","distinguished":null,"title":"Read John Platt's view on key ML trends seen at NIPS 2014 at Montreal","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2phlk9/read_john_platts_view_on_key_ml_trends_seen_at/","selftext":"","media":null,"ups":3,"retrieved_on":1441016611,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"name":"t3_2phlk9","link_flair_css_class":null,"gilded":0,"id":"2phlk9","archived":true,"from":null,"author":"MLBlogTeam","is_self":false,"from_id":null,"secure_media":null,"url":"http://aka.ms/o0ery2","created":1418752641,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"edited":false,"score":3,"quarantine":false}
{"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418775768","secure_media_embed":{},"title":"[FP-Growth process] Help! Rapidminer is driving me mental","distinguished":null,"thumbnail":"self","downs":0,"permalink":"/r/MachineLearning/comments/2piwp2/fpgrowth_process_help_rapidminer_is_driving_me/","link_flair_text":null,"retrieved_on":1441016000,"ups":0,"media":null,"selftext":"Alright so I'm trying to do this simple FP-Growth process on Rapidminer 5.3. \n\nIt's really straight forward copy of this:\n\nhttps://www.youtube.com/watch?v=oXrUz5CWM4E\n\nMy source files are txt files - average length 24 pages. Around 173 KB.\n\nI'm doing this on just one text file. Now I'm running an i7 six core processor with 16GB of RAM on Win 8.1\n\nWhat it does is - start the process and then keeps running until it runs out of memory.  Has anyone had this problem and how have you solved it? \n\nPS- I went on the forum, tried all the solutions, no joy. \n\nPPS - not an advanced user so I haven't gone too far beyond this video aside from trying to use \"Memory Materialise\" and \"Memory Clear\"  which does exactly the same. \n\nApologies for being a klutz and thanks in advance. ","archived":true,"id":"2piwp2","name":"t3_2piwp2","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"is_self":true,"author":"D-Hex","from":null,"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418775768,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2piwp2/fpgrowth_process_help_rapidminer_is_driving_me/","from_id":null,"quarantine":false,"score":0,"edited":false}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418799942","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Favorite Machine Learning Books?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2pk137/favorite_machine_learning_books/","link_flair_text":null,"media":null,"selftext":"I'm looking to pick up some hard copies of machine learning resources, do you guys have any favorites in mind? Since the field moves so fast, I doubt there's any good hard cover books for the newest deep learning techniques, but fundamentals in statistics are always good. Also, if anyone has a favorite resource book for image classification that'd be great too.","retrieved_on":1441015477,"ups":18,"saved":false,"over_18":false,"num_comments":13,"author_flair_css_class":null,"archived":true,"id":"2pk137","link_flair_css_class":null,"name":"t3_2pk137","gilded":0,"author":"matlab484","from":null,"is_self":true,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pk137/favorite_machine_learning_books/","created":1418799942,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":18}
{"permalink":"/r/MachineLearning/comments/2pkeao/a_reliable_source_of_comments_on_machine_learning/","link_flair_text":null,"media":null,"selftext":"I'm sure we all faced the challenge of going thought the same classic (and not so classic) papers on Machine Learning and read them, fail to understand them because there are some bits of information missing: comments. Some papers are difficult to understand (maybe the explanation is not exactly clear), some can have small (or even big!) mistakes, some can have missing information to understand them.\nMy question is: is there any reliable collaborative source of comments on Machine Learning papers (even maybe with comments per sentence or formula). It can be a blog, a wiki or a subreddit.","retrieved_on":1441015306,"ups":11,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418812904","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"A reliable source of comments on Machine Learning papers?","created":1418812904,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pkeao/a_reliable_source_of_comments_on_machine_learning/","from_id":null,"stickied":false,"quarantine":false,"score":11,"edited":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"id":"2pkeao","archived":true,"link_flair_css_class":null,"name":"t3_2pkeao","gilded":0,"author":"galapag0","from":null,"is_self":true}
{"created_utc":"1418818460","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"jongomez.github.io","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/ZO8-JzR-IGf7quwoQQ84M3JkhgBE-UvhOYpd4lnTYhQ.jpg","distinguished":null,"title":"Hey guys, I'm working on an intelligent bot that runs on your browser. I'd love to hear what you think about it so far.","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pkjpp/hey_guys_im_working_on_an_intelligent_bot_that/","selftext":"","media":null,"ups":1,"retrieved_on":1441015235,"author_flair_css_class":null,"num_comments":4,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2pkjpp","gilded":0,"id":"2pkjpp","archived":true,"from":null,"author":"jonGomez","is_self":false,"from_id":null,"url":"http://jongomez.github.io/samurai/","secure_media":null,"created":1418818460,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"edited":false,"score":1,"quarantine":false}
{"author":"pl47","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":5,"archived":true,"id":"2pkkf4","name":"t3_2pkkf4","link_flair_css_class":null,"gilded":0,"quarantine":false,"score":2,"edited":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418819122,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pkkf4/is_adaboost_in_sklearn_just_gradient_boosting_but/","from_id":null,"stickied":false,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Is adaboost in sklearn just gradient boosting but with an exponential loss function ?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418819122","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hi,\n\n\nReading about adaboost it says that it has an exponential loss function.\nThe sklearn version one can edit the loss functions, \nthis is something only gradient boosting could do. \nSo what is going on ? \n\n","retrieved_on":1441015226,"ups":2,"permalink":"/r/MachineLearning/comments/2pkkf4/is_adaboost_in_sklearn_just_gradient_boosting_but/","link_flair_text":null}
{"selftext":"","media":null,"ups":67,"retrieved_on":1441015181,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pknvo/tutorial_using_convolutional_neural_nets_to/","thumbnail":"http://b.thumbs.redditmedia.com/BmcP8kTIy3jKAeFC2M_04p5LrufUOJEh_ZkR6i1Oxjk.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Tutorial: Using convolutional neural nets to detect facial keypoints (based on Python and theano).","author_flair_text":null,"created_utc":"1418822078","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"danielnouri.org","score":67,"edited":false,"quarantine":false,"from_id":null,"from_kind":null,"created":1418822078,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/","stickied":false,"from":null,"author":"ogrisel","is_self":false,"author_flair_css_class":null,"num_comments":9,"over_18":false,"saved":false,"name":"t3_2pknvo","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2pknvo"}
{"ups":3,"retrieved_on":1441015118,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pkssu/a_new_multilayer_model_for_hierarchical_temporal/","secure_media_embed":{},"title":"A New Multilayer Model for Hierarchical Temporal Memory","thumbnail":"http://b.thumbs.redditmedia.com/dkxsFR3ppiVJqSAlW8YpwemUQ3RTRaGbmZLqI__TvdU.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"inbits.com","author_flair_text":null,"created_utc":"1418825847","subreddit":"MachineLearning","media_embed":{},"score":3,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"created":1418825847,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://inbits.com/2014/12/multilayer-model-for-hierarchical-temporal-memory/","is_self":false,"from":null,"author":"fergbyrne","gilded":0,"link_flair_css_class":null,"name":"t3_2pkssu","archived":true,"id":"2pkssu","num_comments":18,"author_flair_css_class":null,"over_18":false,"saved":false}
{"selftext":"","media":{"oembed":{"thumbnail_height":360,"type":"video","url":"http://www.youtube.com/watch?v=t4kyRyKyOpo","author_url":"http://www.youtube.com/user/TEDtalksDirector","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"TED","height":338,"provider_url":"http://www.youtube.com/","provider_name":"YouTube","thumbnail_width":480,"description":"What happens when we teach a computer how to learn? Technologist Jeremy Howard shares some surprising new developments in the fast-moving field of deep learning, a technique that can give computers the ability to learn Chinese, or to recognize objects in photos, or to help think through a medical diagnosis.","title":"Jeremy Howard: The wonderful and terrifying implications of computers that can learn","width":600,"thumbnail_url":"http://i.ytimg.com/vi/t4kyRyKyOpo/hqdefault.jpg"},"type":"youtube.com"},"ups":0,"retrieved_on":1441015082,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pkvic/jeremy_howard_the_wonderful_and_terrifying/","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{"scrolling":false,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600},"title":"Jeremy Howard: The wonderful and terrifying implications of computers that can learn","author_flair_text":null,"created_utc":"1418827637","subreddit":"MachineLearning","media_embed":{"scrolling":false,"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"hide_score":false,"domain":"youtube.com","score":0,"edited":false,"quarantine":false,"from_id":null,"created":1418827637,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":{"oembed":{"type":"video","url":"http://www.youtube.com/watch?v=t4kyRyKyOpo","author_url":"http://www.youtube.com/user/TEDtalksDirector","thumbnail_height":360,"author_name":"TED","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","provider_name":"YouTube","provider_url":"http://www.youtube.com/","title":"Jeremy Howard: The wonderful and terrifying implications of computers that can learn","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","thumbnail_width":480,"description":"What happens when we teach a computer how to learn? Technologist Jeremy Howard shares some surprising new developments in the fast-moving field of deep learning, a technique that can give computers the ability to learn Chinese, or to recognize objects in photos, or to help think through a medical diagnosis."},"type":"youtube.com"},"url":"https://www.youtube.com/watch?v=t4kyRyKyOpo","stickied":false,"from":null,"author":"CaptainHoek","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2pkvic","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2pkvic"}
{"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418829222","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"What do you enjoy about machine learning?","thumbnail":"self","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pky4b/what_do_you_enjoy_about_machine_learning/","ups":2,"retrieved_on":1441015049,"selftext":"I'm a student and have just been introduced to the basic concepts of machine learning. I have taken a CS course on machine learning as well as a Cognitive Science course about neural models.\n\nI can really tell that I'm just barely scratching the surface right now. My only motivation to keep learning about it is basically that I want to understand it at a higher level. Curiosities I have about the subject are usually left unexplored because of my inability to approach them. The first thing that got me interested in machine learning was [this video](https://www.youtube.com/watch?v=bBt0imn77Zg).\n\nSo these are my questions for you (and myself):\n\n\n1. Do you have anything that you would like to explore with machine learning? What is it that you are curious about?\n\n\n2. What do you enjoy the most about machine learning?\n\nI know to some people my questions may come across as childish but I really find these questions important. I don't like having my curiosity constrained by a syllabus. Even if you think I shouldn't consider these questions I would value even a brief explanation. \n\nMy Answers:\n\n1. I find that stochastic exploration for solutions yields interesting results. I would also like to know if there is any application for having several unique perspectives on how to solve a problem.\n\n2. I don't know much about it yet but I feel that I will be able to use it to gain a better understanding of the world.","media":null,"link_flair_css_class":null,"gilded":0,"name":"t3_2pky4b","id":"2pky4b","archived":true,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"AfraidOfToasters","stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1418829222,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pky4b/what_do_you_enjoy_about_machine_learning/","edited":false,"score":2,"quarantine":false}
{"author":"GibbsSamplePlatter","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":4,"author_flair_css_class":null,"archived":true,"id":"2pl5ty","gilded":0,"link_flair_css_class":null,"name":"t3_2pl5ty","quarantine":false,"edited":false,"score":3,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pl5ty/deep_neural_nets_discovering_novel_features/","from_kind":null,"created":1418833500,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Deep neural nets discovering novel features?","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418833500","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"Do we have any good examples of deep neural networks capturing novel features that are then fed back into the science community?  \n\nI vaguely recall something about cancer detection net finding out that surrounding healthy cells are just as important for detection, but can't find the link.  \n\nAny others are appreciated too. ","retrieved_on":1441014949,"ups":3,"permalink":"/r/MachineLearning/comments/2pl5ty/deep_neural_nets_discovering_novel_features/","link_flair_text":null}
{"media":null,"selftext":"","retrieved_on":1441014880,"ups":1,"permalink":"/r/MachineLearning/comments/2plb4d/towards_deep_neural_network_architectures_robust/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Towards Deep Neural Network Architectures Robust to Adversarial Examples [it's amusing how many people are selling \"adversarial examples\" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure]","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418836254","author_flair_text":null,"domain":"arxiv.org","hide_score":false,"quarantine":false,"edited":false,"score":1,"url":"http://arxiv.org/abs/1412.5068","secure_media":null,"created":1418836254,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"[deleted]","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"id":"2plb4d","archived":true,"name":"t3_2plb4d","gilded":0,"link_flair_css_class":null}
{"secure_media_embed":{},"title":"How to best visualize breast cancer data","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418836392","retrieved_on":1441014877,"ups":0,"media":null,"selftext":"I'm playing around with the wisconsin breast cancer data that predicts malignant or benign. \n\nI'm can't figure out which visualization to use to form my Hypothesis. I want my hypothesis to be something like \"which attribute would have the most effect on predicting the class variable\". \n\nI made a scatterplot matrix for this data but I think it doesn't show good visualization from which I can make manual predictions by just looking at the image. \n\nWhich graph/visualization should I use to extract some meaningful information from the data by just visually looking at it?\n\nhttps://dl.dropboxusercontent.com/s/d6jg7jwpb75erq3/Rplot14.png?dl=0\n\n\n     &gt; library(foreign)\n\n     &gt; breast &lt;- read.arff(\"http://www.cs.iastate.edu/~cs573x/labs/lab1/breast-cancer-   \n           wisconsin.arff\")\n\n      &gt; breast$class &lt;- as.numeric(as.character(breast$class))\n","permalink":"/r/MachineLearning/comments/2plbe6/how_to_best_visualize_breast_cancer_data/","link_flair_text":null,"is_self":true,"author":"omnipresent101","from":null,"id":"2plbe6","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2plbe6","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"score":0,"edited":false,"stickied":false,"created":1418836392,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2plbe6/how_to_best_visualize_breast_cancer_data/","secure_media":null,"from_id":null}
{"edited":false,"score":0,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418838024,"secure_media":null,"url":"http://blog.adammenges.com/the-philosophy-behind-headline/","stickied":false,"from":null,"author":"therealadammenges","is_self":false,"author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"name":"t3_2plelx","link_flair_css_class":null,"gilded":0,"id":"2plelx","archived":true,"selftext":"","media":null,"ups":0,"retrieved_on":1441014835,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2plelx/headline_machine_learning_hackernews_reddit_pocket/","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Headline = Machine Learning + Hackernews + Reddit + Pocket","author_flair_text":null,"created_utc":"1418838024","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"blog.adammenges.com"}
{"from":null,"author":"impossibru_ML","is_self":true,"num_comments":7,"author_flair_css_class":null,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2plofg","id":"2plofg","archived":true,"edited":false,"score":44,"quarantine":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2plofg/impossibru_meme_is_a_face_in_the_toronto_faces/","secure_media":null,"created":1418842865,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"\"Impossibru\" meme is a face in the Toronto Faces Dataset","secure_media_embed":{},"created_utc":"1418842865","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"I was looking through the 0-fold test set of the Toronto Faces Dataset and saw [this face](http://i.imgur.com/NDY6W4K.jpg) looking at me. His expression is \"disgust.\"\n\nThe image on the right is from the TFD. How did this happen?","media":null,"ups":44,"retrieved_on":1441014708,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2plofg/impossibru_meme_is_a_face_in_the_toronto_faces/"}
{"downs":0,"thumbnail":"default","distinguished":null,"title":"Machine Learning Trends from NIPS 2014, blog post by John Platt","secure_media_embed":{},"created_utc":"1418843413","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"blogs.technet.com","hide_score":false,"selftext":"","media":null,"ups":1,"retrieved_on":1441014694,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2plpgq/machine_learning_trends_from_nips_2014_blog_post/","from":null,"author":"[deleted]","is_self":false,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"gilded":0,"name":"t3_2plpgq","link_flair_css_class":null,"id":"2plpgq","archived":true,"edited":false,"score":1,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://blogs.technet.com/b/machinelearning/archive/2014/12/16/machine-learning-trends-from-nips-2014.aspx","created":1418843413,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false}
{"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"id":"2plrfo","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2plrfo","author":"[deleted]","from":null,"is_self":false,"secure_media":null,"url":"http://arxiv.org/abs/1412.5068","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418844381,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":1,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418844381","author_flair_text":null,"domain":"arxiv.org","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Towards Deep Neural Network Architectures Robust to Adversarial Examples [it's amusing how many people are selling \"adversarial examples\" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure]","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2plrfo/towards_deep_neural_network_architectures_robust/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441014669,"ups":1}
{"permalink":"/r/MachineLearning/comments/2pls8q/towards_dnn_architectures_robust_to_adversarial/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441014658,"ups":1,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418844796","author_flair_text":null,"domain":"arxiv.org","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Towards DNN Architectures Robust to Adversarial Examples - it's amusing how many people are selling \"adversarial examples\" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure","secure_media_embed":{},"secure_media":null,"url":"http://arxiv.org/abs/1412.5068","subreddit_id":"t5_2r3gv","created":1418844796,"from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":1,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2pls8q","name":"t3_2pls8q","gilded":0,"link_flair_css_class":null,"author":"[deleted]","from":null,"is_self":false}
{"quarantine":false,"edited":false,"score":1,"secure_media":null,"url":"http://arxiv.org/abs/1412.5068","subreddit_id":"t5_2r3gv","created":1418845006,"from_kind":null,"from_id":null,"stickied":false,"author":"[deleted]","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2plsn7","link_flair_css_class":null,"gilded":0,"name":"t3_2plsn7","media":null,"selftext":"","retrieved_on":1441014653,"ups":1,"permalink":"/r/MachineLearning/comments/2plsn7/towards_dnn_architectures_robust_to_adversarial/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Towards DNN Architectures Robust to Adversarial Examples - it's amusing how many people are using \"adversarial examples\" as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418845006","author_flair_text":null,"domain":"arxiv.org","hide_score":false}
{"ups":5,"retrieved_on":1441014648,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2plt23/towards_dnn_architectures_robust_to_adversarial/","title":"Towards DNN Architectures Robust to Adversarial Examples - there are some who are exploiting adversarial examples as an insurmountable weakness of neural nets for computer vision; nope, it's just a temporary anomaly to be overcome by a new model or training procedure","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"arxiv.org","hide_score":false,"created_utc":"1418845217","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","edited":false,"score":5,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://arxiv.org/abs/1412.5068","subreddit_id":"t5_2r3gv","created":1418845217,"from_kind":null,"is_self":false,"from":null,"author":"test3545","gilded":0,"name":"t3_2plt23","link_flair_css_class":null,"archived":true,"id":"2plt23","num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false}
{"domain":"blog.yhathq.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418850978","author_flair_text":null,"title":"Reducing your R memory footprint by 7000x","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","permalink":"/r/MachineLearning/comments/2pm51i/reducing_your_r_memory_footprint_by_7000x/","link_flair_text":null,"retrieved_on":1441014492,"ups":0,"media":null,"selftext":"","id":"2pm51i","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2pm51i","saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"is_self":false,"author":"hernamesbarbara","from":null,"stickied":false,"secure_media":null,"url":"http://blog.yhathq.com/posts/reducing-your-r-memory-footprint-by-7000x.html","created":1418850978,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"quarantine":false,"score":0,"edited":false}
{"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Cluster Validity Indices (CVI) in Python","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418896950","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hi.\n\nI've been working on the implementation of some CVIs that I needed for a project because sklearn only has Silhouete score implemented. I currently have Dunn and Davis Bouldin indices.\n\nhttps://github.com/actjqm/jqm_cvi\n\nI used this opportunity to try to learn Cython. I would greatly appreciate tips and comments on my .pyx.\n\nTell me if i should post this to /r/python instead. Posted here because I have the impression there are a lot o Python enthusiasts around here.\n\n(sorry for functions with different inputs)","retrieved_on":1441013546,"ups":0,"permalink":"/r/MachineLearning/comments/2po61b/cluster_validity_indices_cvi_in_python/","link_flair_text":null,"author":"actjqm","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2po61b","link_flair_css_class":null,"gilded":0,"name":"t3_2po61b","quarantine":false,"score":0,"edited":false,"subreddit_id":"t5_2r3gv","created":1418896950,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2po61b/cluster_validity_indices_cvi_in_python/","secure_media":null,"from_id":null,"stickied":false}
{"title":"How to do pixel accurate analysis of a user interface (classification of \"clickable\" for example) ?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418898779","author_flair_text":null,"retrieved_on":1441013523,"ups":0,"media":null,"selftext":"How can we do \"segmentation\" on a UI to find buttons or clickable areas ?\n\nThe traditional sliding window is great to find if the picture contains an item of interest.\n\nHow do we do this for pixel accurate detection of a button ? We use a sliding sliding window that only says \"true\" if the pixel at the center of the picture is \"clickable\" ?\n\nIs this done with several levels of zoom, to first find large areas that contains clickable parts, then with smaller sliding windows in the clickable areas to find smaller areas and ultimately pixel accurate classification ?","permalink":"/r/MachineLearning/comments/2po7tw/how_to_do_pixel_accurate_analysis_of_a_user/","link_flair_text":null,"is_self":true,"author":"Schlagv","from":null,"archived":true,"id":"2po7tw","gilded":0,"link_flair_css_class":null,"name":"t3_2po7tw","saved":false,"over_18":false,"num_comments":4,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":0,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2po7tw/how_to_do_pixel_accurate_analysis_of_a_user/","secure_media":null,"created":1418898779,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null}
{"from":null,"author":"blkorcut","is_self":true,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2po8ww","archived":true,"id":"2po8ww","score":11,"edited":1418900505,"quarantine":false,"from_id":null,"from_kind":null,"created":1418899899,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2po8ww/rml_ive_hit_a_wall_trying_to_code_a_simple_rnn/","stickied":false,"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"r/ML, i've hit a wall trying to code a simple RNN, please help!","author_flair_text":null,"created_utc":"1418899899","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"Hey guys,\n\nI've come across a really crazy bug happening to me in coding backpropagation for an RNN in pure python. I'm comparing my implementation's results against an implementation from theano (which is much easier to code as there is no backpropagation to figure out). The weirdest bug is that I'm getting the last row of weights correct for the input -&gt; hidden layer connections, but the rest are incorrect. I.e, I'm correctly figuring out the derivative of the weights from the 5th unit of the first layer to each unit in the hidden layer, but none of the rest. All info is here:\n\nhttp://stackoverflow.com/questions/27544698/pure-python-rnn-and-theano-rnn-computing-different-gradients-code-and-results\n\n(and note that in the code df is the derivative of the hidden activation function f)\n\nI'm totally blocked here, can't figure it out for the life of me. I really thought my backpropagation code is correct, but there must be an issue there. I'm doing gradient checks too and theano's implementation is computing the correct gradient. Really, really not sure what's happening with the last row of weights for input-&gt;hidden, though, that really makes no sense to me.\n\nYoure my last hope r/ML!","media":null,"ups":11,"retrieved_on":1441013509,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2po8ww/rml_ive_hit_a_wall_trying_to_code_a_simple_rnn/"}
{"permalink":"/r/MachineLearning/comments/2poxpx/deepspeech_scaling_up_endtoend_speech_recognition/","link_flair_text":null,"retrieved_on":1441013128,"ups":50,"media":null,"selftext":"Research at Baidu for a best of its class spectrum-to-text RNN speech recognition highlighted also [here](http://venturebeat.com/2014/12/18/baidu-researchers-beef-up-the-search-giants-speech-recognition-savvy/).\n\nEDIT actual [link](http://arxiv-web3.library.cornell.edu/abs/1412.5567).\nEDIT2 allegedly better results reported by ibm watson group, see comments.","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418918728","author_flair_text":null,"title":"DeepSpeech: Scaling up end-to-end speech recognition","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2poxpx/deepspeech_scaling_up_endtoend_speech_recognition/","subreddit_id":"t5_2r3gv","created":1418918728,"from_kind":null,"from_id":null,"quarantine":false,"edited":1418945118,"score":50,"id":"2poxpx","archived":true,"name":"t3_2poxpx","gilded":0,"link_flair_css_class":null,"saved":false,"over_18":false,"num_comments":19,"author_flair_css_class":null,"is_self":true,"author":"jesuslop","from":null}
{"selftext":"","media":null,"ups":1,"retrieved_on":1441013078,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pp1ib/do_we_need_hundreds_of_classifiers_to_solve_real/","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?","author_flair_text":null,"created_utc":"1418920805","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"jmlr.org","edited":false,"score":1,"quarantine":false,"from_id":null,"created":1418920805,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf","secure_media":null,"stickied":false,"from":null,"author":"[deleted]","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pp1ib","archived":true,"id":"2pp1ib"}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1418932212,"url":"http://www.reddit.com/r/MachineLearning/comments/2ppojv/implementing_a_deep_lstmrnn_\u00e0_la_sequence_to/","secure_media":null,"edited":false,"score":13,"quarantine":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2ppojv","archived":true,"id":"2ppojv","num_comments":19,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"qurun","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ppojv/implementing_a_deep_lstmrnn_\u00e0_la_sequence_to/","ups":13,"retrieved_on":1441012780,"selftext":"I've been looking into \"Sequence to Sequence Learning with Neural Networks\" by Sutskever, Vinyals and Le ([arXiv:1409.3215](http://arxiv.org/abs/1409.3215)), and would like to understand how/why they implement a deep (4-layer) Long-Short Term Memory (LSTM) network.  \n\nThey do not explain the details of how they tie together the network, but from following the references I get the impression that they are using a connection structure like this: http://i.imgur.com/J3DwxSF.png (from \"Hybrid speech recognition with deep bidirectional LSTM\" http://www.cs.toronto.edu/~graves/asru_2013.pdf ), except using LSTM units.  \n\nIf they were using a deep RNN, though, I would think that a connection like this would work better: http://i.imgur.com/9txOrbN.png .  The difference is that I feed the output of the whole network into the input at the next time step, instead of feeding each layer's outputs into the same layer's inputs at the next time step.  \n\nWhy do they do it this way?  \n\nAlso, is there a good, modern reference that compares and contrasts approaches to recurrent neural networks?  For example, I would like to know: \n\n* Why are these special memory units better than a simple (possibly deep) recurrent neural network (RNN)?  \n\n* Is there any good understanding of the difference between LSTM units and the simpler memory units introduced in \"Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation\" ([arXiv:1406.1078](http://arxiv.org/abs/1406.1078))?  It seems like this paper gets good results even with a shallow network (they use only one layer of hidden units). \n\n* Do any neural network packages have good built-in support for RNNs and these variants?  \nSo far, I have found LISA Groundhog https://github.com/lisa-groundhog/GroundHog and some simpler theano examples (like https://github.com/gwtaylor/theano-rnn).  \nIn torch7, nnx has a simple recurrent module (https://github.com/clementfarabet/lua---nnx#nnx.Recurrent).  I don't know whether/how it works with deep RNNs, and haven't found support for LSTMs or other memory modules.  \nI haven't found anything in Caffe.  ","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418932212","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Implementing a deep LSTM/RNN \u00e0 la \"Sequence to sequence learning...\"","thumbnail":"self","downs":0,"distinguished":null}
{"downs":0,"thumbnail":"self","distinguished":null,"title":"Landscaping the relationship of Machine Learning with other academic fields","secure_media_embed":{},"created_utc":"1418939864","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"Let's try to figure out how the following academic fields relate to each other :\n1. Machine Learning\n2. Data Mining\n3. Artificial Intelligence\n4. Computational Intelligence\n5. Operations Research","media":null,"ups":0,"retrieved_on":1441012575,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pq4dq/landscaping_the_relationship_of_machine_learning/","from":null,"author":"bharatkhatri","is_self":true,"num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pq4dq","archived":true,"id":"2pq4dq","score":0,"edited":false,"quarantine":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pq4dq/landscaping_the_relationship_of_machine_learning/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1418939864,"from_kind":null,"stickied":false}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418942776","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"What version of Python are you all using?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2pqa9k/what_version_of_python_are_you_all_using/","link_flair_text":null,"media":null,"selftext":"I'm following some video tutorials to learn the language and many of them still use version 2, even recent ones.  Just curious if it's worthwhile to still learn using ver. 2.","retrieved_on":1441012498,"ups":0,"saved":false,"over_18":false,"num_comments":16,"author_flair_css_class":null,"archived":true,"id":"2pqa9k","gilded":0,"link_flair_css_class":null,"name":"t3_2pqa9k","author":"greatluck","from":null,"is_self":true,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pqa9k/what_version_of_python_are_you_all_using/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418942776,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0}
{"selftext":"I've only just scratched the surface of ML and I've implemented a NN which seems to work, but I don't understand completely why and how, and I've become obsessed with this\u2026 which is strange as I never cared about math\u2026 which is a problem I guess.\n\nAnyway, I kindof got the \"aha\" moment watching this video https://www.youtube.com/watch?v=p1-FiWjThs8&amp;spfreload=10 and I wanted to plot E/w just for me to se how the derivative is telling me if I need to increase or decrease w.\n\n\nMy code is quite straight forward, i set a particular weight in the output layer in a loop and then assign:\n\n\nerr=((target[0]-output[0])*(target[0]-output[0]))/2;\n\ndtErr=(target[0]-output[0])*NeuralNetwork::derivativeSigmoid(output[0], 1);\n\n\nWhen plotted for w [-1,1] i should get the MSE and its derivative, right?\nWrong.\n\nI get MSE, but instead of derivative I get the inverse derivative, meaning it's positive when MSE is climbing. Now what. MSE is quadratic, so switching (t-y)^2 to (y-t)^2 would give the same result, but the derivative would then be (y-t), which would actually plot the correct graph. But of course, making that change in the neural net corrupts it.\n\nWhat am I missing? Why is that derivative positive when it should be negative and why is that ok??\n\t\t","media":null,"ups":1,"retrieved_on":1441012362,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pqkrh/derrivative_of_mse_tyyt/","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"derrivative of MSE\u2026 (t-y)!=(y-t)\u2026","author_flair_text":null,"created_utc":"1418948408","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","edited":false,"score":1,"quarantine":false,"from_id":null,"created":1418948408,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2pqkrh/derrivative_of_mse_tyyt/","secure_media":null,"stickied":false,"from":null,"author":"nevemkwa","is_self":true,"num_comments":3,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2pqkrh","gilded":0,"id":"2pqkrh","archived":true}
{"score":8,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pqp6h/bill_gates_briefly_discusses_machine_learning/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1418950972,"is_self":true,"from":null,"author":"b4xt3r","gilded":0,"name":"t3_2pqp6h","link_flair_css_class":null,"archived":true,"id":"2pqp6h","author_flair_css_class":null,"num_comments":11,"saved":false,"over_18":false,"ups":8,"retrieved_on":1441012305,"selftext":"The entire interview is interesting from a historical perspective but I found [Bill Gates' brief thought about machine learning](https://www.youtube.com/watch?v=lz6IQX7uDk4&amp;t=6m40s) to be the most interesting.  The ML part of the interview is all of 15 seconds long but gives some historical perspective to the field back in 1995.","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pqp6h/bill_gates_briefly_discusses_machine_learning/","title":"Bill Gates briefly discusses machine learning with David Letterman in 1995","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1418950972","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning"}
{"secure_media_embed":{},"title":"Learning object classification with pose","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1418960548","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1441012094,"selftext":"I'm trying to understand how to model a learner, that can understand objects and the pose of the object (This is an idea for a research problem). The output I'd want would be a 'cup on it side' (90 degrees tilted), 'a cup upright' (normal position), 'a cup inverted' (180 degree tilted), 'a knife laying flat' (on a table), 'a knife horizontal' (while being used by someone). It would be okay to give output like 'a knife - 180 degrees'. I used natural language to get the point across. \n\nI think the subtlety here would be that the algorithm learns through looking at objects and the natural pose of it. How can you guide a learner, (maybe a deep net using sparse auto encoders as an unsupervised learning method would be great!), to learn pose as well as the objects?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pr5gf/learning_object_classification_with_pose/","is_self":true,"from":null,"author":"mackie__m","gilded":0,"name":"t3_2pr5gf","link_flair_css_class":null,"archived":true,"id":"2pr5gf","num_comments":4,"author_flair_css_class":null,"over_18":false,"saved":false,"score":0,"edited":1418963596,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1418960548,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pr5gf/learning_object_classification_with_pose/","secure_media":null}
{"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"archived":true,"id":"2prbnm","gilded":0,"name":"t3_2prbnm","link_flair_css_class":null,"author":"[deleted]","from":null,"is_self":false,"secure_media":{"type":"youtube.com","oembed":{"author_name":"Yoshua Bengio","height":450,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FmlXzufEk-2E%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DmlXzufEk-2E&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FmlXzufEk-2E%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=mlXzufEk-2E","type":"video","author_url":"http://www.youtube.com/user/YoYoZen","thumbnail_height":360,"title":"The Deep Learning Saga","thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FmlXzufEk-2E%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"thumbnail_width":480,"description":"How the brain works, according to Geoff Hinton. Presented at the NIPS'2010 workshops banquet for the Deep Learning and Unsupervised Feature Learning workshop. Realized by Yoshua Bengio with the help of Olivier Delalleau, and the complicity of Andrew Ng, Yann LeCun, Marc'Aurelio Ranzato, and Honglak Lee.","provider_url":"http://www.youtube.com/","provider_name":"YouTube"}},"url":"https://www.youtube.com/watch?v=mlXzufEk-2E","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418964487,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":1,"media_embed":{"scrolling":false,"height":450,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FmlXzufEk-2E%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DmlXzufEk-2E&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FmlXzufEk-2E%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"subreddit":"MachineLearning","created_utc":"1418964487","author_flair_text":null,"domain":"youtube.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"The Deep Learning Saga","secure_media_embed":{"scrolling":false,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FmlXzufEk-2E%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DmlXzufEk-2E&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FmlXzufEk-2E%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":450},"permalink":"/r/MachineLearning/comments/2prbnm/the_deep_learning_saga/","link_flair_text":null,"media":{"type":"youtube.com","oembed":{"width":600,"thumbnail_url":"http://i.ytimg.com/vi/mlXzufEk-2E/hqdefault.jpg","title":"The Deep Learning Saga","description":"How the brain works, according to Geoff Hinton. Presented at the NIPS'2010 workshops banquet for the Deep Learning and Unsupervised Feature Learning workshop. Realized by Yoshua Bengio with the help of Olivier Delalleau, and the complicity of Andrew Ng, Yann LeCun, Marc'Aurelio Ranzato, and Honglak Lee.","thumbnail_width":480,"provider_url":"http://www.youtube.com/","provider_name":"YouTube","height":450,"author_name":"Yoshua Bengio","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FmlXzufEk-2E%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DmlXzufEk-2E&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FmlXzufEk-2E%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","author_url":"http://www.youtube.com/user/YoYoZen","url":"http://www.youtube.com/watch?v=mlXzufEk-2E","type":"video","thumbnail_height":360}},"selftext":"","retrieved_on":1441012014,"ups":1}
{"from_kind":null,"created":1418971839,"subreddit_id":"t5_2r3gv","url":"https://wit.ai/blog/2014/12/18/adam-keynote","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"id":"2prmje","archived":true,"link_flair_css_class":null,"name":"t3_2prmje","gilded":0,"author":"[deleted]","from":null,"is_self":false,"permalink":"/r/MachineLearning/comments/2prmje/the_story_of_siri_by_its_founder_adam_cheyer/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441011873,"ups":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1418971839","hide_score":false,"domain":"wit.ai","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"The Story of Siri, by its founder Adam Cheyer"}
{"is_self":true,"author":"harfharf11","from":null,"archived":true,"id":"2prsqw","gilded":0,"name":"t3_2prsqw","link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"quarantine":false,"edited":false,"score":0,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2prsqw/how_would_you_approach_this_problem/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1418977556,"from_id":null,"title":"How would you approach this problem?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418977556","author_flair_text":null,"retrieved_on":1441011792,"ups":0,"media":null,"selftext":"Hey all,\n\nLet's say I'm faced with this problem: A user has to choose among 10 options. Each option draws from a 5D random variable, let's say X = [is_black, is_sedan, is_cheap, is_fast, is_efficient] (pretty sure that's not standard notation, sorry). Each dimension is binary. If you are given 100 users, how would you model this problem in order to make good predictions on what future users would choose? This is what I've tried so far:\n\nCombining the data from all of the observations (where one observation is one user selecting among 10 options), and unrolling each observation such that 10 rows were produced per observation, each containing X's outcome, and an additional output column y where y = 1 if the user chose the car and 0 otherwise, I split the data into a training and test set and fed the training set into a random forest, logistic regression, and SVM. The test results were pitiful.\n\nI then figured that I was discarding important information by \"unrolling\" each user choice into independent rows. So then I processed the test data into chunks of 10 (since we know that a user must make a choice), and picked the highest raw scoring row as the predicted choice using SVM and logreg. This too performed poorly. For fun I also tried just using the raw counts of each possible outcome of X seen in the training data and using that as a \"probability\" of each option being chosen, and selecting the highest. This too did poorly.\n\nI don't know how I would represent each observation without unrolling it as I had done to make it suitable for the algorithms I have chosen. I.e., training in a way that preserves the idea that a choice was made among each observation. My first thought is, if that were done, you would be training multiple models on tiny batches of 10, and that doesn't seem like a good idea.\n\nAm I on the wrong track here? Does anyone have any better ideas or suggestions?\n\nAlso, if this is the wrong subreddit for this type of question I apologize. Thanks!","permalink":"/r/MachineLearning/comments/2prsqw/how_would_you_approach_this_problem/","link_flair_text":null}
{"media":null,"selftext":"","retrieved_on":1441011714,"ups":2,"permalink":"/r/MachineLearning/comments/2pryt4/communicationefficient_distributed_dual/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Communication-Efficient Distributed Dual Coordinate Ascent","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1418983897","author_flair_text":null,"domain":"arxiv.org","hide_score":false,"quarantine":false,"edited":false,"score":2,"url":"http://arxiv.org/abs/1409.1458","secure_media":null,"created":1418983897,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"jellchou","from":null,"is_self":false,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"id":"2pryt4","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2pryt4"}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1418985521,"from_kind":null,"url":"http://www.infrabazaar.com/","secure_media":null,"edited":false,"score":1,"quarantine":false,"name":"t3_2ps08a","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2ps08a","num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"infrabazaarpvt","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ps08a/buy_and_sell_used_construction_equipment/","ups":1,"retrieved_on":1441011695,"selftext":"","media":null,"hide_score":false,"domain":"infrabazaar.com","author_flair_text":null,"created_utc":"1418985521","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Buy And Sell Used Construction Equipment","thumbnail":"default","downs":0,"distinguished":null}
{"selftext":"","media":null,"ups":17,"retrieved_on":1441011489,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2psg5c/mlpack_a_scalable_c_machine_learning_library/","downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/O03VoKyibWrUbExGkcxokmCmWbMLhcCAcrmvHjk0KVE.jpg","distinguished":null,"title":"mlpack: A scalable C++ machine learning library","secure_media_embed":{},"created_utc":"1418999732","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"github.com","hide_score":false,"edited":false,"score":17,"quarantine":false,"from_id":null,"url":"https://github.com/rcurtin/mlpack","secure_media":null,"from_kind":null,"created":1418999732,"subreddit_id":"t5_2r3gv","stickied":false,"from":null,"author":"kraakf","is_self":false,"author_flair_css_class":null,"num_comments":6,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2psg5c","id":"2psg5c","archived":true}
{"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"[Academic Research] Machine learning in computer security","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419007309","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hello guys,\n\nI am looking for academic research papers about the usage of machine learning in the computer security. At the moment I haven`t found much about this topic on google scholar and [dblp](http://www.informatik.uni-trier.de/~ley/db/). Most of the security research focuses on for example.: detection of malware with signature/rule based patterns. So usually complex event processing and no use of machine learning.\n\nIt seems to me that computer security does not apply machine learning that much, for example in IDS systems.\n\nWhere is machine learning applied in computer security heavily?\n\nPlease recommend me some papers and topics, where I can do further research on?\n\nI really appreciate your answers!\n","retrieved_on":1441011320,"ups":4,"permalink":"/r/MachineLearning/comments/2pst73/academic_research_machine_learning_in_computer/","link_flair_text":null,"author":"piscoster","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":4,"id":"2pst73","archived":true,"name":"t3_2pst73","link_flair_css_class":null,"gilded":0,"quarantine":false,"edited":false,"score":4,"created":1419007309,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2pst73/academic_research_machine_learning_in_computer/","secure_media":null,"from_id":null,"stickied":false}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Increase Efficiency of Distillation Columns through Maleta Distillation Trays","author_flair_text":null,"created_utc":"1419008881","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"maletacd-distillation-column.blogspot.com","selftext":"","media":null,"ups":1,"retrieved_on":1441011283,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2psw2e/increase_efficiency_of_distillation_columns/","from":null,"author":"maletacdcom","is_self":false,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2psw2e","id":"2psw2e","archived":true,"edited":false,"score":1,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419008881,"secure_media":null,"url":"http://maletacd-distillation-column.blogspot.com/2014/12/increase-efficiency-of-distillation_53.html","stickied":false}
{"title":"Andrew Ng and Baidu Announces Breakthrough In Speech Recognition","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/0v-Osl07IqmfLWyJqiZFYwCD-vnB98L22coRiYVioMs.jpg","domain":"forbes.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419012224","author_flair_text":null,"retrieved_on":1441011202,"ups":73,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2pt2bl/andrew_ng_and_baidu_announces_breakthrough_in/","link_flair_text":null,"is_self":false,"author":"TangerineX","from":null,"archived":true,"id":"2pt2bl","link_flair_css_class":null,"name":"t3_2pt2bl","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":27,"quarantine":false,"edited":false,"score":73,"stickied":false,"secure_media":null,"url":"http://www.forbes.com/sites/roberthof/2014/12/18/baidu-announces-breakthrough-in-speech-recognition-claiming-to-top-google-and-apple/","subreddit_id":"t5_2r3gv","created":1419012224,"from_kind":null,"from_id":null}
{"permalink":"/r/MachineLearning/comments/2pt79u/any_existing_work_on_supervised_learning_of/","link_flair_text":null,"retrieved_on":1441011138,"ups":0,"media":null,"selftext":"","hide_score":false,"domain":"plus.google.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419014753","secure_media_embed":{},"title":"Any existing work on supervised learning of features from audio waveform input?","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/PTQimhf1LNHFCo_4YAliajFCy9R_z1ejK9P6qoZuxdo.jpg","downs":0,"stickied":false,"from_kind":null,"created":1419014753,"subreddit_id":"t5_2r3gv","url":"https://plus.google.com/117584495470836419167/posts/9kdW1Qr7e1z","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":0,"archived":true,"id":"2pt79u","gilded":0,"link_flair_css_class":null,"name":"t3_2pt79u","over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"is_self":false,"author":"animus144","from":null}
{"author_flair_text":null,"created_utc":"1419016837","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"convolution without summation?","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ptbdp/convolution_without_summation/","selftext":"Hi, I'm new to deep learning. Here's the question about Convolutional Net:\nSo a m*n feature map can be mapped to a (m-k+1)*(n-k+1) feature map by a k*k filter. But I don't think it's a (complete) convolution. It seems that a complete convolution computing should further sum the (m-k+1)*(n-k+1) features. However, the name and the mathematical function of convolution appeared in almost every materials make me confusing. I just want to know whether my understanding of Conv-Net is correct, that it's actually convolutions without summations.","media":null,"ups":1,"retrieved_on":1441011084,"num_comments":2,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2ptbdp","gilded":0,"archived":true,"id":"2ptbdp","from":null,"author":"sshidy","is_self":true,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419016837,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ptbdp/convolution_without_summation/","secure_media":null,"stickied":false,"score":1,"edited":false,"quarantine":false}
{"secure_media_embed":{},"title":"The Geometry of Classifiers","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/aoIVJqw8S4SmDa3-iryngghjU8240fYuY0YXQ3dcKSk.jpg","downs":0,"hide_score":false,"domain":"win-vector.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419017871","retrieved_on":1441011058,"ups":1,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2ptddn/the_geometry_of_classifiers/","link_flair_text":null,"is_self":false,"author":"rrenaud","from":null,"id":"2ptddn","archived":true,"link_flair_css_class":null,"name":"t3_2ptddn","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"quarantine":false,"edited":false,"score":1,"stickied":false,"created":1419017871,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.win-vector.com/blog/2014/12/the-geometry-of-classifiers/","secure_media":null,"from_id":null}
{"selftext":"Andrew Ng has said many times that the holy grail of deep learning is unsupervised learning, i.e., the use of unlabeled training data. Why has there been no real progress in unsupervised learning?","media":null,"ups":0,"retrieved_on":1441011037,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ptf26/when_will_unsupervised_learning_be_as_good_as/","downs":0,"thumbnail":"self","distinguished":null,"title":"When will unsupervised learning be as good as supervised learning?","secure_media_embed":{},"created_utc":"1419018742","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"edited":false,"score":0,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ptf26/when_will_unsupervised_learning_be_as_good_as/","subreddit_id":"t5_2r3gv","created":1419018742,"from_kind":null,"stickied":false,"from":null,"author":"sixwings","is_self":true,"num_comments":6,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2ptf26","id":"2ptf26","archived":true}
{"author":"georgeo","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":2,"author_flair_css_class":null,"archived":true,"id":"2pthgf","gilded":0,"link_flair_css_class":null,"name":"t3_2pthgf","quarantine":false,"edited":1419028589,"score":0,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pthgf/svm_w_rbf_in_opencl/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1419019906,"from_id":null,"stickied":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"SVM w/ RBF in OpenCL?","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419019906","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"I saw that libsvm offers a CUDA implementation. Is there anything similar for OpenCL?","retrieved_on":1441011006,"ups":0,"permalink":"/r/MachineLearning/comments/2pthgf/svm_w_rbf_in_opencl/","link_flair_text":null}
{"score":0,"edited":1419026192,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419025920,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2ptt09/learning_about_machine_learning/","stickied":false,"from":null,"author":"brianmannmath","is_self":true,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2ptt09","gilded":0,"link_flair_css_class":null,"id":"2ptt09","archived":true,"selftext":"","media":null,"ups":0,"retrieved_on":1441010856,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2ptt09/learning_about_machine_learning/","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Learning about Machine Learning","author_flair_text":null,"created_utc":"1419025920","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning"}
{"created_utc":"1419028592","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"fastest way to get off the ground with convnets","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pty3w/fastest_way_to_get_off_the_ground_with_convnets/","selftext":"I've been building a neural network library as a learning tool for myself (so I'm not expecting to beat state-of-the-art systems but still want reasonable performance).  I've gotten quite far with vanilla feed-forward nets but am now trying to implements conv-nets and am finding it quite hard to find decent resources.  \n\nSince I actually want to use this system for something interesting like image classification I'm looking for a GPU based backend and I'd like to reduce the dependencies I need.  Currently I'm looking at incorporating [cudarray](https://github.com/andersbll/cudarray) but am having problems getting it to compile so I'd also like to consider alternatives.  My main requirements are something reasonably fast (GPU based on nvidia GPU), with a python interface or simple enough that I could write a cython wrapper, and low level enough that it just does convolutions and maybe pooling (I don't want to call another NN library).\n\nI've also found a general dirth of good CNN resources so would appreciate any tutorials links, etc... on implementing a CNN.\n\nthanks!","media":null,"ups":2,"retrieved_on":1441010790,"author_flair_css_class":null,"num_comments":12,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pty3w","id":"2pty3w","archived":true,"from":null,"author":"spurious_recollectio","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pty3w/fastest_way_to_get_off_the_ground_with_convnets/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1419028592,"from_kind":null,"stickied":false,"edited":false,"score":2,"quarantine":false}
{"retrieved_on":1441010746,"ups":1,"media":null,"selftext":"Hello /r/machinelearning,\n\n\nI play in an recreational/upper skill level basketball league in Portland.  The people that run the league track games by recording the final score (as well as what week/season/year, and the team names obviously).  I thought it might be neat to try and develop an algorithm that can determine most ideal team match-ups based on past performance, so I asked the league if they had historical data, and if they would be willing to share.\n\nThe league got back to me right away and gave me a CSV file of data going back 4 years for approximately ~200 teams over a number of different leagues/skill levels.  \n\nGiven that I don't have many parameters to work with (final score, date and team names), and that I'm just starting to learn about machine learning (I'm a mechanical engineer by trade, just dipping my toes into the water regarding software, I know a little R, but I'm more familiar with Python), I was hoping that /r/machinelearning would give me some ideas on where to start.\n\nAny input would be greatly appreciated!","permalink":"/r/MachineLearning/comments/2pu1hx/received_interesting_but_limited_dataset_where_to/","link_flair_text":null,"title":"Received interesting but limited data-set, where to begin with probabilistic modeling?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419030487","author_flair_text":null,"quarantine":false,"edited":false,"score":1,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2pu1hx/received_interesting_but_limited_dataset_where_to/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1419030487,"from_kind":null,"from_id":null,"is_self":true,"author":"Ogi010","from":null,"id":"2pu1hx","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2pu1hx","saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":2}
{"score":2,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419033308,"url":"http://www.reddit.com/r/MachineLearning/comments/2pu6ez/academic_research_domain_transfer_of_knowledge_in/","secure_media":null,"is_self":true,"from":null,"author":"mackie__m","gilded":0,"name":"t3_2pu6ez","link_flair_css_class":null,"id":"2pu6ez","archived":true,"author_flair_css_class":null,"num_comments":2,"over_18":false,"saved":false,"ups":2,"retrieved_on":1441010682,"selftext":"I'm looking at research topics for ML, and this seems to be something that regularly comes up. One paper I found is [this](http://www.machinelearning.org/proceedings/icml2007/papers/329.pdf), where it uses a phenomenon called Rule Transfer to speed up domain transfer of knowledge. \n\nSome questions I now have are:\nWith popularity of deep nets and auto encoders, is this problem interesting anymore? \nGiven that we can automatically learn interesting characteristics, does it make sense to look at this problem? Why not just learn everything? \n\nIt would be great to get some insight about this topic. I would appreciate some help. Thanks!","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pu6ez/academic_research_domain_transfer_of_knowledge_in/","secure_media_embed":{},"title":"[Academic Research] Domain Transfer of knowledge in ML","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419033308","subreddit":"MachineLearning","media_embed":{}}
{"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419062657","author_flair_text":null,"title":"Hands on tutorial for creating new classifier with Caffe?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","permalink":"/r/MachineLearning/comments/2pvc1c/hands_on_tutorial_for_creating_new_classifier/","link_flair_text":null,"retrieved_on":1441010143,"ups":1,"media":null,"selftext":"I checked the fine tuning tutorial on Caffe's site, but I'm still confused. Lets say you have a data set of dogs and cats, and want to use one of the prebuilt networks like AlexNet with Caffe. How does this work? Do you just switch out the last layer with something else? Right now the prebuilt ones will make predictions with the 1000 categories from Imagenet, how do you make it predict on your set, like 'cat' or 'dog'?  \n\nHas anyone seen a practical tutorial for something like this, with code? Thanks!","archived":true,"id":"2pvc1c","link_flair_css_class":null,"gilded":0,"name":"t3_2pvc1c","saved":false,"over_18":false,"num_comments":4,"author_flair_css_class":null,"is_self":true,"author":"matlab484","from":null,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pvc1c/hands_on_tutorial_for_creating_new_classifier/","created":1419062657,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"quarantine":false,"edited":false,"score":1}
{"selftext":"","media":null,"ups":17,"retrieved_on":1441010024,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pvl7t/another_paper_researching_ways_to_combat/","thumbnail":"http://b.thumbs.redditmedia.com/Jtt9BQant7WBKPC-d7D90sC6lqnEd7QL8zJokMuIsho.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Another paper researching ways to combat misclassification of adversarial examples by neural nets.","author_flair_text":null,"created_utc":"1419072769","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"plus.google.com","edited":false,"score":17,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419072769,"from_kind":null,"url":"https://plus.google.com/103174629363045094445/posts/1iVqjjmsVAX","secure_media":null,"stickied":false,"from":null,"author":"test3545","is_self":false,"author_flair_css_class":null,"num_comments":18,"over_18":false,"saved":false,"gilded":0,"name":"t3_2pvl7t","link_flair_css_class":null,"id":"2pvl7t","archived":true}
{"saved":false,"over_18":false,"num_comments":15,"author_flair_css_class":null,"archived":true,"id":"2pvz1c","link_flair_css_class":null,"name":"t3_2pvz1c","gilded":0,"author":"mostly_complaints","from":null,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/2pvz1c/seminal_papers_on_machine_learning/","secure_media":null,"from_kind":null,"created":1419086879,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"quarantine":false,"score":17,"edited":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419086879","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Seminal papers on machine learning","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2pvz1c/seminal_papers_on_machine_learning/","link_flair_text":null,"media":null,"selftext":"What papers would you consider groundbreaking and a must read for students of the field?","retrieved_on":1441009845,"ups":17}
{"from_id":null,"url":"http://maletacdcom.wordpress.com/2014/12/19/the-maleta-cyclic-distillation-llc-at-the-conference-distillation-absorption-2014-showed-pilot-distillation-column-in-cyclic-operation/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419088428,"stickied":false,"score":1,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"name":"t3_2pw11t","gilded":0,"link_flair_css_class":null,"id":"2pw11t","archived":true,"from":null,"author":"maletacdcom","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pw11t/the_maleta_cyclic_distillation_llc_at_the/","selftext":"","media":null,"ups":1,"retrieved_on":1441009818,"created_utc":"1419088428","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"maletacdcom.wordpress.com","hide_score":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"The Maleta cyclic distillation LLC at the conference Distillation and Absorption 2014 showed pilot distillation column in cyclic operation","secure_media_embed":{}}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Ng's course.","author_flair_text":null,"created_utc":"1419091383","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"","media":null,"ups":5,"retrieved_on":1441009765,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2pw56g/ngs_course/","from":null,"author":"NicolasGuacamole","is_self":true,"author_flair_css_class":null,"num_comments":6,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2pw56g","archived":true,"id":"2pw56g","edited":false,"score":5,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419091383,"url":"http://www.reddit.com/r/MachineLearning/comments/2pw56g/ngs_course/","secure_media":null,"stickied":false}
{"stickied":false,"secure_media":null,"url":"http://dataelixir.com/?referred=true","from_kind":null,"created":1419091714,"subreddit_id":"t5_2r3gv","from_id":null,"quarantine":false,"score":0,"edited":false,"archived":true,"id":"2pw5od","gilded":0,"name":"t3_2pw5od","link_flair_css_class":null,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"is_self":false,"author":"[deleted]","from":null,"permalink":"/r/MachineLearning/comments/2pw5od/data_elixir_issue_14_ai_developments_interviews/","link_flair_text":null,"retrieved_on":1441009759,"ups":0,"media":null,"selftext":"","domain":"dataelixir.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419091714","author_flair_text":null,"title":"Data Elixir, Issue 14: AI developments, interviews, data creeps, \"best of 2014\" collections","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default"}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419111937,"from_kind":null,"secure_media":null,"url":"http://blog.dlib.net/2014/12/dlib-1812-released.html","score":12,"edited":false,"quarantine":false,"gilded":0,"name":"t3_2px2y1","link_flair_css_class":null,"archived":true,"id":"2px2y1","num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"davis685","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2px2y1/v1812_of_the_dlib_c_machine_learning_library_was/","ups":12,"retrieved_on":1441009266,"selftext":"","media":null,"hide_score":false,"domain":"blog.dlib.net","author_flair_text":null,"created_utc":"1419111937","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"v18.12 of the dlib C++ machine learning library was just released","thumbnail":"default","downs":0,"distinguished":null}
{"url":"http://www.analyticbridge.com/profiles/blogs/200-machine-learning-and-data-science-resources","secure_media":null,"from_kind":null,"created":1419116194,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"id":"2pxa54","archived":true,"link_flair_css_class":null,"name":"t3_2pxa54","gilded":0,"author":"urinec","from":null,"is_self":false,"permalink":"/r/MachineLearning/comments/2pxa54/200_machine_learning_and_data_science_resources/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441009173,"ups":0,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419116194","author_flair_text":null,"domain":"analyticbridge.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"200 machine learning and data science resources","secure_media_embed":{}}
{"is_self":true,"author":"[deleted]","from":null,"archived":true,"id":"2pykro","name":"t3_2pykro","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":4,"quarantine":false,"edited":false,"score":1,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2pykro/question_which_model_to_predict_air_cleanness/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1419150605,"from_kind":null,"from_id":null,"title":"[Question] Which model to predict air cleanness (Ozone/CO2) ?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419150605","author_flair_text":null,"retrieved_on":1441008569,"ups":1,"media":null,"selftext":"How hard it is to predict the air pollution?\n\nI am an agronomist, doing some research on some small plants. The plants are very sensitive to air pollution in urban area (need deep explanation here, but it's not the problem.).\n\nThe Problem: We want to predict the ozone and particulate concentration in the air. Specifically, we have a dataset of collected ozone and particulate measurements in the last three months with hour-frequency. We could get other information (such: weather/wind/ ..).\n\nWhat is the best model to predict and sense the cleanness of the air?\n\nSelf-learner here, can anyone recommend paper/research project to check?","permalink":"/r/MachineLearning/comments/2pykro/question_which_model_to_predict_air_cleanness/","link_flair_text":null}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419154235","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Machine learning vs Reasoning and knowledge base: When to use what?","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2pynza/machine_learning_vs_reasoning_and_knowledge_base/","link_flair_text":null,"media":null,"selftext":"Hello guys,\n\nin a lot of papers scientists use a knowledge base to check or represent data.\n\nI am comparing machine learning versus reasoning systems, which use a knowledge base as a backeend. In my opinion machine learning wins over logic, when:\n\n* Performance is an issue\n* What is not in the knowledge base is not known to the reasoning system\n* Unknown anomalies are completely invisible to the creator of the knowledge base\n\nWhat do you think is an argument against/for using logic and a knowlege base versus machine learning?\n\nI appreciate your reply!","retrieved_on":1441008527,"ups":13,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":9,"archived":true,"id":"2pynza","gilded":0,"link_flair_css_class":null,"name":"t3_2pynza","author":"piscoster","from":null,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/2pynza/machine_learning_vs_reasoning_and_knowledge_base/","secure_media":null,"from_kind":null,"created":1419154235,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":13}
{"media":null,"selftext":"Hi,\nWhich of these programming languages makes more sense to implement a Neural Net like http://research.microsoft.com/apps/pubs/?id=226584 or http://nips.cc/Conferences/2014/Program/event.php?ID=4554 ? \nI know Python well and have written algorithms using theano , but these papers have some elements which are hard to write in theano. In case, theano doesnt suffice to write them, what would be best alternative out of these 3 ?\nI have never worked in C++ and Fortran and used C in some courses in my college. All three seem to be having support for GPU and interoperability with Python . \nIs there any other factor I should take into consideration while choosing one ?","retrieved_on":1441008263,"ups":3,"permalink":"/r/MachineLearning/comments/2pz8c3/what_programming_language_to_choose_out_of_c_c/","link_flair_text":null,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"What Programming Language to choose out of C / C++ and Fortran ?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419176595","hide_score":false,"domain":"self.MachineLearning","quarantine":false,"score":3,"edited":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419176595,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2pz8c3/what_programming_language_to_choose_out_of_c_c/","from_id":null,"stickied":false,"author":"muktabh","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":42,"author_flair_css_class":null,"id":"2pz8c3","archived":true,"gilded":0,"name":"t3_2pz8c3","link_flair_css_class":null}
{"retrieved_on":1441007879,"ups":7,"media":null,"selftext":"This might be more suitable for /r/statistics but I am trying to learn about convolutional neural networks so this seemed like a better place.\n\nI was watching Geoffery Hinton's coursera lecture on convolutional neural nets and at one point he says \"replicated feature detectors achieve 'tranlational equivariance' rather than 'translational invariance' \".\n\nI do not have background in statistics so was wondering if someone could give an explanation in layman terms.","permalink":"/r/MachineLearning/comments/2q01z5/difference_between_invariance_and_equivariancein/","link_flair_text":null,"title":"Difference between invariance and equivariance(in terms of convolutional neural networks)","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419194844","author_flair_text":null,"quarantine":false,"edited":false,"score":7,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2q01z5/difference_between_invariance_and_equivariancein/","secure_media":null,"created":1419194844,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"is_self":true,"author":"nilspin","from":null,"archived":true,"id":"2q01z5","link_flair_css_class":null,"name":"t3_2q01z5","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":3}
{"retrieved_on":1441007793,"ups":3,"media":null,"selftext":"Hello guys,\n\nI am looking for academic research papers about the usage of machine learning with complex event processing. I plan to focus much more on the topic machine learning, because even though my background is in statistics and computer science, I haven`t done much research in this field.\n\nSpecifically I want to focus, how to determine if an event depends on an other event. At the moment I haven`t found much about this topic on google scholar and dblp. \n\nMost of the complex event processing research focuses on reasoning and system execution. So usually these systems do not use machine learning.\n\n* Where is machine learning applied in complex event processing?\n* Is it possible to find correlation of events, or even patterns, in an event stream via machine learning?\n\nPlease recommend me some papers, where I can do further research on?\n\nI really appreciate your answers!\n","permalink":"/r/MachineLearning/comments/2q08ll/academic_research_machine_learning_and_complex/","link_flair_text":null,"title":"[Academic research] Machine learning and complex event processing - Correlation of streamed events","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419198541","author_flair_text":null,"quarantine":false,"score":3,"edited":false,"stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q08ll/academic_research_machine_learning_and_complex/","created":1419198541,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"is_self":true,"author":"Regentag","from":null,"archived":true,"id":"2q08ll","link_flair_css_class":null,"name":"t3_2q08ll","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":7}
{"author_flair_css_class":null,"num_comments":7,"saved":false,"over_18":false,"name":"t3_2q0hg9","gilded":0,"link_flair_css_class":null,"id":"2q0hg9","archived":true,"from":null,"author":"sungiv","is_self":true,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q0hg9/how_to_train_autoencoders_with_tied_weights/","created":1419203589,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false,"edited":false,"score":8,"quarantine":false,"created_utc":"1419203589","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"How to train Auto-Encoders with Tied Weights?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q0hg9/how_to_train_autoencoders_with_tied_weights/","selftext":"Hi there.\n\nI started to read about auto-encoders a short time ago and I am trying to imagine how I could employ an under-complete AE (I'm considering the simplest scenario possible, no denoising, only with a hidden layer).\n\nThe idea of reconstructing the input in the output layer seems trivial to me, but I just can't understand how it is possible to apply the backpropagation algorithm in the training phase if one considers W in the encoder and W' the decoder. Furthermore, what advantages or disadvantages will I have if I use tied weights and why that simple property assures me those advantages? And how do I assure that both matrices will continue to be transpose when I backpropagate the error?\n\nI am pretty sure that something is wring in my brain and I can't see what is going on. I hope someone can help :)\n\nThank you in advance.","media":null,"ups":8,"retrieved_on":1441007679}
{"edited":false,"score":1,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419207423,"secure_media":null,"url":"https://medium.com/@ahousley/introducing-seldon-the-open-predictive-platform-514adf3f1ce6","is_self":false,"from":null,"author":"ahousley","name":"t3_2q0oau","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2q0oau","num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"ups":1,"retrieved_on":1441007590,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q0oau/introducing_seldon_the_open_predictive_platform/","secure_media_embed":{},"title":"Introducing Seldon: the Open Predictive Platform","thumbnail":"http://b.thumbs.redditmedia.com/Ma5eYLGVWTRoOfQ1qrvUI92FjxzjAveJsRcIrknufZs.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"medium.com","author_flair_text":null,"created_utc":"1419207423","subreddit":"MachineLearning","media_embed":{}}
{"from_id":null,"url":"https://cireneikual.wordpress.com/2014/12/21/continuous-hierarchical-temporal-memory-chtm-update/","secure_media":null,"created":1419208399,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false,"edited":false,"score":9,"quarantine":false,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"name":"t3_2q0pzk","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2q0pzk","from":null,"author":"CireNeikual","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q0pzk/continuous_hierarchical_temporal_memory_chtm_part/","selftext":"","media":null,"ups":9,"retrieved_on":1441007568,"created_utc":"1419208399","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"cireneikual.wordpress.com","hide_score":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg","distinguished":null,"title":"Continuous Hierarchical Temporal Memory (CHTM) Part 1","secure_media_embed":{}}
{"from":null,"author":"rantana","is_self":false,"author_flair_css_class":null,"num_comments":1,"saved":false,"over_18":false,"name":"t3_2q14r0","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2q14r0","score":10,"edited":false,"quarantine":false,"from_id":null,"url":"http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf","secure_media":null,"from_kind":null,"created":1419217098,"subreddit_id":"t5_2r3gv","stickied":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"Naive Bayes SVM: Simple, Good Sentiment and Topic Classification","secure_media_embed":{},"created_utc":"1419217098","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"nlp.stanford.edu","hide_score":false,"selftext":"","media":null,"ups":10,"retrieved_on":1441007377,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q14r0/naive_bayes_svm_simple_good_sentiment_and_topic/"}
{"downs":0,"thumbnail":"self","distinguished":null,"title":"Image a circle of neurons. Each neuron is (or can) be connected to any other neuron. The weights between them can only be binary (0 or 1). What is this architecture called?","secure_media_embed":{},"created_utc":"1419224072","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"Edit: That should real \"imagine\", not \"image\".\n\nTitle says it all. \n\nLet's imagine we have, say, 10 neurons, all in a circle. Now, each neuron can be connected to any other one, (except itself of course). However also crucially, the weights associated between every neurons can only ever be binary, that is to say, either 0 or 1. \n\nWhat might this structure be called? As an aside, would the modeling of such a structure be possible with something like Theano, or Caffe? \n\nThanks!","media":null,"ups":0,"retrieved_on":1441007227,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q1gb4/image_a_circle_of_neurons_each_neuron_is_or_can/","from":null,"author":"Ayakalam","is_self":true,"num_comments":28,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2q1gb4","archived":true,"id":"2q1gb4","edited":false,"score":0,"quarantine":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q1gb4/image_a_circle_of_neurons_each_neuron_is_or_can/","secure_media":null,"created":1419224072,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false}
{"score":25,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"created":1419234708,"subreddit_id":"t5_2r3gv","url":"http://techtv.mit.edu/collections/bcs/videos/30698-what-s-wrong-with-convolutional-nets","secure_media":null,"is_self":false,"from":null,"author":"downtownslim","name":"t3_2q1v2x","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2q1v2x","author_flair_css_class":null,"num_comments":20,"over_18":false,"saved":false,"ups":25,"retrieved_on":1441007035,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q1v2x/whats_wrong_with_convolutional_nets/","secure_media_embed":{},"title":"What's wrong with convolutional nets?","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"techtv.mit.edu","author_flair_text":null,"created_utc":"1419234708","subreddit":"MachineLearning","media_embed":{}}
{"secure_media_embed":{},"title":"Improving Naive Bayes accuracy for text classification?","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419240976","subreddit":"MachineLearning","media_embed":{},"ups":8,"retrieved_on":1441006956,"selftext":"Hi everyone,\n\nI am performing document (text) classification on the category of websites, and use the website content (tokenized, stemmed and lowercased) as the feature set for my data.\n\nMy problem is that I have two over-represented categories which has vastly more data points than any other (roughly 70% or 4000~ of my data points are of his one of these two categories, while about 20 other categories make up the last 30%, some of which have fewer than 50 data points). I had a couple questions as to how I could improve on this:\n\n1. What could I do to improve the accuracy of my classifier in this case of sparse data for some of the labels? Should I simply discard a certain proportion of the data points in the category which is over-represented? Should I use something other than Gaussian Naive Bayes with tf-idf?\n\n2. After I perform the classification, I save the tfidf vector as well as the classifier to disk. However, when I re-rerun the classification on the same data, I sometimes get different results from what I initially got (for example, if previously a data point was classified as \"Entertainment\", it might receive \"News\" now). Is this indicative of an error in my implementation, or expected?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q2185/improving_naive_bayes_accuracy_for_text/","is_self":true,"from":null,"author":"trininxs","name":"t3_2q2185","gilded":0,"link_flair_css_class":null,"id":"2q2185","archived":true,"num_comments":14,"author_flair_css_class":null,"over_18":false,"saved":false,"score":8,"edited":1419260902,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"created":1419240976,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2q2185/improving_naive_bayes_accuracy_for_text/","secure_media":null}
{"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419247269","hide_score":false,"domain":"kingsunmachinery.com","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"Plastic Bag Making Machine","permalink":"/r/MachineLearning/comments/2q276r/plastic_bag_making_machine/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441006879,"ups":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"id":"2q276r","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2q276r","author":"ramylemon","from":null,"is_self":false,"created":1419247269,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.kingsunmachinery.com/category/flexible-packaging-machinery/plastic-bag-making-machines-39.html","secure_media":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0}
{"link_flair_css_class":null,"gilded":0,"name":"t3_2q2bbx","id":"2q2bbx","archived":true,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"cavedave","stickied":false,"from_id":null,"secure_media":null,"url":"http://www.ams.org/samplings/feature-column/fc-2014-12","from_kind":null,"subreddit_id":"t5_2r3gv","created":1419251376,"edited":false,"score":22,"quarantine":false,"domain":"ams.org","hide_score":false,"created_utc":"1419251376","author_flair_text":"naive","media_embed":{},"subreddit":"MachineLearning","title":"How to Grow and Prune a Classification Tree","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/os0ulRhyjL-Unf6JVXt08lCnF0rwdoJN9wF9kg3GeSs.jpg","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q2bbx/how_to_grow_and_prune_a_classification_tree/","ups":22,"retrieved_on":1441006825,"selftext":"","media":null}
{"from":null,"author":"Foxtr0t","is_self":false,"author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2q2r1i","id":"2q2r1i","archived":true,"edited":false,"score":8,"quarantine":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419263063,"url":"http://fastml.com/interactive-in-browser-3d-visualization-of-datasets/","secure_media":null,"stickied":false,"thumbnail":"http://b.thumbs.redditmedia.com/J8JndP3h6F42alO_oJZqCxW2tOA-v1luZL0o_PmR5Vk.jpg","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Interactive in-browser 3D visualization of datasets","author_flair_text":null,"created_utc":"1419263063","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"fastml.com","selftext":"","media":null,"ups":8,"retrieved_on":1441006621,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q2r1i/interactive_inbrowser_3d_visualization_of_datasets/"}
{"edited":false,"score":25,"quarantine":false,"stickied":false,"from_id":null,"created":1419268034,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/","secure_media":null,"is_self":false,"from":null,"author":"vodkagoodmeatrotten","link_flair_css_class":null,"name":"t3_2q301e","gilded":0,"archived":true,"id":"2q301e","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"ups":25,"retrieved_on":1441006505,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q301e/convolutional_nets_and_cifar10_an_interview_with/","secure_media_embed":{},"title":"Convolutional Nets and CIFAR-10: An Interview with Yann LeCun","thumbnail":"http://b.thumbs.redditmedia.com/3t8At2RrC54tqJ2f0K1ghuedlmozqCVeHmbSeIBtZhg.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"blog.kaggle.com","author_flair_text":null,"created_utc":"1419268034","subreddit":"MachineLearning","media_embed":{}}
{"num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"name":"t3_2q3rn2","gilded":0,"archived":true,"id":"2q3rn2","from":null,"author":"omnipresent101","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q3rn2/how_can_i_get_train_time_and_test_time_in_weka/","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419281612,"stickied":false,"edited":false,"score":1,"quarantine":false,"created_utc":"1419281612","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"How can I get \"Train time\" and \"Test time\" in WEKA?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q3rn2/how_can_i_get_train_time_and_test_time_in_weka/","selftext":"I'm using weka J48 on my dataset to predict the quality of the wine: http://zangsir.weebly.com/uploads/3/1/3/8/3138983/wine2.arff\n\nWhen I classify the data in Weka using J48 and a 66% percentage split, I get the following Classifier output. But nowhere in this output do I see the \"Train time\" and \"Test time\".\n\nHow can I get the \"Train time\" and \"Test time\"? Do I need to run with option \"use training set\" to get the \"train time\" and then run again with 66% split to get the \"test time\"?\n\n\n        === Run information ===\n        \n        Scheme:weka.classifiers.trees.J48 -C 0.25 -M 2\n        Relation:     wines\n        Instances:    6497\n        Attributes:   13\n                      fixed.acidity\n                      volatile.acidity\n                      citric.acid\n                      residual.sugar\n                      chlorides\n                      free.sulfur.dioxide\n                      total.sulfur.dioxide\n                      density\n                      pH\n                      sulphates\n                      alcohol\n                      quality\n                      kind\n        Test mode:split 66.0% train, remainder test\n        \n        === Classifier model (full training set) ===\n        \n        J48 pruned tree\n        ------------------\n        \n        total.sulfur.dioxide &lt;= 67\n        |   chlorides &lt;= 0.049\n        |   |   sulphates &lt;= 0.55\n        |   |   |   density &lt;= 0.99455: white (108.0/1.0)\n        |   |   |   density &gt; 0.99455\n        |   |   |   |   residual.sugar &lt;= 2.65: red (5.0)\n        |   |   |   |   residual.sugar &gt; 2.65: white (7.0)\n        |   |   sulphates &gt; 0.55\n        |   |   |   chlorides &lt;= 0.037: white (15.0/1.0)\n        |   |   |   chlorides &gt; 0.037\n        |   |   |   |   alcohol &lt;= 10.6: white (4.0/1.0)\n        |   |   |   |   alcohol &gt; 10.6: red (22.0)\n        |   chlorides &gt; 0.049\n        |   |   density &lt;= 0.99442\n        |   |   |   pH &lt;= 3.19\n        |   |   |   |   total.sulfur.dioxide &lt;= 36: red (6.0)\n        |   |   |   |   total.sulfur.dioxide &gt; 36: white (10.0)\n        |   |   |   pH &gt; 3.19: red (98.0/1.0)\n        |   |   density &gt; 0.99442: red (1129.0)\n        total.sulfur.dioxide &gt; 67\n        |   chlorides &lt;= 0.067\n        |   |   citric.acid &lt;= 0.11\n        |   |   |   pH &lt;= 3.49: white (80.0/1.0)\n        |   |   |   pH &gt; 3.49\n        |   |   |   |   total.sulfur.dioxide &lt;= 100: red (11.0)\n        |   |   |   |   total.sulfur.dioxide &gt; 100: white (7.0/1.0)\n        |   |   citric.acid &gt; 0.11\n        |   |   |   chlorides &lt;= 0.059\n        |   |   |   |   volatile.acidity &lt;= 0.575: white (4254.0/14.0)\n        |   |   |   |   volatile.acidity &gt; 0.575\n        |   |   |   |   |   pH &lt;= 3.45: white (37.0)\n        |   |   |   |   |   pH &gt; 3.45: red (5.0/1.0)\n        |   |   |   chlorides &gt; 0.059\n        |   |   |   |   total.sulfur.dioxide &lt;= 89\n        |   |   |   |   |   fixed.acidity &lt;= 8.4: white (5.0/1.0)\n        |   |   |   |   |   fixed.acidity &gt; 8.4: red (4.0)\n        |   |   |   |   total.sulfur.dioxide &gt; 89\n        |   |   |   |   |   sulphates &lt;= 0.53: white (123.0)\n        |   |   |   |   |   sulphates &gt; 0.53\n        |   |   |   |   |   |   volatile.acidity &lt;= 0.47\n        |   |   |   |   |   |   |   fixed.acidity &lt;= 6.15: red (3.0/1.0)\n        |   |   |   |   |   |   |   fixed.acidity &gt; 6.15: white (35.0)\n        |   |   |   |   |   |   volatile.acidity &gt; 0.47: red (2.0)\n        |   chlorides &gt; 0.067\n        |   |   total.sulfur.dioxide &lt;= 153\n        |   |   |   density &lt;= 0.99498\n        |   |   |   |   alcohol &lt;= 11.1: white (75.0/1.0)\n        |   |   |   |   alcohol &gt; 11.1\n        |   |   |   |   |   density &lt;= 0.992: white (11.0)\n        |   |   |   |   |   density &gt; 0.992: red (5.0)\n        |   |   |   density &gt; 0.99498\n        |   |   |   |   volatile.acidity &lt;= 0.305\n        |   |   |   |   |   fixed.acidity &lt;= 7.8: white (16.0)\n        |   |   |   |   |   fixed.acidity &gt; 7.8: red (3.0)\n        |   |   |   |   volatile.acidity &gt; 0.305\n        |   |   |   |   |   residual.sugar &lt;= 8.2: red (282.0/1.0)\n        |   |   |   |   |   residual.sugar &gt; 8.2\n        |   |   |   |   |   |   pH &lt;= 3.14: white (11.0)\n        |   |   |   |   |   |   pH &gt; 3.14: red (6.0)\n        |   |   total.sulfur.dioxide &gt; 153: white (118.0/1.0)\n        \n        Number of Leaves  : \t31\n        \n        Size of the tree : \t61\n        \n        \n        Time taken to build model: 0.1 seconds\n        \n        === Evaluation on test split ===\n        === Summary ===\n        \n        Correctly Classified Instances        2166               98.0534 %\n        Incorrectly Classified Instances        43                1.9466 %\n        Kappa statistic                          0.9461\n        Mean absolute error                      0.0217\n        Root mean squared error                  0.1357\n        Relative absolute error                  5.8797 %\n        Root relative squared error             31.8558 %\n        Total Number of Instances             2209     \n        \n        === Detailed Accuracy By Class ===\n        \n                       TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class\n                         0.954     0.011      0.963     0.954     0.959      0.975    red\n                         0.989     0.046      0.986     0.989     0.987      0.975    white\n        Weighted Avg.    0.981     0.038      0.98      0.981     0.981      0.975\n        \n        === Confusion Matrix ===\n        \n            a    b   &lt;-- classified as\n          501   24 |    a = red\n           19 1665 |    b = white\n        \n        \n\n","media":null,"ups":1,"retrieved_on":1441006147}
{"id":"2q3s9p","archived":true,"name":"t3_2q3s9p","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":21,"is_self":false,"author":"themelink","from":null,"stickied":false,"secure_media":null,"url":"http://blog.datadive.net/interpreting-random-forests/","subreddit_id":"t5_2r3gv","from_kind":null,"created":1419281910,"from_id":null,"quarantine":false,"score":3,"edited":false,"domain":"blog.datadive.net","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419281910","author_flair_text":null,"title":"Interpreting random forests","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"default","permalink":"/r/MachineLearning/comments/2q3s9p/interpreting_random_forests/","link_flair_text":null,"retrieved_on":1441006139,"ups":3,"media":null,"selftext":""}
{"from_kind":null,"created":1419282225,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q3swd/can_multilayer_perceptrons_and_standard/","from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":2,"archived":true,"id":"2q3swd","name":"t3_2q3swd","gilded":0,"link_flair_css_class":null,"author":"[deleted]","from":null,"is_self":true,"permalink":"/r/MachineLearning/comments/2q3swd/can_multilayer_perceptrons_and_standard/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441006131,"ups":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419282225","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"can Multilayer perceptrons and standard Backpropagation algorithm take revenge against Deep learning architectures and methods?"}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q4cai/any_advice/","selftext":"Greetings,\nI am a first year bioinformatics bachelor student and I got interested in machine learning. So far, I have experience only with python, and was wondering if I should continue with python, or start learn a new language. \nI want to know what programming language(s) you use for machine learning, and why? What are the pros and cons of different languages for machine learning? \n","media":null,"ups":0,"retrieved_on":1441005850,"created_utc":"1419292263","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"Any advice?","secure_media_embed":{},"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q4cai/any_advice/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419292263,"stickied":false,"score":0,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":5,"saved":false,"over_18":false,"name":"t3_2q4cai","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2q4cai","from":null,"author":"irwt","is_self":true}
{"hide_score":false,"domain":"regex.inginf.units.it","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419294028","secure_media_embed":{},"title":"Machine Learning Lab - Regex Generator - Automatic Generation of Text Extraction Patterns from Examples with Genetic Programming","distinguished":null,"thumbnail":"default","downs":0,"permalink":"/r/MachineLearning/comments/2q4fou/machine_learning_lab_regex_generator_automatic/","link_flair_text":null,"retrieved_on":1441005806,"ups":7,"media":null,"selftext":"","archived":true,"id":"2q4fou","gilded":0,"link_flair_css_class":null,"name":"t3_2q4fou","over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"is_self":false,"author":"bboyjkang","from":null,"stickied":false,"created":1419294028,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://regex.inginf.units.it/index.html","from_id":null,"quarantine":false,"score":7,"edited":false}
{"title":"Convolutional neural network morphing pictures of chairs","secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FQCSW4isBDL0%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQCSW4isBDL0&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FQCSW4isBDL0%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":450,"width":600,"scrolling":false},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/tiDTVYyYT3aGG_lZ-8h0vzvEhWMmEMCdU8hPHjZcO9Y.jpg","distinguished":null,"domain":"youtube.com","hide_score":false,"created_utc":"1419295982","author_flair_text":null,"media_embed":{"scrolling":false,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FQCSW4isBDL0%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQCSW4isBDL0&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FQCSW4isBDL0%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":450},"subreddit":"MachineLearning","ups":40,"retrieved_on":1441005760,"selftext":"","media":{"oembed":{"description":"Video for the paper \"Learning to Generate Chairs with Convolutional Neural Networks\" http://arxiv.org/abs/1411.5928 50 chairs morphed into each other with our generative convolutional neural network. Generated images are 128x128 pixels, hence the small size of the chair images.","thumbnail_width":480,"thumbnail_url":"http://i.ytimg.com/vi/QCSW4isBDL0/hqdefault.jpg","width":600,"title":"Chairs morphing","provider_name":"YouTube","provider_url":"http://www.youtube.com/","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FQCSW4isBDL0%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQCSW4isBDL0&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FQCSW4isBDL0%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":450,"author_name":"Computer Vision Freiburg","thumbnail_height":360,"author_url":"http://www.youtube.com/channel/UC351jap1wiOJvKXr3mhODlg","type":"video","url":"http://www.youtube.com/watch?v=QCSW4isBDL0"},"type":"youtube.com"},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q4j8x/convolutional_neural_network_morphing_pictures_of/","is_self":false,"from":null,"author":"Noncomment","link_flair_css_class":null,"gilded":0,"name":"t3_2q4j8x","archived":true,"id":"2q4j8x","author_flair_css_class":null,"num_comments":30,"saved":false,"over_18":false,"edited":false,"score":40,"quarantine":false,"stickied":false,"from_id":null,"url":"https://www.youtube.com/watch?v=QCSW4isBDL0","secure_media":{"type":"youtube.com","oembed":{"provider_url":"http://www.youtube.com/","provider_name":"YouTube","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FQCSW4isBDL0%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","title":"Chairs morphing","description":"Video for the paper \"Learning to Generate Chairs with Convolutional Neural Networks\" http://arxiv.org/abs/1411.5928 50 chairs morphed into each other with our generative convolutional neural network. Generated images are 128x128 pixels, hence the small size of the chair images.","thumbnail_width":480,"author_url":"http://www.youtube.com/channel/UC351jap1wiOJvKXr3mhODlg","type":"video","url":"http://www.youtube.com/watch?v=QCSW4isBDL0","thumbnail_height":360,"height":450,"author_name":"Computer Vision Freiburg","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FQCSW4isBDL0%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DQCSW4isBDL0&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FQCSW4isBDL0%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"450\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"}},"created":1419295982,"subreddit_id":"t5_2r3gv","from_kind":null}
{"media":null,"selftext":"","retrieved_on":1441005708,"ups":3,"permalink":"/r/MachineLearning/comments/2q4n8l/machine_learning_resources/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/0ZZ9GcltpAmXm38KwJoCV7KSoAxWLFs-lXrP198D6NQ.jpg","title":"Machine Learning Resources","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419298266","author_flair_text":null,"domain":"startup.ml","hide_score":false,"quarantine":false,"score":3,"edited":false,"secure_media":null,"url":"http://www.startup.ml/resources","created":1419298266,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"author":"gwulfs","from":null,"is_self":false,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"id":"2q4n8l","archived":true,"gilded":0,"name":"t3_2q4n8l","link_flair_css_class":null}
{"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"archived":true,"id":"2q4qhq","link_flair_css_class":null,"gilded":0,"name":"t3_2q4qhq","author":"ankitsablok89","from":null,"is_self":true,"subreddit_id":"t5_2r3gv","created":1419300120,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q4qhq/looking_for_simple_machine_learning_projects_that/","from_id":null,"stickied":false,"quarantine":false,"score":0,"edited":false,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419300120","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Looking for simple machine learning projects that deal with real world data to get started!!!","permalink":"/r/MachineLearning/comments/2q4qhq/looking_for_simple_machine_learning_projects_that/","link_flair_text":null,"media":null,"selftext":"Now that I am over with my exams I want to get my hands dirty with some machine learning and I plan to learn some machine learning algorithms by coding them myself either in Python or Java or C++ and I am looking for some projects that I accomplish in a span of 2 months. Any project suggestions will be helpful. Thanks a lot in advance :).","retrieved_on":1441005666,"ups":0}
{"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Can someone ELI5? What can I actually do (research-wise) by deriving hidden topic models?","author_flair_text":null,"created_utc":"1419312124","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"What can actually be done from there with the information?","media":null,"ups":1,"retrieved_on":1441005401,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q5az5/can_someone_eli5_what_can_i_actually_do/","from":null,"author":"the14thd0ct0r","is_self":true,"num_comments":3,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2q5az5","gilded":0,"link_flair_css_class":null,"id":"2q5az5","archived":true,"score":1,"edited":false,"quarantine":false,"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419312124,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q5az5/can_someone_eli5_what_can_i_actually_do/","stickied":false}
{"permalink":"/r/MachineLearning/comments/2q6viy/see_what_wired_mag_calls_a_new_microsoft/","link_flair_text":null,"retrieved_on":1441004608,"ups":0,"media":null,"selftext":"","hide_score":false,"domain":"aka.ms","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419355348","secure_media_embed":{},"title":"See what WIRED mag calls \"a new Microsoft technology that seems borrowed from the world of Star Trek\u201d","distinguished":null,"thumbnail":"default","downs":0,"stickied":false,"created":1419355348,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://aka.ms/x8sufq","from_id":null,"quarantine":false,"score":0,"edited":false,"id":"2q6viy","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2q6viy","over_18":false,"saved":false,"num_comments":1,"author_flair_css_class":null,"is_self":false,"author":"MLBlogTeam","from":null}
{"from_id":null,"secure_media":null,"url":"http://machinelearningmastery.com/linear-algebra-machine-learning/","created":1419358164,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"edited":false,"score":39,"quarantine":false,"author_flair_css_class":null,"num_comments":8,"saved":false,"over_18":false,"name":"t3_2q7152","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2q7152","from":null,"author":"jasonb","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q7152/linear_algebra_for_machine_learning/","selftext":"","media":null,"ups":39,"retrieved_on":1441004535,"created_utc":"1419358164","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"machinelearningmastery.com","hide_score":false,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/iBKTAHS_Xss_XEYgdg_ApDLZTQv5jDKLS0JepO5BNp0.jpg","distinguished":null,"title":"Linear Algebra for Machine Learning","secure_media_embed":{}}
{"is_self":false,"author":"compsens","from":null,"archived":true,"id":"2q7fm1","link_flair_css_class":null,"gilded":0,"name":"t3_2q7fm1","over_18":false,"saved":false,"num_comments":1,"author_flair_css_class":null,"quarantine":false,"score":7,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","created":1419365334,"from_kind":null,"url":"http://nuit-blanche.blogspot.com/2014/12/deep-fried-convnets.html","secure_media":null,"from_id":null,"secure_media_embed":{},"title":"Deep Fried Convnets (x-post r/CompressiveSensing)","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/H1_pn27hJbCKXf-lyahsIGJbx7hGdzhJxQIWZHOuBXE.jpg","downs":0,"hide_score":false,"domain":"nuit-blanche.blogspot.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419365334","retrieved_on":1441004348,"ups":7,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2q7fm1/deep_fried_convnets_xpost_rcompressivesensing/","link_flair_text":null}
{"stickied":false,"from_id":null,"from_kind":null,"created":1419368992,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q7mxb/semantic_hash_for_newbs/","edited":false,"score":2,"quarantine":false,"name":"t3_2q7mxb","gilded":0,"link_flair_css_class":null,"id":"2q7mxb","archived":true,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"[deleted]","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q7mxb/semantic_hash_for_newbs/","ups":2,"retrieved_on":1441004253,"selftext":"Hi,\n\nSo I have some math background but I'm having a bit of trouble understanding what I need to do.\n\nI get the impression that I have a a multi layer neural network with SDAs in the layers. \n\nMy goal is to find a semantic hash for documents. Maybe there's a pybrains implementation somewhere I could look at when I want to make my own.\n\n\n\nHere are some resources I found.\nhttp://deeplearning.net/tutorial/code/SdA.py\nhttp://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf\n","media":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419368992","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Semantic Hash for newbs","thumbnail":"default","downs":0,"distinguished":null}
{"is_self":false,"from":null,"author":"[deleted]","gilded":0,"link_flair_css_class":null,"name":"t3_2q7ppm","id":"2q7ppm","archived":true,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"score":4,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"created":1419370432,"subreddit_id":"t5_2r3gv","from_kind":null,"url":"http://arxiv.org/pdf/1412.6056.pdf","secure_media":null,"secure_media_embed":{},"title":"Yann LeCun: Training convolutional features on unlabeled video data","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"arxiv.org","author_flair_text":null,"created_utc":"1419370432","subreddit":"MachineLearning","media_embed":{},"ups":4,"retrieved_on":1441004217,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q7ppm/yann_lecun_training_convolutional_features_on/"}
{"created_utc":"1419372486","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"How can I cluster strings with multiple words?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q7tu3/how_can_i_cluster_strings_with_multiple_words/","selftext":"I have a set of about 10^5 short strings (1-6 words each) and I want to cluster them into about 10^3 groups based on the presence of similar words.  All I can think of now is creating a TDM after stemming and then doing hierarchical clustering.  Any better ideas?\n\nThe purpose of this is for regression, so I could use fuzzy clustering or a numeric vector for each string as well.  I just would like to reduce the dimensionality.  ","media":null,"ups":2,"retrieved_on":1441004163,"author_flair_css_class":null,"num_comments":3,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2q7tu3","archived":true,"id":"2q7tu3","from":null,"author":"DemonKingWart","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q7tu3/how_can_i_cluster_strings_with_multiple_words/","secure_media":null,"from_kind":null,"created":1419372486,"subreddit_id":"t5_2r3gv","stickied":false,"edited":false,"score":2,"quarantine":false}
{"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Backprop gradient twice as big as finite difference suggests on neural network. Any advice?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419378880","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hey guys,\n\nI am programming a neural network from scratch in Python, and when I do a numerical check of the backpropagation gradient, I see numbers like the following example: \n\nGradient: -0.0375543629722\nFinite difference: 0.0187042723576 \n\nThe analytical gradient is around twice as big (ignoring the sign) as the finite difference.\n\nI am using the following formula for the numerical gradient:\n\nf(x + eps) - f(x - eps) / 2*eps\n\nFor the single unit sigmoid output layer I am using the following analytical gradient: (target - prediction) * activation_of_hidden_unit\n\nIt's based on the cross-entropy loss, and uses Stochastic Gradient Descent (maybe it has something to do with it?).\n\nThe network trains and generalizes, but I am curious and a little bit worried about this gradient problem.\n\nIf you need more details, please tell.\n\nThank you for your help!","retrieved_on":1441004009,"ups":2,"permalink":"/r/MachineLearning/comments/2q85pn/backprop_gradient_twice_as_big_as_finite/","link_flair_text":null,"author":"ledmmaster","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":4,"author_flair_css_class":null,"archived":true,"id":"2q85pn","link_flair_css_class":null,"name":"t3_2q85pn","gilded":0,"quarantine":false,"edited":false,"score":2,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419378880,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q85pn/backprop_gradient_twice_as_big_as_finite/","from_id":null,"stickied":false}
{"is_self":false,"author":"CireNeikual","from":null,"archived":true,"id":"2q8t59","link_flair_css_class":null,"name":"t3_2q8t59","gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"score":0,"edited":false,"stickied":false,"url":"https://cireneikual.wordpress.com/2014/12/24/continuous-hierarchical-temporal-memory-temporal-inference/","secure_media":null,"created":1419392828,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"title":"Continuous Hierarchical Temporal Memory Part 2","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/6E0YXUncXhFZE-HgC8IH7COjePvS20nr4P30WrcpyiU.jpg","domain":"cireneikual.wordpress.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419392828","author_flair_text":null,"retrieved_on":1441003706,"ups":0,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2q8t59/continuous_hierarchical_temporal_memory_part_2/","link_flair_text":null}
{"permalink":"/r/MachineLearning/comments/2q9d18/forklift_training/","link_flair_text":null,"media":null,"selftext":"","retrieved_on":1441003448,"ups":0,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419405946","author_flair_text":null,"domain":"ausloadshifting.com.au","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Forklift Training","secure_media_embed":{},"secure_media":null,"url":"http://www.ausloadshifting.com.au/forklift","subreddit_id":"t5_2r3gv","created":1419405946,"from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"id":"2q9d18","archived":true,"link_flair_css_class":null,"gilded":0,"name":"t3_2q9d18","author":"aus_andr","from":null,"is_self":false}
{"ups":0,"retrieved_on":1441003240,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q9t41/buy_self_priming_mud_pumps_online/","secure_media_embed":{},"title":"Buy Self Priming Mud Pumps online","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"reliableengineers.net","author_flair_text":null,"created_utc":"1419422026","subreddit":"MachineLearning","media_embed":{},"score":0,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419422026,"from_kind":null,"url":"http://www.reliableengineers.net/self-priming-mud-pumps.php","secure_media":null,"is_self":false,"from":null,"author":"reliableengineers1","gilded":0,"link_flair_css_class":null,"name":"t3_2q9t41","id":"2q9t41","archived":true,"author_flair_css_class":null,"num_comments":4,"over_18":false,"saved":false}
{"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/maaWurI-FSkPwjyJ9mXFHkSPj5TGFVATDmfI-7j01fk.jpg","distinguished":null,"title":"Making Sense of Word2vec Extensions [with code]","secure_media_embed":{},"created_utc":"1419428622","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"radimrehurek.com","hide_score":false,"selftext":"","media":null,"ups":38,"retrieved_on":1441003158,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q9zfp/making_sense_of_word2vec_extensions_with_code/","from":null,"author":"piskvorky","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"name":"t3_2q9zfp","gilded":0,"link_flair_css_class":null,"id":"2q9zfp","archived":true,"edited":false,"score":38,"quarantine":false,"from_id":null,"url":"http://radimrehurek.com/2014/12/making-sense-of-word2vec/","secure_media":null,"from_kind":null,"created":1419428622,"subreddit_id":"t5_2r3gv","stickied":false}
{"is_self":true,"from":null,"author":"SomeoneisWondering","link_flair_css_class":null,"name":"t3_2q9zh9","gilded":0,"archived":true,"id":"2q9zh9","author_flair_css_class":null,"num_comments":57,"saved":false,"over_18":false,"edited":1419550298,"score":23,"quarantine":false,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2q9zh9/who_is_interested_in_working_on_open_source/","subreddit_id":"t5_2r3gv","created":1419428671,"from_kind":null,"title":"Who is interested in working on Open Source Data-Science Projects [As a mentor or contributor].","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1419428671","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":23,"retrieved_on":1441003157,"selftext":"I'd like to work on data-driven open source projects in my spare time. If you are interested in working as a group of people to build large-scale projects during the weekends, exchange ideas, chatting and discussing different models. Please, comment on this post (you can help us learn new things, or we can build things together )\n\nLet's focus on tackling different problems.\n\n\n**UPDATE**\n\nIf you are interested in this initiative, we have started a new subreddit [r/OpenDataScience][1]. We strongly encourage you to join this subreddit and add comments with your ideas, suggestions, and interest!\n\n[1]: http://www.reddit.com/r/OpenDataScience/","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2q9zh9/who_is_interested_in_working_on_open_source/"}
{"hide_score":false,"domain":"vanityfair.com","author_flair_text":null,"created_utc":"1419429593","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Enthusiasts and Skeptics Debate Artificial Intelligence - Vanity Fair","thumbnail":"default","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qa0i2/enthusiasts_and_skeptics_debate_artificial/","ups":0,"retrieved_on":1441003144,"selftext":"","media":null,"gilded":0,"link_flair_css_class":null,"name":"t3_2qa0i2","archived":true,"id":"2qa0i2","author_flair_css_class":null,"num_comments":0,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"[deleted]","stickied":false,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419429593,"secure_media":null,"url":"http://www.vanityfair.com/culture/2014/11/artificial-intelligence-singularity-theory","score":0,"edited":false,"quarantine":false}
{"ups":15,"retrieved_on":1441003075,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qa5t7/feature_extraction_in_deep_neural_nets_and/","secure_media_embed":{},"title":"Feature Extraction in Deep Neural Nets and Renormalization in Quantum Mechanics characterized by same mathematics","thumbnail":"http://b.thumbs.redditmedia.com/5D-0I0l0TakSNbdmKXanr_FNvlsZ2NvZSs6Vnv01SrU.jpg","downs":0,"distinguished":null,"hide_score":false,"domain":"quantamagazine.org","author_flair_text":null,"created_utc":"1419433668","subreddit":"MachineLearning","media_embed":{},"score":15,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"created":1419433668,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"https://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos/","is_self":false,"from":null,"author":"blindConjecture","link_flair_css_class":null,"name":"t3_2qa5t7","gilded":0,"id":"2qa5t7","archived":true,"author_flair_css_class":null,"num_comments":5,"over_18":false,"saved":false}
{"author":"[deleted]","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":1,"archived":true,"id":"2qa919","name":"t3_2qa919","link_flair_css_class":null,"gilded":0,"quarantine":false,"score":0,"edited":1419436559,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419435917,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qa919/python_neural_net_fails_to_train_gives_roughly/","from_id":null,"stickied":false,"distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"Python Neural Net Fails to Train. Gives roughly the same output values on every output neuron.","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419435917","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"Hi all,\n\nJust wrote up my first Neural Network Class in python. Everything as far as I can tell should work, but there is some bug in it that I can't seem to find(Probably staring me right in the face). \n\nI first tried it on 10,000 examples of the MNIST data, then again when trying to replicate the sign function, and again when trying to replicate a XOR Gate. Every time, regardless of the # of epochs, it always produces output from all the output neurons(regardless of how many there may be) that are all roughly the same value, but the cost function seems to be going down. \n\nI am using batch gradient descent, all done using vectors(no loop for each training example).  \n\n    #Neural Network Class\n    \n    import numpy as np\n    \n    \n    \n    class NeuralNetwork:\n        \n        #methods\n        def __init__(self,layer_shape):\n            #Useful Network Info\n            self.__layer_shape = layer_shape\n            self.__layers = len(layer_shape)\n            \n            #Initialize Random Weights\n            self.__weights = [] \n            self.__weight_sizes = []\n            for i in range(len(layer_shape)-1):\n                current_weight_size = (layer_shape[i+1],layer_shape[i]+1)\n                self.__weight_sizes.append(current_weight_size)\n                self.__weights.append(np.random.normal(loc=0.1,scale=0.1,size=current_weight_size))\n            \n        def sigmoid(self,z):\n            return (1/(1+np.exp(-z)))\n        \n        def sig_prime(self,z):\n            return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))\n        \n        \n        def Feedforward(self,input,Train=False):\n            self.__input_cases = np.shape(input)[0]\n            \n            #Empty list to hold the output of every layer.\n            output_list = []\n            #Appends the output of the the 1st input layer.\n            output_list.append(input)\n            \n            for i in range(self.__layers-1):\n                if i == 0:\n                    output = self.sigmoid(np.dot(np.concatenate((np.ones((self.__input_cases,1)),input),1),self.__weights[0].T))\n                    output_list.append(output)\n                else:\n                    output = self.sigmoid(np.dot(np.concatenate((np.ones((self.__input_cases,1)),output),1),self.__weights[i].T))                 \n                    output_list.append(output)\n            \n            #Returns the final output if not training.         \n            if Train == False:\n                return output_list[-1]\n            #Returns the entire output_list if need for training\n            else:\n                return output_list\n        \n        def CostFunction(self,input,target,error_func=1):\n            \"\"\"Gives the cost of using a particular weight matrix \n            based off of the input and targeted output\"\"\"\n            \n            #Run the network to get output using current theta matrices.\n            output = self.Feedforward(input)\n            \n            \n            #####Allows user to choose Cost Functions.##### \n            \n            #\n            #Log Based Error Function\n            #\n            if error_func == 0:\n                error = np.multiply(-target,np.log(output))-np.multiply((1-target),np.log(1-output))\n                total_error = np.sum(np.sum(error))\n            #    \n            #Squared Error Cost Function\n            #\n            elif error_func == 1:\n                error = (target - output)**2\n                total_error = 0.5 * np.sum(np.sum(error))\n                \n            return total_error\n        \n        def Weight_Grad(self,input,target,output_list):\n            \n                    #Finds the Deltas for Each Layer\n                    # \n                    deltas = []\n                    for i in range(self.__layers - 1):\n                        #Finds Error Delta for the last layer\n                        if i == 0:\n                            \n                            error = (target-output_list[-1])\n                            \n                            error_delta = -1*np.multiply(error,np.multiply(output_list[-1],(1-output_list[-1])))\n                            deltas.append(error_delta)\n                        #Finds Error Delta for the hidden layers   \n                        else:\n                            #Weight matrices have bias values removed\n                            error_delta = np.multiply(np.dot(deltas[-1],self.__weights[-i][:,1:]),output_list[-i-1]*(1-output_list[-i-1]))\n                            deltas.append(error_delta)\n                            \n                    #\n                    #Finds the Deltas for each Weight Matrix\n                    #\n                    Weight_Delta_List = []\n                    deltas.reverse()\n                    for i in range(len(self.__weights)):\n                         \n                        current_weight_delta = (1/self.__input_cases) * np.dot(deltas[i].T,np.concatenate((np.ones((self.__input_cases,1)),output_list[i]),1))\n                        Weight_Delta_List.append(current_weight_delta)\n                        #print(\"Weight\",i,\"Delta:\",\"\\n\",current_weight_delta)\n                        #print()\n                         \n                    #\n                    #Combines all Weight Deltas into a single row vector\n                    #\n                    Weight_Delta_Vector = np.array([[]])\n                    for i in Weight_Delta_List:\n                        \n                        Weight_Delta_Vector = np.concatenate((Weight_Delta_Vector,np.reshape(i,(1,-1))),1)\n                    return Weight_Delta_List        \n            \n        def Train(self,input_data,target):\n            #\n            #Gradient Checking:\n            #\n            \n            #First Get Gradients from first iteration of Back Propagation \n            output_list = self.Feedforward(input_data,Train=True)\n            self.__input_cases = np.shape(input_data)[0]\n            \n            Weight_Delta_List = self.Weight_Grad(input_data,target,output_list)  \n            \n            #Creates List of Gradient Approx arrays set to zero.\n            grad_approx_list = []\n            for i in self.__weight_sizes:\n                current_grad_approx = np.zeros(i)\n                grad_approx_list.append(current_grad_approx)\n            \n            \n            #Compute Approx. Gradient for every Weight Change\n            for W in range(len(self.__weights)):\n                for index,value in np.ndenumerate(self.__weights[W]):\n                    orig_value = self.__weights[W][index]      #Saves the Original Value\n                    print(\"Orig Value:\", orig_value)\n                    \n                    #Sets weight to  weight +/- epsilon\n                    self.__weights[W][index] = orig_value+.00001\n                    cost_plusE = self.CostFunction(input_data, target)\n                    \n                    self.__weights[W][index] = orig_value-.00001\n                    cost_minusE = self.CostFunction(input_data, target)\n                    \n                    #Solves for grad approx:\n                    grad_approx = (cost_plusE-cost_minusE)/(2*.00001)\n                    grad_approx_list[W][index] = grad_approx\n                    \n                    #Sets Weight Value back to its original value\n                    self.__weights[W][index] = orig_value\n            \n               \n            #\n            #Print Gradients from Back Prop. and Grad Approx. side-by-side:\n            #\n            \n            print(\"Back Prop. Grad\",\"\\t\",\"Grad. Approx\")\n            print(\"-\"*15,\"\\t\",\"-\"*15)\n            for W in range(len(self.__weights)):\n                for index, value in np.ndenumerate(self.__weights[W]):\n                    print(self.__weights[W][index],\"\\t\"*3,grad_approx_list[W][index])\n            \n            print(\"\\n\"*3)\n            input_ = input(\"Press Enter to continue:\")\n           \n            \n            #\n            #Perform Weight Updates for X number of Iterations\n            #\n            for i in range(10000):\n            #Run the network\n                output_list = self.Feedforward(input_data,Train=True)\n                self.__input_cases = np.shape(input_data)[0]\n                \n                Weight_Delta_List = self.Weight_Grad(input_data,target,output_list)\n                           \n                    \n                for w in range(len(self.__weights)):\n                    #print(self.__weights[w])\n                    #print(Weight_Delta_List[w])\n                    self.__weights[w] = self.__weights[w] - (.01*Weight_Delta_List[w]) \n            \n                   \n            print(\"Done\")\n\nI even implememented Gradient Checking and the values are different, and I thought I would try replacing the Back Propagation updates with the Approx. Gradient Checking values, but that gave the same results, causing me to doubt even my Gradient Checking code. \n\nHere are some of the values being produced when training for the XOR Gate:\nBack Prop. Grad: 0.0756102610697 \t0.261814503398 \t     0.0292734023876 \t\t\t\n\nGrad Approx:      0.05302210631166       0.0416095559674      0.0246847342122 \n\nCost:\nBefore Training: 0.508019225507\nAfter Training 0.50007095103 (After 10000 Epochs)\n\nOutput for 4 different examples(after training):\n\n[ 0.49317733]\n [ 0.49294556]\n [ 0.50489004]\n [ 0.50465824]\n\nSo my question is, is there any obvious problem with my Back Propagation, or my gradient checking? Are there any usual problems when a ANN shows these symptoms(Outputs are all roughly the same/Cost is going down)? ","retrieved_on":1441003033,"ups":0,"permalink":"/r/MachineLearning/comments/2qa919/python_neural_net_fails_to_train_gives_roughly/","link_flair_text":null}
{"permalink":"/r/MachineLearning/comments/2qabc3/regression_without_negative_examples/","link_flair_text":null,"retrieved_on":1441003003,"ups":4,"media":null,"selftext":"","hide_score":false,"domain":"efavdb.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419437369","secure_media_embed":{},"title":"Regression without negative examples","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/9YZNQQJbg2oEZ_bdJnuZpfTrklJIDCEPsDKT48-UMDE.jpg","downs":0,"stickied":false,"from_kind":null,"created":1419437369,"subreddit_id":"t5_2r3gv","url":"http://efavdb.com/methods-regression-without-negative-examples/","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":4,"id":"2qabc3","archived":true,"name":"t3_2qabc3","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":0,"author_flair_css_class":null,"is_self":false,"author":"efavdb","from":null}
{"is_self":false,"author":"mttd","from":null,"id":"2qayxv","archived":true,"gilded":0,"name":"t3_2qayxv","link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"score":56,"edited":false,"stickied":false,"url":"http://blogs.technet.com/b/machinelearning/archive/2014/12/16/machine-learning-trends-from-nips-2014.aspx","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419450559,"from_id":null,"title":"Machine Learning Trends from NIPS 2014","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/ftHpWQAnv3s0dZOYmKtT5se6GYnxC1PtnSwz-u2EnQg.jpg","domain":"blogs.technet.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419450559","author_flair_text":null,"retrieved_on":1441002697,"ups":56,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2qayxv/machine_learning_trends_from_nips_2014/","link_flair_text":null}
{"retrieved_on":1441002453,"ups":4,"media":{"type":"youtube.com","oembed":{"title":"Jeremy Howard: The wonderful and terrifying implications of computers that can learn","thumbnail_url":"http://i.ytimg.com/vi/t4kyRyKyOpo/hqdefault.jpg","width":600,"thumbnail_width":480,"description":"What happens when we teach a computer how to learn? Technologist Jeremy Howard shares some surprising new developments in the fast-moving field of deep learning, a technique that can give computers the ability to learn Chinese, or to recognize objects in photos, or to help think through a medical diagnosis.","provider_url":"http://www.youtube.com/","provider_name":"YouTube","author_name":"TED","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=t4kyRyKyOpo","type":"video","author_url":"http://www.youtube.com/user/TEDtalksDirector","thumbnail_height":360}},"selftext":"","permalink":"/r/MachineLearning/comments/2qbht2/the_wonderful_and_terrifying_implications_of/","link_flair_text":null,"secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"height":338,"scrolling":false},"title":"The wonderful and terrifying implications of computers that can learn","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"youtube.com","subreddit":"MachineLearning","media_embed":{"scrolling":false,"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"author_flair_text":null,"created_utc":"1419462124","quarantine":false,"score":4,"edited":false,"stickied":false,"subreddit_id":"t5_2r3gv","created":1419462124,"from_kind":null,"secure_media":{"type":"youtube.com","oembed":{"provider_name":"YouTube","provider_url":"http://www.youtube.com/","description":"What happens when we teach a computer how to learn? Technologist Jeremy Howard shares some surprising new developments in the fast-moving field of deep learning, a technique that can give computers the ability to learn Chinese, or to recognize objects in photos, or to help think through a medical diagnosis.","thumbnail_width":480,"width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","title":"Jeremy Howard: The wonderful and terrifying implications of computers that can learn","thumbnail_height":360,"author_url":"http://www.youtube.com/user/TEDtalksDirector","type":"video","url":"http://www.youtube.com/watch?v=t4kyRyKyOpo","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Ft4kyRyKyOpo%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dt4kyRyKyOpo&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2Ft4kyRyKyOpo%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"author_name":"TED"}},"url":"http://www.youtube.com/watch?v=t4kyRyKyOpo","from_id":null,"is_self":false,"author":"pateras","from":null,"archived":true,"id":"2qbht2","name":"t3_2qbht2","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0}
{"from_id":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419471518,"secure_media":null,"url":"http://www.youtube.com/attribution_link?a=azHB6fCUx58&amp;u=%2Fwatch%3Fv%3DtIs_uR7SMq8%26feature%3Dshare","stickied":false,"score":0,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":2,"over_18":false,"saved":false,"name":"t3_2qbv4z","link_flair_css_class":null,"gilded":0,"id":"2qbv4z","archived":true,"from":null,"author":"zzaixcarwash","is_self":false,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qbv4z/wash_car_machine_32kw_industrial_steam_cleaner/","selftext":"","media":null,"ups":0,"retrieved_on":1441002280,"author_flair_text":null,"created_utc":"1419471518","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"youtube.com","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Wash car machine --32kw Industrial steam cleaner"}
{"quarantine":false,"edited":false,"score":0,"url":"http://rollformingmachinery.wordpress.com/2014/12/04/roll-forming-machine/","secure_media":null,"created":1419490744,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false,"author":"rollformingmachinery","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"id":"2qcj69","archived":true,"name":"t3_2qcj69","link_flair_css_class":null,"gilded":0,"media":null,"selftext":"","retrieved_on":1441001969,"ups":0,"permalink":"/r/MachineLearning/comments/2qcj69/roll_forming_machine/","link_flair_text":null,"distinguished":null,"downs":0,"thumbnail":"default","title":"Roll Forming Machine","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419490744","author_flair_text":null,"domain":"rollformingmachinery.wordpress.com","hide_score":false}
{"retrieved_on":1441001492,"ups":17,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2qdjz8/machine_learning_with_statistical_and_causal/","link_flair_text":null,"secure_media_embed":{},"title":"Machine Learning With Statistical And Causal Methods","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/BYFNG3dXWc-enekqRJS3Qf129o7zfLwTsY6FslsVc4A.jpg","downs":0,"hide_score":false,"domain":"machinelearningmastery.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419529297","quarantine":false,"score":17,"edited":false,"stickied":false,"created":1419529297,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://machinelearningmastery.com/machine-learning-statistical-causal-methods/","from_id":null,"is_self":false,"author":"alexeyr","from":null,"id":"2qdjz8","archived":true,"link_flair_css_class":null,"name":"t3_2qdjz8","gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0}
{"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qe3qa/question_about_what_computer_to_buy_if_im_going/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1419542222,"from_id":null,"stickied":false,"quarantine":false,"score":5,"edited":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":36,"archived":true,"id":"2qe3qa","link_flair_css_class":null,"name":"t3_2qe3qa","gilded":0,"author":"[deleted]","from":null,"is_self":true,"permalink":"/r/MachineLearning/comments/2qe3qa/question_about_what_computer_to_buy_if_im_going/","link_flair_text":null,"media":null,"selftext":"I'm halfway into Andrew ng coursera course ( currently watching previous videos . I started late so I'll be getting the SOA by joining the next batch) \n\nI'm hoping that after doing this course and reading some more , I'll be in a position to start attempting kaggle challenges ? Or be in a position to make a publishable paper ( nothing grand ) in a local conference. \n\nI think I'll start with deep learning after finishing the coursera programme. \n\nNow my noob question is , can I accomplish the above goals with a normal laptop ? \n\nI don't have access to any labs and I cannot pay dollars for any cloud services. So please advice me which areas from machine learning I can focus on with my limited hardware.\n\nAlso be brutally honest if I'm being over ambitious in thinking that I will be in any position to start solving problems after a coursera programme. \n\nThank you","retrieved_on":1441001176,"ups":5,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419542222","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"Question about what computer to buy if I'm going to do a project in machine learning","secure_media_embed":{}}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qey2p/regression_problems_with_extremely_right_skewed/","ups":3,"retrieved_on":1441000782,"selftext":"Hey /r/MachineLearning ,\n\nI'm starting a project soon that will require some supervised machine learning. the target variable is severely right skewed. The target looks like it almost follows a Poisson distribution, even though its not count data. My features will be almost 100% categorical. \n\nThis is the first time I've really worked on a problem such as this. My background is in basic econometrics where assumptions of normally distributed everything is really never violated. \n\nAre there any sources out there for techniques or algorithms that work particularly well in a case such as mine? ","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1419563661","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Regression problems with extremely right skewed targets","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"stickied":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qey2p/regression_problems_with_extremely_right_skewed/","from_kind":null,"created":1419563661,"subreddit_id":"t5_2r3gv","edited":false,"score":3,"quarantine":false,"gilded":0,"name":"t3_2qey2p","link_flair_css_class":null,"id":"2qey2p","archived":true,"author_flair_css_class":null,"num_comments":8,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"AWKWARD_HANDS_GUY"}
{"is_self":true,"author":"odkken","from":null,"id":"2qfd1c","archived":true,"link_flair_css_class":null,"name":"t3_2qfd1c","gilded":0,"over_18":false,"saved":false,"num_comments":4,"author_flair_css_class":null,"quarantine":false,"edited":1419574888,"score":6,"stickied":false,"subreddit_id":"t5_2r3gv","created":1419574658,"from_kind":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qfd1c/how_to_use_adaboost_for_more_complex/","secure_media":null,"from_id":null,"secure_media_embed":{},"title":"How to use AdaBoost for more \"complex\" classifications?","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419574658","retrieved_on":1441000588,"ups":6,"media":null,"selftext":"In [this](http://www.wired.com/2014/01/how-to-hack-okcupid/all/) article, Chris McKinlay says he used AdaBoost to choose the proper \"importances\" of questions he answered on okcupid.\n\nIf you haven't read and don't want to read the article, or are unfamiliar with okcupid and the question system, here's the data and problem he had:\n\nThe goal is to \"match\" as highly as possible with as many users as possible, each of whom may have answered an arbitrary number of questions.  These questions may have between 2 and 4 answers each, and for the sake of simplicity, let's pretend that the formula for a match% between you and another user is the sum of your matching answers, divided by the total number of matching questions (denominator = # of questions you have in common, numerator = # of those questions you answered the same way).  The real formula is slightly more complex, but the approach would be the same regarding \"picking\" a correct answer (he actually used boosting to find the ideal \"importance\" to place on a given question, rather than the right answer).\n\nIn any case, the point is you want to pick a certain value for each question, such that you maximize your match% with as many users as possible - something you might quantify by the sum of match% over all users.\n\nNow I've watched the [MIT course on AI](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi) up to and including the lecture on boosting, but I don't understand how you would apply it to a problem like this.  Honestly I don't even know where to begin with choosing rules for the weak learners.  I don't have any \"rules\" about what values to choose for each question (if the user is under 5'5, choose A, etc) - I'm just trying to fit the data I have.\n\nIs this not the way boosting is supposed to be used?  Is there likely some other optimization left out of how he figured this out?","permalink":"/r/MachineLearning/comments/2qfd1c/how_to_use_adaboost_for_more_complex/","link_flair_text":null}
{"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"name":"t3_2qfmfv","link_flair_css_class":null,"gilded":0,"id":"2qfmfv","archived":true,"from":null,"author":"hnbljq","is_self":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419583576,"from_kind":null,"url":"http://www.youtube.com/watch?v=EOhsZ6CnQtU&amp;feature=share","secure_media":{"type":"youtube.com","oembed":{"title":"Regrinding and fine grinding equipment-tower mill","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FEOhsZ6CnQtU%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","thumbnail_width":480,"description":"Offical website:\u3010http://www.bailingmachinery.com/\u3011 Alibaba website:\u3010http://zzbailing.en.alibaba.com/\u3011 Product webpage:\u3010http://www.bailingmachinery.com/products/ballmill/250.html\u3011 Tower mill is a kind of vertical mill. Bailing brand tower mill is composed of vertical cylinder, spiral vane and driving device, etc. It is our newly designed equipment after optimizing all the technical data from home and abroad. Features: 1.","provider_name":"YouTube","provider_url":"http://www.youtube.com/","author_name":"Henan Bailing","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FEOhsZ6CnQtU%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEOhsZ6CnQtU&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FEOhsZ6CnQtU%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=EOhsZ6CnQtU","type":"video","author_url":"http://www.youtube.com/channel/UCn-hoE2DcPwVI04bYFSOvVA","thumbnail_height":360}},"stickied":false,"score":0,"edited":false,"quarantine":false,"author_flair_text":null,"created_utc":"1419583576","subreddit":"MachineLearning","media_embed":{"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FEOhsZ6CnQtU%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEOhsZ6CnQtU&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FEOhsZ6CnQtU%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"hide_score":false,"domain":"youtube.com","thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FEOhsZ6CnQtU%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEOhsZ6CnQtU&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FEOhsZ6CnQtU%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"title":"Regrinding and fine grinding equipment-tower mill","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qfmfv/regrinding_and_fine_grinding_equipmenttower_mill/","selftext":"","media":{"type":"youtube.com","oembed":{"height":338,"author_name":"Henan Bailing","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FEOhsZ6CnQtU%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEOhsZ6CnQtU&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FEOhsZ6CnQtU%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","author_url":"http://www.youtube.com/channel/UCn-hoE2DcPwVI04bYFSOvVA","type":"video","url":"http://www.youtube.com/watch?v=EOhsZ6CnQtU","thumbnail_height":360,"width":600,"thumbnail_url":"http://i.ytimg.com/vi/EOhsZ6CnQtU/hqdefault.jpg","title":"Regrinding and fine grinding equipment-tower mill","description":"Offical website:\u3010http://www.bailingmachinery.com/\u3011 Alibaba website:\u3010http://zzbailing.en.alibaba.com/\u3011 Product webpage:\u3010http://www.bailingmachinery.com/products/ballmill/250.html\u3011 Tower mill is a kind of vertical mill. Bailing brand tower mill is composed of vertical cylinder, spiral vane and driving device, etc. It is our newly designed equipment after optimizing all the technical data from home and abroad. Features: 1.","thumbnail_width":480,"provider_name":"YouTube","provider_url":"http://www.youtube.com/"}},"ups":0,"retrieved_on":1441000467}
{"domain":"stat.cmu.edu","hide_score":false,"created_utc":"1419607094","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Videos from Convex Optimization @CMU","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qg77o/videos_from_convex_optimization_cmu/","ups":39,"retrieved_on":1441000197,"selftext":"","media":null,"gilded":0,"link_flair_css_class":null,"name":"t3_2qg77o","id":"2qg77o","archived":true,"num_comments":5,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"matiskay","stickied":false,"from_id":null,"secure_media":null,"url":"http://www.stat.cmu.edu/~ryantibs/convexopt/#videos","created":1419607094,"subreddit_id":"t5_2r3gv","from_kind":null,"edited":false,"score":39,"quarantine":false}
{"domain":"dataelixir.com","hide_score":false,"created_utc":"1419620774","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Data Elixir, Issue 15: machine learning in the real-world, how to trick a neural net, data science resources, best data viz tools of 2014","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qgrqd/data_elixir_issue_15_machine_learning_in_the/","ups":0,"retrieved_on":1440999931,"selftext":"","media":null,"link_flair_css_class":null,"name":"t3_2qgrqd","gilded":0,"archived":true,"id":"2qgrqd","num_comments":1,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"lonriesberg","stickied":false,"from_id":null,"url":"http://dataelixir.com/issues/15/?referred=true","secure_media":null,"created":1419620774,"subreddit_id":"t5_2r3gv","from_kind":null,"edited":false,"score":0,"quarantine":false}
{"from":null,"author":"vincentg64","is_self":true,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2qgtfu","gilded":0,"archived":true,"id":"2qgtfu","edited":false,"score":0,"quarantine":false,"from_id":null,"from_kind":null,"created":1419621821,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2qgtfu/64_new_external_resources_and_articles_about/","secure_media":null,"stickied":false,"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"64 new external resources and articles about machine learning, data science, and big data","author_flair_text":null,"created_utc":"1419621821","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"http://www.datascienceworld.com/profiles/blogs/55-new-external-resources-and-articles-about-data-science-big","media":null,"ups":0,"retrieved_on":1440999909,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qgtfu/64_new_external_resources_and_articles_about/"}
{"distinguished":null,"downs":0,"thumbnail":"default","title":"Jeremy Howard TED talk: The wonderful and terrifying implications of computers that can learn","secure_media_embed":{},"media_embed":{"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fembed.ted.com%2Ftalks%2Fjeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn.html&amp;url=http%3A%2F%2Fwww.ted.com%2Ftalks%2Fjeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn&amp;image=http%3A%2F%2Fimg.tedcdn.com%2Fr%2Fimages.ted.com%2Fimages%2Fted%2F4707d8e88ba824e4a9ad05ee2446d93576117d21_2880x1620.jpg%3Fc%3D1280%252C720%26ll%3D1%26quality%3D89%26w%3D1200&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=ted\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"subreddit":"MachineLearning","created_utc":"1419633561","author_flair_text":null,"domain":"ted.com","hide_score":false,"media":{"type":"ted.com","oembed":{"height":338,"author_name":"Jeremy Howard","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fembed.ted.com%2Ftalks%2Fjeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn.html&amp;url=http%3A%2F%2Fwww.ted.com%2Ftalks%2Fjeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn&amp;image=http%3A%2F%2Fimg.tedcdn.com%2Fr%2Fimages.ted.com%2Fimages%2Fted%2F4707d8e88ba824e4a9ad05ee2446d93576117d21_2880x1620.jpg%3Fc%3D1280%252C720%26ll%3D1%26quality%3D89%26w%3D1200&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=ted\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","type":"video","thumbnail_height":675,"thumbnail_url":"http://img.tedcdn.com/r/images.ted.com/images/ted/4707d8e88ba824e4a9ad05ee2446d93576117d21_2880x1620.jpg?c=1280%2C720&amp;ll=1&amp;quality=89&amp;w=1200","width":600,"title":"The wonderful and terrifying implications of computers that can learn","description":"What happens when we teach a computer how to learn? Technologist Jeremy Howard shares some surprising new developments in the fast-moving field of deep learning, a technique that can give computers the ability to learn Chinese, or to recognize objects in photos, or to help think through a medical diagnosis.","thumbnail_width":1200,"provider_url":"http://www.ted.com","provider_name":"TED"}},"selftext":"","retrieved_on":1440999663,"ups":0,"permalink":"/r/MachineLearning/comments/2qhcgc/jeremy_howard_ted_talk_the_wonderful_and/","link_flair_text":null,"author":"whyoy","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":2,"archived":true,"id":"2qhcgc","link_flair_css_class":null,"gilded":0,"name":"t3_2qhcgc","quarantine":false,"edited":false,"score":0,"url":"http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn","secure_media":null,"created":1419633561,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false}
{"permalink":"/r/MachineLearning/comments/2qhtze/algebraic_approach_to_classification/","link_flair_text":null,"retrieved_on":1440999436,"ups":38,"media":null,"selftext":"So I was reading [this interview with a Kaggle winner](http://blog.kaggle.com/2014/07/28/11th-hour-win-of-greek-media-monitoring-challenge/). This chap is a Russian professor who used an approach based on what he calls \"the algebraic approach to classification\".    \n&gt;My algorithm consisted of two parts: linear combinations of regressors for each label and a binary decision rule. Such algorithms are very popular in Russia, for example in \u00abthe algebraic approach to classification\u00bb. This technique had been developing by academician Yuri Zhuravlev and his scientific school since 1978 and is unknown in Europe and USA.   \n     \nThe bloke he's talking about is a Russian mathematician named [Yuri Zhuravlev](https://en.wikipedia.org/wiki/Yuri_Zhuravlev). I've tried to look this up, but I tend to run into really obscure mathematical papers that were seemingly translated from Russian using Google translate so haven't mustered the courage to take them on yet. Does anyone know what he's talking about here?","domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419645155","author_flair_text":null,"title":"Algebraic approach to classification?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","stickied":false,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qhtze/algebraic_approach_to_classification/","subreddit_id":"t5_2r3gv","created":1419645155,"from_kind":null,"from_id":null,"quarantine":false,"edited":false,"score":38,"archived":true,"id":"2qhtze","name":"t3_2qhtze","link_flair_css_class":null,"gilded":0,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":7,"is_self":true,"author":"[deleted]","from":null}
{"gilded":0,"name":"t3_2qilk3","link_flair_css_class":null,"id":"2qilk3","archived":true,"num_comments":17,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":true,"from":null,"author":"[deleted]","stickied":false,"from_id":null,"from_kind":null,"created":1419665453,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qilk3/has_anyone_worked_on_a_machine_learning_project/","score":6,"edited":false,"quarantine":false,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419665453","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Has anyone worked on a machine learning project and towards the end realised that there was a better way ? What do you usually do in such situations ?","thumbnail":"default","downs":0,"distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qilk3/has_anyone_worked_on_a_machine_learning_project/","ups":6,"retrieved_on":1440999078,"selftext":"For example you realised that your choice of classifier was not the best , you could have chosen a better way for dimensionality reduction. Just wanted to hear some stories.\n\nI'm looking to start a project and since I'm new to this field , I'm mostly looking to solve existing problems with a better approach. \n\n","media":null}
{"author":"[deleted]","from":null,"is_self":false,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":0,"id":"2qipux","archived":true,"name":"t3_2qipux","link_flair_css_class":null,"gilded":0,"quarantine":false,"edited":false,"score":1,"url":"http://arxiv.org/pdf/1412.7210.pdf","secure_media":null,"created":1419670028,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"distinguished":null,"downs":0,"thumbnail":"default","title":"A fully connected, recurrent autoencoder; or: \"I connected everything, and it worked!\"","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419670028","author_flair_text":null,"domain":"arxiv.org","hide_score":false,"media":null,"selftext":"","retrieved_on":1440999023,"ups":1,"permalink":"/r/MachineLearning/comments/2qipux/a_fully_connected_recurrent_autoencoder_or_i/","link_flair_text":null}
{"from":null,"author":"[deleted]","is_self":true,"author_flair_css_class":null,"num_comments":5,"saved":false,"over_18":false,"gilded":0,"name":"t3_2qjf0z","link_flair_css_class":null,"archived":true,"id":"2qjf0z","score":22,"edited":1419697712,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qjf0z/a_first_kit_of_machine_learning_resources/","created":1419697073,"subreddit_id":"t5_2r3gv","from_kind":null,"stickied":false,"downs":0,"thumbnail":"default","distinguished":null,"title":"A First Kit of Machine Learning Resources","secure_media_embed":{},"created_utc":"1419697073","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"The Machine Learning Salon Kit contains 150 pages of useful websites to start machine learning. It's free from any comment, registration or advertising.\n[MachineLearningSalonKit](http://www.machinelearningsalon.org/download.html)\n\nThe Machine Learning Salon is collecting worldwide information from websites such as Reddit, DataTau, LinkedIn, Google Scolar, University websites, ... and sometimes we are posting new information too (PyData NYC 2014, Royal Society, etc.).","media":null,"ups":22,"retrieved_on":1440998696,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qjf0z/a_first_kit_of_machine_learning_resources/"}
{"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419701233","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"default","downs":0,"secure_media_embed":{},"title":"FTRL proximal questions (theory and implementation)","permalink":"/r/MachineLearning/comments/2qjl7k/ftrl_proximal_questions_theory_and_implementation/","link_flair_text":null,"media":null,"selftext":"Hi,  \nI've been trying to implement FTRL proximal for the avuzu competition(ads click rate prediction) on Kaggle. However I'm a begginer in ML and I'm unsure about some aspects of this algorithm.  \n  \n&amp;nbsp;\n\nI've been using this [paper](http://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf).  \n  \n&amp;nbsp;\n\nAm I justified in using FTRL proximal instead of simple online gradient descent in this particular contest considering there are only about 30 features? From what I've understood the advantage over OGD is that it introduces sparsity and reduces memory usage when storing the weights. But since there are only 30 features and we are only storing 2 weight vectors at any time in OGD (unless I'm mistaken about that part) why go through the trouble of using FTRL?  \nAm I missing another advantage or did I misunderstand something?  \n   \n&amp;nbsp;\n\nOn a side note the paper talks about feature vectors with billions of coefficient and that really boggles my mind. How do they even find that many features in the behavior of online users... even using high polynomial features...  \n  \n&amp;nbsp;\n\nMy second problem is the implementation itself, there is a nice one in the kaggle forum but I figured I'd try to do my own before looking at it. Here is [mine](https://github.com/EtienneDesticourt/Kaggle-Avuzu/blob/master/FTRLProx.py), lines 50 to 80 is ftrl proximal.  \nI can't seem to pinpoint the problem, I've tried with up to 100 000 training examples, many different parameters( alpha, beta, l1, l2), with as few as one feature but nothing seems to make it converge and I have an accuracy of 0.5 whereas simple batch logistic regression gets ~0.94. So all I can think of is an error in my implementation. I've copied the Algorithm 1 from the paper page 2 but before (and after) doing so I've read multiple times the theory(page 2-3) to try and understand what I was doing, yet it still doesn't work.\nI wish I could pinpoint more precisely my problem before asking but I just don't know what's wrong, I'm sorry.  \n  \n&amp;nbsp;\n\nI'd be very grateful for any help answering these two questions.\n\n&amp;nbsp;\n\nCheers\n\n","retrieved_on":1440998616,"ups":1,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"id":"2qjl7k","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2qjl7k","author":"[deleted]","from":null,"is_self":true,"from_kind":null,"created":1419701233,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qjl7k/ftrl_proximal_questions_theory_and_implementation/","from_id":null,"stickied":false,"quarantine":false,"score":1,"edited":1419702473}
{"media":null,"selftext":"","retrieved_on":1440998511,"ups":8,"permalink":"/r/MachineLearning/comments/2qjtck/thesis_quantum_algorithms_for_linear_algebra_and/","link_flair_text":null,"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/hHnJs-swuz4yoD3eRPqP7kXrpy7Gl0hyYNljLfqA5uk.jpg","downs":0,"secure_media_embed":{},"title":"Thesis: Quantum Algorithms for Linear Algebra and Machine Learning; Anupam Prakash (x-post r/CompressiveSensing )","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419706429","hide_score":false,"domain":"nuit-blanche.blogspot.com","quarantine":false,"score":8,"edited":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419706429,"secure_media":null,"url":"http://nuit-blanche.blogspot.com/2014/12/thesis-quantum-algorithms-for-linear.html","from_id":null,"stickied":false,"author":"compsens","from":null,"is_self":false,"over_18":false,"saved":false,"num_comments":2,"author_flair_css_class":null,"archived":true,"id":"2qjtck","link_flair_css_class":null,"gilded":0,"name":"t3_2qjtck"}
{"stickied":false,"created":1419708308,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qjwf7/adaboost_on_svm/","from_id":null,"quarantine":false,"score":2,"edited":false,"archived":true,"id":"2qjwf7","link_flair_css_class":null,"name":"t3_2qjwf7","gilded":0,"over_18":false,"saved":false,"num_comments":7,"author_flair_css_class":null,"is_self":true,"author":"nukich74","from":null,"permalink":"/r/MachineLearning/comments/2qjwf7/adaboost_on_svm/","link_flair_text":null,"retrieved_on":1440998471,"ups":2,"media":null,"selftext":"Adaboost is recommened to use with weak classifiers. I also heard, that it is not good idea to use it with SVM. Looking on algo i cant guess why it is bad idea. Can anyone explain?","hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419708308","secure_media_embed":{},"title":"Adaboost on svm","distinguished":null,"thumbnail":"self","downs":0}
{"downs":0,"thumbnail":"self","distinguished":null,"title":"How to cluster text sentences unsupervised?","secure_media_embed":{},"created_utc":"1419722332","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"selftext":"Hi everyone.\n\nI need to take about 200k sentences and cluster them to groups based on text similarity.  \nI don't want to specify a constant number of clusters - I want it to just figure out groups based on a \"tolerance\" variable that i could play with.\n\nI read about word2vec/doc2vec that pays attention to the context of the words and not just as a random bag of words and I think it could help me, but i couldn't figure out how to use it for clustering purposes.\n\nAny help is welcome, thanks !","media":null,"ups":5,"retrieved_on":1440998174,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qkjd5/how_to_cluster_text_sentences_unsupervised/","from":null,"author":"desegel","is_self":true,"author_flair_css_class":null,"num_comments":7,"saved":false,"over_18":false,"name":"t3_2qkjd5","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2qkjd5","edited":false,"score":5,"quarantine":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qkjd5/how_to_cluster_text_sentences_unsupervised/","secure_media":null,"created":1419722332,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false}
{"secure_media_embed":{},"title":"#Future #MIT #CarnegieMellon #MachineLearning #AI #2015 Eric Miley, #MBA \"On Machine Learning and the Next Wave of Innovation\"","thumbnail":"default","downs":0,"distinguished":null,"hide_score":false,"domain":"linkedin.com","author_flair_text":null,"created_utc":"1419745715","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1440997727,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qlhu6/future_mit_carnegiemellon_machinelearning_ai_2015/","is_self":false,"from":null,"author":"[deleted]","link_flair_css_class":null,"gilded":0,"name":"t3_2qlhu6","id":"2qlhu6","archived":true,"author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"edited":false,"score":0,"quarantine":false,"stickied":false,"from_id":null,"created":1419745715,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"https://www.linkedin.com/pulse/machine-learning-next-wave-innovation-eric-miley-mba?trk=prof-post"}
{"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419780483","secure_media_embed":{},"title":"WEKA question on Naive Bayes classifiers","distinguished":null,"thumbnail":"default","downs":0,"permalink":"/r/MachineLearning/comments/2qmdoj/weka_question_on_naive_bayes_classifiers/","link_flair_text":null,"retrieved_on":1440997313,"ups":1,"media":null,"selftext":"x","id":"2qmdoj","archived":true,"gilded":0,"name":"t3_2qmdoj","link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":6,"author_flair_css_class":null,"is_self":true,"author":"[deleted]","from":null,"stickied":false,"created":1419780483,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/2qmdoj/weka_question_on_naive_bayes_classifiers/","secure_media":null,"from_id":null,"quarantine":false,"edited":1421765207,"score":1}
{"link_flair_css_class":null,"name":"t3_2qmh6a","gilded":0,"archived":true,"id":"2qmh6a","num_comments":8,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"soulslicer0","stickied":false,"from_id":null,"secure_media":null,"url":"http://sebastianraschka.com/Articles/2014_matrix_cheatsheet.html","created":1419783162,"subreddit_id":"t5_2r3gv","from_kind":null,"edited":false,"score":124,"quarantine":false,"domain":"sebastianraschka.com","hide_score":false,"created_utc":"1419783162","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"The cheat sheet for MATLAB, Python NumPy, R, and Julia","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/y4LcOnrYPcv3WiCwUasOFVZFMKIjGxXTDnDElT4FbbY.jpg","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qmh6a/the_cheat_sheet_for_matlab_python_numpy_r_and/","ups":124,"retrieved_on":1440997268,"selftext":"","media":null}
{"hide_score":false,"domain":"linkedin.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419792645","secure_media_embed":{},"title":"\"On Machine Learning and the Next Wave of Innovation\" - Eric Miley, Carnegie Mellon University","distinguished":null,"thumbnail":"default","downs":0,"permalink":"/r/MachineLearning/comments/2qmvt6/on_machine_learning_and_the_next_wave_of/","link_flair_text":null,"retrieved_on":1440997078,"ups":1,"media":null,"selftext":"","archived":true,"id":"2qmvt6","gilded":0,"name":"t3_2qmvt6","link_flair_css_class":null,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"is_self":false,"author":"ericmileymba","from":null,"stickied":false,"from_kind":null,"created":1419792645,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"https://www.linkedin.com/pulse/machine-learning-next-wave-innovation-eric-miley-mba?trk=prof-post","from_id":null,"quarantine":false,"edited":false,"score":1}
{"quarantine":false,"score":4,"edited":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419794173,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qmybu/do_you_need_an_academic_background_for_machine/","from_id":null,"stickied":false,"author":"charlesbukowksi","from":null,"is_self":true,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":27,"archived":true,"id":"2qmybu","gilded":0,"link_flair_css_class":null,"name":"t3_2qmybu","media":null,"selftext":"If you're a self-taught programmer and avid autodidact, is machine learning or a subset of it (like neural networks) something you could develop expertise in, compared to say mobile programming, or is it something that would be best with an academic background and connections?  I'm speaking with a mind to study it and potentially use it entrepreneurially.\n\nWhat kind of math background do you need, or does it depend on the specific field?  I never studied advanced maths like multivariate calculus or linear algebra but I would be willing to learn.","retrieved_on":1440997046,"ups":4,"permalink":"/r/MachineLearning/comments/2qmybu/do_you_need_an_academic_background_for_machine/","link_flair_text":null,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Do you need an academic background for machine learning and data science?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419794173","hide_score":false,"domain":"self.MachineLearning"}
{"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419795477,"from_kind":null,"url":"http://efavdb.com/nba-dash/","secure_media":null,"edited":false,"score":1,"quarantine":false,"name":"t3_2qn0m5","gilded":0,"link_flair_css_class":null,"archived":true,"id":"2qn0m5","author_flair_css_class":null,"num_comments":1,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"efavdb","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qn0m5/machine_learningbased_nba_predictions_visualized/","ups":1,"retrieved_on":1440997016,"selftext":"","media":null,"hide_score":false,"domain":"efavdb.com","author_flair_text":null,"created_utc":"1419795477","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"Machine learning-based NBA predictions visualized","thumbnail":"default","downs":0,"distinguished":null}
{"archived":true,"id":"2qnla2","name":"t3_2qnla2","link_flair_css_class":null,"gilded":0,"over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"is_self":false,"author":"onewugtwowugs","from":null,"stickied":false,"from_kind":null,"created":1419807186,"subreddit_id":"t5_2r3gv","url":"https://medium.com/@hannawallach/big-data-machine-learning-and-the-social-sciences-927a8e20460d","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":0,"hide_score":false,"domain":"medium.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419807186","secure_media_embed":{},"title":"Big Data, Machine Learning, and the Social Sciences: Fairness, Accountability, and Transparency","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/Kzab6_Rp4Y4epRvOoy73xHJADGsJNopIJSEnr81HtfE.jpg","downs":0,"permalink":"/r/MachineLearning/comments/2qnla2/big_data_machine_learning_and_the_social_sciences/","link_flair_text":null,"retrieved_on":1440996748,"ups":0,"media":null,"selftext":""}
{"link_flair_css_class":null,"name":"t3_2qo93i","gilded":0,"archived":true,"id":"2qo93i","num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"yanirse","stickied":false,"from_id":null,"secure_media":null,"url":"http://yanirseroussi.com/2014/12/29/stochastic-gradient-boosting-choosing-the-best-number-of-iterations/","created":1419821076,"from_kind":null,"subreddit_id":"t5_2r3gv","edited":false,"score":8,"quarantine":false,"domain":"yanirseroussi.com","hide_score":false,"created_utc":"1419821076","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"Stochastic Gradient Boosting: Choosing the Best Number of Iterations","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/HGKM4cQuQXN96offH5TMT2ww17bLm9QR9AviFVg0M4M.jpg","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qo93i/stochastic_gradient_boosting_choosing_the_best/","ups":8,"retrieved_on":1440996439,"selftext":"","media":null}
{"edited":false,"score":1,"quarantine":false,"stickied":false,"from_id":null,"url":"https://www.youtube.com/watch?v=XMis0euHdyM","secure_media":{"oembed":{"title":"15 Biggest Unsolved Mysteries in the World","width":600,"thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FXMis0euHdyM%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","thumbnail_width":480,"description":"15 Biggest Unsolved Mysteries in the World","provider_name":"YouTube","provider_url":"http://www.youtube.com/","author_name":"Amazing world","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FXMis0euHdyM%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXMis0euHdyM&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FXMis0euHdyM%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=XMis0euHdyM","type":"video","author_url":"http://www.youtube.com/channel/UCKmNZ2N4MsHX2bpmPgZJfvw","thumbnail_height":360},"type":"youtube.com"},"subreddit_id":"t5_2r3gv","created":1419827573,"from_kind":null,"is_self":false,"from":null,"author":"JosephCox123","gilded":0,"name":"t3_2qojmx","link_flair_css_class":null,"archived":true,"id":"2qojmx","author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"ups":1,"retrieved_on":1440996303,"selftext":"","media":{"oembed":{"author_url":"http://www.youtube.com/channel/UCKmNZ2N4MsHX2bpmPgZJfvw","type":"video","url":"http://www.youtube.com/watch?v=XMis0euHdyM","thumbnail_height":360,"height":338,"author_name":"Amazing world","version":"1.0","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FXMis0euHdyM%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXMis0euHdyM&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FXMis0euHdyM%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","provider_url":"http://www.youtube.com/","provider_name":"YouTube","width":600,"thumbnail_url":"http://i.ytimg.com/vi/XMis0euHdyM/hqdefault.jpg","title":"15 Biggest Unsolved Mysteries in the World","description":"15 Biggest Unsolved Mysteries in the World","thumbnail_width":480},"type":"youtube.com"},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qojmx/15_biggest_unsolved_mysteries_in_the_world/","title":"15 Biggest Unsolved Mysteries in the World","secure_media_embed":{"scrolling":false,"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FXMis0euHdyM%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXMis0euHdyM&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FXMis0euHdyM%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"downs":0,"thumbnail":"default","distinguished":null,"domain":"youtube.com","hide_score":false,"created_utc":"1419827573","author_flair_text":null,"media_embed":{"width":600,"height":338,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FXMis0euHdyM%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DXMis0euHdyM&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FXMis0euHdyM%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false},"subreddit":"MachineLearning"}
{"score":1,"edited":false,"quarantine":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qowgz/packaging_machine/","secure_media":null,"created":1419836189,"from_kind":null,"subreddit_id":"t5_2r3gv","stickied":false,"from":null,"author":"ramylemon","is_self":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"gilded":0,"name":"t3_2qowgz","link_flair_css_class":null,"id":"2qowgz","archived":true,"selftext":"","media":null,"ups":1,"retrieved_on":1440996137,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qowgz/packaging_machine/","downs":0,"thumbnail":"default","distinguished":null,"title":"Packaging Machine","secure_media_embed":{},"created_utc":"1419836189","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false}
{"thumbnail":"default","downs":0,"distinguished":null,"secure_media_embed":{},"title":"With wax/foam/water cleaning! Car wash equipment--Self service car wash","author_flair_text":null,"created_utc":"1419841775","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"youtube.com","selftext":"","media":null,"ups":1,"retrieved_on":1440996055,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qp2rh/with_waxfoamwater_cleaning_car_wash_equipmentself/","from":null,"author":"zzaixcarwash","is_self":false,"num_comments":0,"author_flair_css_class":null,"over_18":false,"saved":false,"link_flair_css_class":null,"name":"t3_2qp2rh","gilded":0,"id":"2qp2rh","archived":true,"score":1,"edited":false,"quarantine":false,"from_id":null,"created":1419841775,"from_kind":null,"subreddit_id":"t5_2r3gv","url":"http://www.youtube.com/attribution_link?a=-y6kzfmV77E&amp;u=%2Fwatch%3Fv%3D7zCVzKbS-Ew%26feature%3Dshare","secure_media":null,"stickied":false}
{"distinguished":null,"downs":0,"thumbnail":"default","title":"Slitting Machine","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419849084","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"","retrieved_on":1440995967,"ups":0,"permalink":"/r/MachineLearning/comments/2qp9l4/slitting_machine/","link_flair_text":null,"author":"ramylemon","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":0,"author_flair_css_class":null,"archived":true,"id":"2qp9l4","link_flair_css_class":null,"name":"t3_2qp9l4","gilded":0,"quarantine":false,"score":0,"edited":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2qp9l4/slitting_machine/","secure_media":null,"from_kind":null,"created":1419849084,"subreddit_id":"t5_2r3gv","from_id":null,"stickied":false}
{"hide_score":false,"domain":"github.com","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419850293","secure_media_embed":{},"title":"Awesome CS courses","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/SIQ6OmVwiJ6dCGjfbYW00LpOkwAkKK_OlV15TzryAZY.jpg","downs":0,"permalink":"/r/MachineLearning/comments/2qpaph/awesome_cs_courses/","link_flair_text":null,"retrieved_on":1440995952,"ups":14,"media":null,"selftext":"","archived":true,"id":"2qpaph","gilded":0,"link_flair_css_class":null,"name":"t3_2qpaph","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"is_self":false,"author":"psamarj","from":null,"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419850293,"url":"https://github.com/prakhar1989/awesome-courses","secure_media":null,"from_id":null,"quarantine":false,"edited":false,"score":14}
{"author_flair_text":null,"created_utc":"1419850469","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"Latent Dirichlet allocation - how am I supposed to pronounce it?","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qpaue/latent_dirichlet_allocation_how_am_i_supposed_to/","selftext":"Dear all,\n\nI am currently busy with Topic Modeling and thus use the term Latent Dirichlet allocation (LDA) quite often. Not being a native English speaker, I am not sure how to pronounce the \"Dirichlet\" part. Could a native English speaker use [Vocaroo](http://vocaroo.com/) (or other) to show it to me? \n\nI am aware the name actually comes from [Mr Dirichlet](http://en.wikipedia.org/wiki/Peter_Gustav_Lejeune_Dirichlet), a German scientist, and that the name should be pronounced as such -- but I'd like to have a US/UK version of the pronunciation too.\n\nThanks!\n","media":null,"ups":0,"retrieved_on":1440995950,"author_flair_css_class":null,"num_comments":7,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2qpaue","id":"2qpaue","archived":true,"from":null,"author":"fawkesdotbe","is_self":true,"from_id":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419850469,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qpaue/latent_dirichlet_allocation_how_am_i_supposed_to/","stickied":false,"score":0,"edited":false,"quarantine":false}
{"secure_media_embed":{},"title":"Why is the natural gradient not used more in machine learning?","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419855445","retrieved_on":1440995893,"ups":38,"media":null,"selftext":"I am reading the Paisley et. al. [Stochastic Variational Inference](http://arxiv.org/abs/1206.7051) paper and saw the concept of a natural gradient for the first time. For the uninitiated, it does gradient descent using a more intrinsic  distance for probability distributions (the symmetrized KL divergence). It makes sense intuitively that this would speed up stochastic optimization, and results confirm it. So I guess my question is, why is this not more popular for doing things like ordinary least-squares regression with SGD? What are the drawbacks to using it? Is the primary impediment the calculation of the Fisher info matrix?","permalink":"/r/MachineLearning/comments/2qpf9x/why_is_the_natural_gradient_not_used_more_in/","link_flair_text":null,"is_self":true,"author":"remington_steele","from":null,"id":"2qpf9x","archived":true,"gilded":0,"name":"t3_2qpf9x","link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":10,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":38,"stickied":false,"from_kind":null,"created":1419855445,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qpf9x/why_is_the_natural_gradient_not_used_more_in/","from_id":null}
{"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"archived":true,"id":"2qpfux","name":"t3_2qpfux","link_flair_css_class":null,"gilded":0,"author":"vrld","from":null,"is_self":false,"url":"https://m.youtube.com/watch?v=cqRcOtD7SgY","secure_media":{"type":"m.youtube.com","oembed":{"author_name":"CCCen","height":338,"html":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcqRcOtD7SgY%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcqRcOtD7SgY&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcqRcOtD7SgY%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","url":"http://www.youtube.com/watch?v=cqRcOtD7SgY","type":"video","author_url":"http://www.youtube.com/user/CCCen","thumbnail_height":360,"title":"From Computation to Consciousness [31c3]","thumbnail_url":"https://i.embed.ly/1/image?url=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcqRcOtD7SgY%2Fhqdefault.jpg&amp;key=b1e305db91cf4aa5a86b732cc9fffceb","width":600,"thumbnail_width":480,"description":"From Computation to Consciousness How computation helps to explain mind, universe and everything How can the physical universe give rise to a mind? I suggest to replace this confusing question by another one: what kind of information processing system is the mind, and how is the mind computed?","provider_url":"http://www.youtube.com/","provider_name":"YouTube"}},"created":1419856130,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"quarantine":false,"score":3,"edited":false,"media_embed":{"scrolling":false,"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FcqRcOtD7SgY%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcqRcOtD7SgY&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcqRcOtD7SgY%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"subreddit":"MachineLearning","created_utc":"1419856130","author_flair_text":null,"domain":"m.youtube.com","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/f3teliMKIzXNoy5hcRShQ0rDVXPjnbEEo0WWEbvPMNk.jpg","title":"From Computation to Consciousness [31c3]","secure_media_embed":{"scrolling":false,"height":338,"width":600,"content":"&lt;iframe class=\"embedly-embed\" src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcqRcOtD7SgY%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcqRcOtD7SgY&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcqRcOtD7SgY%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"permalink":"/r/MachineLearning/comments/2qpfux/from_computation_to_consciousness_31c3/","link_flair_text":null,"media":{"oembed":{"provider_name":"YouTube","provider_url":"http://www.youtube.com/","thumbnail_width":480,"description":"From Computation to Consciousness How computation helps to explain mind, universe and everything How can the physical universe give rise to a mind? I suggest to replace this confusing question by another one: what kind of information processing system is the mind, and how is the mind computed?","title":"From Computation to Consciousness [31c3]","thumbnail_url":"http://i.ytimg.com/vi/cqRcOtD7SgY/hqdefault.jpg","width":600,"thumbnail_height":360,"type":"video","url":"http://www.youtube.com/watch?v=cqRcOtD7SgY","author_url":"http://www.youtube.com/user/CCCen","html":"&lt;iframe class=\"embedly-embed\" src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.youtube.com%2Fembed%2FcqRcOtD7SgY%3Ffeature%3Doembed&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcqRcOtD7SgY&amp;image=http%3A%2F%2Fi.ytimg.com%2Fvi%2FcqRcOtD7SgY%2Fhqdefault.jpg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=text%2Fhtml&amp;schema=youtube\" width=\"600\" height=\"338\" scrolling=\"no\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","version":"1.0","author_name":"CCCen","height":338},"type":"m.youtube.com"},"selftext":"","retrieved_on":1440995885,"ups":3}
{"secure_media_embed":{},"title":"Ask an expert: how soon until human-level A.I.","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419856449","subreddit":"MachineLearning","media_embed":{},"ups":0,"retrieved_on":1440995881,"selftext":"I know this is an impossible question, but with a group of experts, maybe the mean guess will actually be close to the truth.\n\nFirst question: what are some important aspects of human cognition that we cannot replicate to some degree. For example, I would say the following can be replicated: sensory processing (deep learning), \nmemory (comptuers are very good at memory), searching for information, ect, and you could also say that computers are good at stuff like planning, since a computer can combinatorically plan.  So what specific \"tasks\" can you name that a human brain can do, but we have no idea how to do it in a computer?  I would say \"understanding\" or even feeling or emotion are things that would be at the top of my list.\n\nSecond question: How many years do you think it will be until we we have a computer system that can \nperform all the functions of a human brain?  I would guess 10 years.","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qpg6h/ask_an_expert_how_soon_until_humanlevel_ai/","is_self":true,"from":null,"author":"maccl","name":"t3_2qpg6h","link_flair_css_class":null,"gilded":0,"archived":true,"id":"2qpg6h","num_comments":11,"author_flair_css_class":null,"over_18":false,"saved":false,"edited":1419857636,"score":0,"quarantine":false,"stickied":false,"from_id":null,"from_kind":null,"created":1419856449,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qpg6h/ask_an_expert_how_soon_until_humanlevel_ai/"}
{"id":"2qpljv","archived":true,"gilded":0,"name":"t3_2qpljv","link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":1,"is_self":true,"author":"0-n1","from":null,"stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2qpljv/anyone_familiar_with_the_minimum_description/","secure_media":null,"from_kind":null,"created":1419861803,"subreddit_id":"t5_2r3gv","from_id":null,"quarantine":false,"score":7,"edited":false,"domain":"self.MachineLearning","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419861803","author_flair_text":null,"title":"Anyone familiar with the minimum description length (MDL) principle? Which codes to choose for which data structures?","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"self","permalink":"/r/MachineLearning/comments/2qpljv/anyone_familiar_with_the_minimum_description/","link_flair_text":null,"retrieved_on":1440995812,"ups":7,"media":null,"selftext":"I've recently found myself quite fascinated with the minimum description length principle and I'm trying to apply it to my own area of research, which is statistical natural language modeling. It seems to me that while one is, in principle, free to choose arbitrary encoding schemes for model and data, the codes can/do introduce bias towards certain structures. Of course, the relevant structures should be reflected by the encoding scheme -- but I am not fully clear on how one should describe each data structure. I am aware that this can drastically depend on the task at hand, but I am wondering whether any of you have any experience with this and would be willing to discuss."}
{"created_utc":"1419875760","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"self.MachineLearning","hide_score":false,"downs":0,"thumbnail":"self","distinguished":null,"title":"Applied physics/physics to ML?","secure_media_embed":{},"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qq8lh/applied_physicsphysics_to_ml/","selftext":"I'm a rising junior studying applied physics/engineering physics with a minor in computer science. I'm interested in potentially going into ML. I have a pretty strong math background from all the physics, and have experience in software development as well (worked at a software startup this last summer). I'm currently in the process of finding an ML professor to work with at my university. I wanted to know if anyone has experience made the jump from physics to machine learning? Based on my CS minor, I have and will be taking courses in algorithms, AI, etc. The applied physics degree is a combination of mostly physics courses, some core EE/Mech Eng courses, and the remaining electives for the degree are very flexible.\n\nFor the electives, I'd figure that I would focus on computational stuff- modelling and simulation. Would this provide me with a solid skillset for pursuing ML in industry or graduate school? ","media":null,"ups":1,"retrieved_on":1440995513,"num_comments":4,"author_flair_css_class":null,"saved":false,"over_18":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2qq8lh","archived":true,"id":"2qq8lh","from":null,"author":"e808","is_self":true,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qq8lh/applied_physicsphysics_to_ml/","secure_media":null,"subreddit_id":"t5_2r3gv","created":1419875760,"from_kind":null,"stickied":false,"score":1,"edited":false,"quarantine":false}
{"from":null,"author":"yudlejoza","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"name":"t3_2qqb60","gilded":0,"link_flair_css_class":null,"id":"2qqb60","archived":true,"score":0,"edited":false,"quarantine":false,"from_id":null,"url":"http://www.forbes.com/sites/anthonykosner/2014/12/29/tech-2015-deep-learning-and-machine-intelligence-will-eat-the-world/","secure_media":null,"from_kind":null,"created":1419877106,"subreddit_id":"t5_2r3gv","stickied":false,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/2zRV-meNitDyIwS-3hTiGpee0W6lmuhGxvwmzk89PuA.jpg","distinguished":null,"title":"Tech 2015: Deep Learning And Machine Intelligence Will Eat The World","secure_media_embed":{},"created_utc":"1419877106","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"forbes.com","hide_score":false,"selftext":"","media":null,"ups":0,"retrieved_on":1440995480,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qqb60/tech_2015_deep_learning_and_machine_intelligence/"}
{"is_self":false,"author":"digitron","from":null,"archived":true,"id":"2qqjur","name":"t3_2qqjur","gilded":0,"link_flair_css_class":null,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":5,"quarantine":false,"score":20,"edited":false,"stickied":false,"url":"http://www.wired.com/2014/12/machine-intelligence-cracks-genetic-controls/","secure_media":null,"created":1419881461,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"title":"Machine Intelligence Cracks Genetic Controls","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/vdO6uOCgnEvJpN6R69Z29SAb7PMB87yukw9l8nrz8x4.jpg","domain":"wired.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419881461","author_flair_text":null,"retrieved_on":1440995367,"ups":20,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2qqjur/machine_intelligence_cracks_genetic_controls/","link_flair_text":null}
{"author":"magwo","from":null,"is_self":true,"over_18":false,"saved":false,"num_comments":25,"author_flair_css_class":null,"archived":true,"id":"2qqnq3","link_flair_css_class":null,"name":"t3_2qqnq3","gilded":0,"quarantine":false,"edited":false,"score":18,"from_kind":null,"created":1419883328,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qqnq3/why_are_humans_designing_neural_networks/","from_id":null,"stickied":false,"distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Why are humans designing neural networks?","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419883328","hide_score":false,"domain":"self.MachineLearning","media":null,"selftext":"It seems to me that a significant bottleneck in the development of ML and NNs is the idea that we as humans should be designing the neural networks - that is - to select the number of hidden layers, the number of neurons in those hidden layers, and the degree of connectivity between them.\n\nWhy are we not letting computers - algorithms or neural networks design the neural networks?\n\nThe human brain did not have its features designed, and the ability of the brain to adapt and relocate functionality suggests that it has the ability to layout/allocate/design new networks.\n\nOne apparent thing that hints at the adaptability of the human NNs is its ability to work with alphabets of varying lengths. It does not appear to be pre-programmed for an alphabet with 20 symbols or whatever - it can work with large alphabets.\n\nDoes state-of-the-art of NN research still involve humans designing networks?\n\nForgive me if this is a common or irrelevant question. I did a search but did not find anything apparent discussing this subject.","retrieved_on":1440995317,"ups":18,"permalink":"/r/MachineLearning/comments/2qqnq3/why_are_humans_designing_neural_networks/","link_flair_text":null}
{"secure_media_embed":{},"title":"Diagnosing problems with autoencoders","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419891565","retrieved_on":1440995103,"ups":2,"media":null,"selftext":"So I have tried a few autoencoders:\n\nbasic autoencoder, contractive autoencoder and denoising autoencoder\n\nthey all work nice when I train them on MNIST dataset. I get nice filters. The problem is, when I feed these autoencoders my images, of the same size as the mnist images, the autoencoders seem to learn one filter only. It looks like superposition of all images.\n\nBasically, it looks like it mashed together every of my images and shows the result as each filter. Each single one of these filters is almost identical.\n\nI've tried different configurations, changing hidden unit sizes, training epochs - by a few different orders of magnitute but the result is always the same.\n\nCan anyone tell me why would this happen?\n\nCan it have something to do with the size of my data? For comparison, MNIST has 50000 examples while my dataset has 500 - could this be the reason?\n\nI would love feedback from someone experienced with autoencoders.","permalink":"/r/MachineLearning/comments/2qr47h/diagnosing_problems_with_autoencoders/","link_flair_text":null,"is_self":true,"author":"paralax77","from":null,"archived":true,"id":"2qr47h","gilded":0,"link_flair_css_class":null,"name":"t3_2qr47h","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":2,"quarantine":false,"edited":false,"score":2,"stickied":false,"subreddit_id":"t5_2r3gv","created":1419891565,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qr47h/diagnosing_problems_with_autoencoders/","from_id":null}
{"edited":false,"score":2,"quarantine":false,"stickied":false,"from_id":null,"subreddit_id":"t5_2r3gv","created":1419893789,"from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qr8m1/exponentialfamily_harmoniums_vs_standard_rbms/","is_self":true,"from":null,"author":"M_Bus","gilded":0,"name":"t3_2qr8m1","link_flair_css_class":null,"archived":true,"id":"2qr8m1","num_comments":4,"author_flair_css_class":null,"over_18":false,"saved":false,"ups":2,"retrieved_on":1440995046,"selftext":"I'm experimenting with Restricted Boltzmann Machines to do some unsupervised learning so I can have an actual generative model of some data I'm working with. Also useful for pre-training of deep belief nets or things like that.\n\nAnyway, a lot of the data I'm working with is real-valued data. Sometimes it's integers between, say, 1 and 20, and sometimes it's real values between 0 and 1,000,000 that are approximately exponentially distributed. I say \"real values,\" but really it's rounded to the nearest 100th. It's distributed according to a real distribution, for all intents and purposes, but it doesn't require a great deal of precision in terms of decimal points.\n\nSo I was toying with the idea of setting up a so-called \"Exponential-Family Harmonium,\" but I would need to use some kind of weird Energy formulation with mixed data types, since some of my input vector coordinates are binary values, some are binomially distributed, and some are maybe exponential or Gaussian. But it occurred to me that I could convert all of my real-valued entries into binary digits and use a standard RBM.\n\nMy question: would that actually work? That is, would converting all non-binary numbers into binary (and then just using a normal Restricted Boltzmann Machine trained with Contrastive Divergence) work? What kind of internal representation of the data would my algorithm come up with? Would it actually make sense when I run the program backwards to generate new examples of data?\n\nHas anyone tried this? If so, how did it work out?","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qr8m1/exponentialfamily_harmoniums_vs_standard_rbms/","secure_media_embed":{},"title":"Exponential-Family Harmoniums vs standard RBMs [Question]","thumbnail":"self","downs":0,"distinguished":null,"hide_score":false,"domain":"self.MachineLearning","author_flair_text":null,"created_utc":"1419893789","subreddit":"MachineLearning","media_embed":{}}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qrbf8/what_algorithms_should_i_be_looking_to_learn/","ups":7,"retrieved_on":1440995010,"selftext":"I have a math major completed, some graduate statistics work under my belt, and have done a decent amount of computer science coursework, but I've forgotten a good bit of it.  Most of that work was in Java.  I'm not an amazing programmer by any means, even though I'm comfortable doing statistical analysis with generalized linear models.\n\nWhat algorithms and data structures should I get really comfortable with if I want to get more comfortable with implementing machine learning algorithms (and I guess general \"data science\")?  I went through some of Andrew Ng's course but found it was more technique-oriented than programming-oriented.  I figure some kind of tree would make a lot of sense (especially for decision trees and random forests), but I don't know what algorithms tend to be used most commonly.  I also get the impression that Bloom filters are used a lot but can't find much information on implementations.\n\nAdditionally, should I try implementing them in a lower-level language?  I'm not a fan of Java, but it seems like a lot of newer good linear algebra libraries are written in C++.  I don't know C++, but would it be worth learning if I wanted to get involved in machine learning?  Also is it worth going through SICP if my eventual goal is to get comfortable enough with programming for machine learning?\n\n**EDIT:  To be clear, I already am comfortable with Java, Matlab, Python, and R.  I'm comfortable with data analysis because of my statistics background.  Here, I'm asking specifically about implementing machine learning algorithms down to the data structures they will be used on.  Not about how to use and interpret them.**","media":null,"domain":"self.MachineLearning","hide_score":false,"created_utc":"1419895225","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"What algorithms should I be looking to learn?","secure_media_embed":{},"downs":0,"thumbnail":"self","distinguished":null,"stickied":false,"from_id":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qrbf8/what_algorithms_should_i_be_looking_to_learn/","secure_media":null,"from_kind":null,"created":1419895225,"subreddit_id":"t5_2r3gv","score":7,"edited":1419953824,"quarantine":false,"gilded":0,"name":"t3_2qrbf8","link_flair_css_class":null,"archived":true,"id":"2qrbf8","author_flair_css_class":null,"num_comments":20,"saved":false,"over_18":false,"is_self":true,"from":null,"author":"beaverteeth92"}
{"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419899536","secure_media_embed":{},"title":"Did anyone here use \"no more pesky learning rates\" algorithm? need some help","distinguished":null,"thumbnail":"self","downs":0,"permalink":"/r/MachineLearning/comments/2qrje1/did_anyone_here_use_no_more_pesky_learning_rates/","link_flair_text":null,"retrieved_on":1440994906,"ups":2,"media":null,"selftext":"I just started to learn neural nets, completed Andrew Ng course and in the middle of Geoffrey Hinton 2012 year course about neural networks on Coursera. He mentions this paper [no more pesky learning rates](http://arxiv.org/pdf/1206.1106.pdf) It's almost 3 years old now. I found only one post about it in this subreddit and it's not much there. There are some implementations on the Internet, ok. But what is hard is to find code that computes Hessian of the network. I couldn't find clear implementation of bbprop mentioned in this paper.. I found this paper [Estimating the Hessian by Back-propagating Curvature](http://arxiv.org/pdf/1206.6464v2.pdf) but I couldn't implement it so far. Should I continue trying or it's not worth it? Can someone share a link or code for bbprop or some other algorithm that can efficiently compute Hessian diagonal?\n\nEdit: At least maybe someone knows what is this V(v_i) variable is? And all those indices, after two days now I almost understand what is happening(not how it works) but I still can't understand what is V stands for. I even made it just random valued vector, but I guess it's wrong. [Here's](http://techtalks.tv/talks/estimating-the-hessian-by-back-propagating-curvature/57305/) a short video about the second paper.","id":"2qrje1","archived":true,"name":"t3_2qrje1","gilded":0,"link_flair_css_class":null,"over_18":false,"saved":false,"num_comments":9,"author_flair_css_class":null,"is_self":true,"author":"lo1201","from":null,"stickied":false,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419899536,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qrje1/did_anyone_here_use_no_more_pesky_learning_rates/","from_id":null,"quarantine":false,"edited":1419909117,"score":2}
{"gilded":0,"name":"t3_2qrver","link_flair_css_class":null,"id":"2qrver","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"rantana","stickied":false,"from_id":null,"url":"http://arxiv.org/abs/1412.7259","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419906297,"score":1,"edited":false,"quarantine":false,"domain":"arxiv.org","hide_score":false,"created_utc":"1419906297","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"C-SVDDNet: An Effective Single-Layer Network for Unsupervised Feature Learning","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qrver/csvddnet_an_effective_singlelayer_network_for/","ups":1,"retrieved_on":1440994751,"selftext":"","media":null}
{"over_18":false,"saved":false,"num_comments":5,"author_flair_css_class":null,"archived":true,"id":"2qs8xk","link_flair_css_class":null,"name":"t3_2qs8xk","gilded":0,"author":"rasputin48","from":null,"is_self":true,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419914367,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qs8xk/deepmind_type_results_against_humans/","from_id":null,"stickied":false,"quarantine":false,"score":1,"edited":false,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419914367","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"Deepmind type results against humans?","permalink":"/r/MachineLearning/comments/2qs8xk/deepmind_type_results_against_humans/","link_flair_text":null,"media":null,"selftext":"Deepmind famously got a computer to master atari games using only visual input.  What is the state of the art for a similar setup against humans.  Can a computer learn to beat a human at pong using visual input and the difference of scores as the objective function?","retrieved_on":1440994575,"ups":1}
{"domain":"github.com","hide_score":false,"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419916528","author_flair_text":null,"title":"LambdaNet - A functional neural network library written in Haskell","secure_media_embed":{},"distinguished":null,"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/Iw5Q-TE3Jtv9MgDTonXipKiR6t6X4hvRWdGkz_pEKow.jpg","permalink":"/r/MachineLearning/comments/2qscld/lambdanet_a_functional_neural_network_library/","link_flair_text":null,"retrieved_on":1440994528,"ups":7,"media":null,"selftext":"","id":"2qscld","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2qscld","saved":false,"over_18":false,"num_comments":1,"author_flair_css_class":null,"is_self":false,"author":"diamondium","from":null,"stickied":false,"url":"https://github.com/jbarrow/LambdaNet","secure_media":null,"created":1419916528,"from_kind":null,"subreddit_id":"t5_2r3gv","from_id":null,"quarantine":false,"score":7,"edited":false}
{"media_embed":{},"subreddit":"MachineLearning","created_utc":"1419918373","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"Sparse coding resources","secure_media_embed":{},"permalink":"/r/MachineLearning/comments/2qsfmk/sparse_coding_resources/","link_flair_text":null,"media":null,"selftext":"Are there any good resources on understanding Sparse coding. Also how come there are so few people using sparse coding? is it not as good as RBMs or autoencoders?\n\nRight now I am using videos from Hugo Larochelle\nhttps://www.youtube.com/watch?v=7a0_iEruGoM\n\nBut I want other sources so I understand the process better.","retrieved_on":1440994489,"ups":2,"saved":false,"over_18":false,"author_flair_css_class":null,"num_comments":3,"archived":true,"id":"2qsfmk","name":"t3_2qsfmk","link_flair_css_class":null,"gilded":0,"author":"chchan","from":null,"is_self":true,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qsfmk/sparse_coding_resources/","from_kind":null,"subreddit_id":"t5_2r3gv","created":1419918373,"from_id":null,"stickied":false,"quarantine":false,"score":2,"edited":false}
{"selftext":"","media":null,"ups":1,"retrieved_on":1440994454,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qsiae/attention_pleasecar_wash_equipmentmobile_car_wash/","downs":0,"thumbnail":"default","distinguished":null,"title":"Attention please!!!Car wash equipment---mobile car wash","secure_media_embed":{},"created_utc":"1419920092","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"youtube.com","hide_score":false,"edited":false,"score":1,"quarantine":false,"from_id":null,"secure_media":null,"url":"http://www.youtube.com/attribution_link?a=EdzcBym4bow&amp;u=%2Fwatch%3Fv%3DcHa0DVYBHMU%26feature%3Dshare","from_kind":null,"subreddit_id":"t5_2r3gv","created":1419920092,"stickied":false,"from":null,"author":"zzaixcarwash","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2qsiae","archived":true,"id":"2qsiae"}
{"edited":false,"score":27,"quarantine":false,"from_id":null,"created":1419920827,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qsje7/how_do_you_initialize_your_neural_network_weights/","stickied":false,"from":null,"author":"rantana","is_self":true,"author_flair_css_class":null,"num_comments":32,"over_18":false,"saved":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2qsje7","archived":true,"id":"2qsje7","selftext":"Has anyone found any success beyond initializing weights randomly from an alpha*N(0,1) distribution?","media":null,"ups":27,"retrieved_on":1440994440,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qsje7/how_do_you_initialize_your_neural_network_weights/","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"How do you initialize your neural network weights?","author_flair_text":null,"created_utc":"1419920827","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning"}
{"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qszgg/how_feasible_is_freelancing_in_machine_learning/","selftext":"I'm not sure if this is an appropriate question for this subreddit - if it isn't, my apologies - but I was wondering if anyone here has any experience doing freelance machine learning work (especially remotely) and could tell me a little bit about potential pitfalls and so on. \n\nSome background: I'm a mathematician by training - I got my PhD a few years ago - and after a couple of postdocs I'm getting a little fed up with the academic career.\n\nI'm currently studying to get another Master's degree, with elements of machine learning, neural networks and computational neuroscience: perhaps it's a little unnecessary, but I like to learn and could afford it and so I decided to treat myself a little. Furthermore, I'm supplementing it by MOOCs and individual study (I'm currently going through Murphy's book). My personal - and, admittedly, possibly quite flawed - impression is that, if I maintain the current pace, at the end of the next semester I should have a pretty solid (if still imperfect, obviously) understanding of the most common elements of machine learning.\n\nLately, I've been also thinking that the freedom and variety of freelance work might suit me better than working for a single business for the foreseeable future; and that, ideally, I would prefer to be able to work remotely. \n\nIs this reasonably feasible, as a longish-term objective, or am I talking nonsense? Also, am I correct in thinking that if I want to do that, I might want to *first* aim for temporary positions, and attempt to transition to full-time, freelance remote work *after* I have some experience and reputation? \n\nThanks a lot!","media":null,"ups":23,"retrieved_on":1440994232,"author_flair_text":null,"created_utc":"1419935289","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"How feasible is freelancing in machine learning?","from_id":null,"created":1419935289,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qszgg/how_feasible_is_freelancing_in_machine_learning/","stickied":false,"score":23,"edited":false,"quarantine":false,"author_flair_css_class":null,"num_comments":20,"over_18":false,"saved":false,"gilded":0,"name":"t3_2qszgg","link_flair_css_class":null,"archived":true,"id":"2qszgg","from":null,"author":"[deleted]","is_self":true}
{"ups":7,"retrieved_on":1440994165,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qt4lx/breakthroughs_in_artificial_intelligence_from/","title":"Breakthroughs in Artificial Intelligence from 2014 | MIT Technology Review","secure_media_embed":{},"downs":0,"thumbnail":"http://b.thumbs.redditmedia.com/ZSIBKU8NSfRiWeDUCjoIqVpZCT2CTnSnz3YXp23pDmI.jpg","distinguished":null,"domain":"technologyreview.com","hide_score":false,"created_utc":"1419940727","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","score":7,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"url":"http://www.technologyreview.com/news/533686/2014-in-computing-breakthroughs-in-artificial-intelligence/","secure_media":null,"created":1419940727,"from_kind":null,"subreddit_id":"t5_2r3gv","is_self":false,"from":null,"author":"mttd","name":"t3_2qt4lx","gilded":0,"link_flair_css_class":null,"id":"2qt4lx","archived":true,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false}
{"domain":"nuit-blanche.blogspot.fr","hide_score":false,"created_utc":"1419944221","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","title":"A Short Review of 2014 on Nuit Blanche: On Compressive Sensing, Machine Learning, Advanced Matrix Factorization and many beautiful things.","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qt7zw/a_short_review_of_2014_on_nuit_blanche_on/","ups":1,"retrieved_on":1440994121,"selftext":"","media":null,"gilded":0,"link_flair_css_class":null,"name":"t3_2qt7zw","archived":true,"id":"2qt7zw","author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"is_self":false,"from":null,"author":"[deleted]","stickied":false,"from_id":null,"url":"http://nuit-blanche.blogspot.fr/2014/12/a-short-review-of-2014-on-nuit-blanche.html","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419944221,"edited":false,"score":1,"quarantine":false}
{"is_self":false,"author":"compsens","from":null,"id":"2qt81z","archived":true,"gilded":0,"link_flair_css_class":null,"name":"t3_2qt81z","over_18":false,"saved":false,"author_flair_css_class":null,"num_comments":0,"quarantine":false,"edited":false,"score":4,"stickied":false,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419944273,"url":"http://nuit-blanche.blogspot.fr/2014/12/a-short-review-of-2014-on-nuit-blanche.html","secure_media":null,"from_id":null,"secure_media_embed":{},"title":"A Short Review of 2014 on Nuit Blanche: On Compressive Sensing, Machine Learning, Advanced Matrix Factorization and many beautiful things (x-post r/CompressiveSensing)","distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/mZCl8RiKET60LVmNfn5iYFCdF6xaPJcdZakIiWPSUnY.jpg","downs":0,"hide_score":false,"domain":"nuit-blanche.blogspot.fr","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419944273","retrieved_on":1440994120,"ups":4,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2qt81z/a_short_review_of_2014_on_nuit_blanche_on/","link_flair_text":null}
{"stickied":false,"from_id":null,"from_kind":null,"created":1419944410,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://datascience.stackexchange.com/questions/3783/how-to-model-this-un-predicatability-problem","score":0,"edited":false,"quarantine":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2qt86u","id":"2qt86u","archived":true,"num_comments":1,"author_flair_css_class":null,"over_18":false,"saved":false,"is_self":false,"from":null,"author":"sashankdvk","link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qt86u/how_to_model_this_un_predicatability_problem/","ups":0,"retrieved_on":1440994118,"selftext":"","media":null,"hide_score":false,"domain":"datascience.stackexchange.com","author_flair_text":null,"created_utc":"1419944410","subreddit":"MachineLearning","media_embed":{},"secure_media_embed":{},"title":"How to model this \"un predicatability\" problem?","thumbnail":"http://b.thumbs.redditmedia.com/DgkI__UkgxwMY41b_9KM0m0RF-vPkIw6QKnpV7VAYkU.jpg","downs":0,"distinguished":null}
{"quarantine":false,"score":25,"edited":false,"stickied":false,"from_kind":null,"created":1419953457,"subreddit_id":"t5_2r3gv","url":"http://drona.csa.iisc.ernet.in/~shivani/Teaching/E0370/Aug-2013/index.html#lectures","secure_media":null,"from_id":null,"is_self":false,"author":"mmahesh","from":null,"archived":true,"id":"2qtk28","link_flair_css_class":null,"name":"t3_2qtk28","gilded":0,"over_18":false,"saved":false,"num_comments":3,"author_flair_css_class":null,"retrieved_on":1440993964,"ups":25,"media":null,"selftext":"","permalink":"/r/MachineLearning/comments/2qtk28/statistical_learning_theory_lecture_notes/","link_flair_text":null,"secure_media_embed":{},"title":"Statistical Learning Theory Lecture Notes","distinguished":null,"thumbnail":"default","downs":0,"hide_score":false,"domain":"drona.csa.iisc.ernet.in","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419953457"}
{"secure_media_embed":{},"title":"Most important tools to learn","distinguished":null,"thumbnail":"self","downs":0,"hide_score":false,"domain":"self.MachineLearning","subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1419960875","retrieved_on":1440993786,"ups":17,"media":null,"selftext":"For someone who has a background in applied math and Python programming experience (incl. numpy and scipy), how would you rank the following tools for ML in terms of importance:\n\n- Matlab\n- R\n- Theano \n- SKlearn\n- Others?\n","permalink":"/r/MachineLearning/comments/2qtxrq/most_important_tools_to_learn/","link_flair_text":null,"is_self":true,"author":"letoseldon","from":null,"id":"2qtxrq","archived":true,"link_flair_css_class":null,"name":"t3_2qtxrq","gilded":0,"over_18":false,"saved":false,"num_comments":16,"author_flair_css_class":null,"quarantine":false,"edited":false,"score":17,"stickied":false,"created":1419960875,"from_kind":null,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qtxrq/most_important_tools_to_learn/","from_id":null}
{"downs":0,"thumbnail":"default","distinguished":null,"title":"Readers\u2019 Choice \u2013 10 Most Popular Microsoft Machine Learning Blog Posts of 2014","secure_media_embed":{},"created_utc":"1419963678","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","domain":"aka.ms","hide_score":false,"selftext":"","media":null,"ups":0,"retrieved_on":1440993714,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qu3bi/readers_choice_10_most_popular_microsoft_machine/","from":null,"author":"MLBlogTeam","is_self":false,"num_comments":0,"author_flair_css_class":null,"saved":false,"over_18":false,"link_flair_css_class":null,"gilded":0,"name":"t3_2qu3bi","archived":true,"id":"2qu3bi","score":0,"edited":false,"quarantine":false,"from_id":null,"url":"http://aka.ms/cl69t6","secure_media":null,"from_kind":null,"subreddit_id":"t5_2r3gv","created":1419963678,"stickied":false}
{"title":"A minibatch learning (using SGD) 101","secure_media_embed":{},"downs":0,"thumbnail":"http://a.thumbs.redditmedia.com/GB45Hkv-VlgsqKbM5zkHfu564vbljFw2Uv5Byg3UVf0.jpg","distinguished":null,"domain":"adventuresindatascience.wordpress.com","hide_score":false,"created_utc":"1419987963","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":1,"retrieved_on":1440993099,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qveuz/a_minibatch_learning_using_sgd_101/","is_self":false,"from":null,"author":"[deleted]","gilded":0,"link_flair_css_class":null,"name":"t3_2qveuz","id":"2qveuz","archived":true,"num_comments":4,"author_flair_css_class":null,"saved":false,"over_18":false,"score":1,"edited":false,"quarantine":false,"stickied":false,"from_id":null,"url":"https://adventuresindatascience.wordpress.com/2014/12/30/minibatch-learning-for-large-scale-data-using-scikit-learn/","secure_media":null,"subreddit_id":"t5_2r3gv","from_kind":null,"created":1419987963}
{"thumbnail":"self","downs":0,"distinguished":null,"secure_media_embed":{},"title":"bias term in online SGD","author_flair_text":null,"created_utc":"1420012632","subreddit":"MachineLearning","media_embed":{},"hide_score":false,"domain":"self.MachineLearning","selftext":"In this paper https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0CCcQFjAB&amp;url=http%3A%2F%2Fresearch.google.com%2Fpubs%2Fpub41159.html&amp;ei=cqujVKaDA9LV8gWGm4CwBg&amp;usg=AFQjCNHeQ_C152a89Rax1FozSDxnT8W1_w Google present a online SGD algorithm. I try to implement it and find that the bias term is not as usual as the bias term which generated by LBFGS. In my opinio, the bias term should represent the average output of a LR model. However, the bias term LR model trained by the online SGD is very different from that trained by LBFGS. Anyone knows it?","media":null,"ups":0,"retrieved_on":1440992583,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qwilv/bias_term_in_online_sgd/","from":null,"author":"jdxyw","is_self":true,"author_flair_css_class":null,"num_comments":6,"over_18":false,"saved":false,"gilded":0,"link_flair_css_class":null,"name":"t3_2qwilv","id":"2qwilv","archived":true,"edited":false,"score":0,"quarantine":false,"from_id":null,"from_kind":null,"created":1420012632,"subreddit_id":"t5_2r3gv","secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qwilv/bias_term_in_online_sgd/","stickied":false}
{"title":"Why does Deep Learning work? - A perspective from Group Theory","secure_media_embed":{},"downs":0,"thumbnail":"default","distinguished":null,"domain":"arxiv.org","hide_score":false,"created_utc":"1420035753","author_flair_text":null,"media_embed":{},"subreddit":"MachineLearning","ups":1,"retrieved_on":1440992290,"selftext":"","media":null,"link_flair_text":null,"permalink":"/r/MachineLearning/comments/2qx56v/why_does_deep_learning_work_a_perspective_from/","is_self":false,"from":null,"author":"vkhuc","name":"t3_2qx56v","link_flair_css_class":null,"gilded":0,"id":"2qx56v","archived":true,"author_flair_css_class":null,"num_comments":0,"saved":false,"over_18":false,"edited":false,"score":1,"quarantine":false,"stickied":false,"from_id":null,"url":"http://arxiv.org/abs/1412.6621","secure_media":null,"created":1420035753,"subreddit_id":"t5_2r3gv","from_kind":null}
{"author":"ihsgnef","from":null,"is_self":true,"saved":false,"over_18":false,"num_comments":7,"author_flair_css_class":null,"id":"2qxu8b","archived":true,"gilded":0,"name":"t3_2qxu8b","link_flair_css_class":null,"quarantine":false,"score":0,"edited":false,"url":"http://www.reddit.com/r/MachineLearning/comments/2qxu8b/this_might_be_vague_but_what_are_the_most/","secure_media":null,"created":1420050652,"subreddit_id":"t5_2r3gv","from_kind":null,"from_id":null,"stickied":false,"distinguished":null,"downs":0,"thumbnail":"self","title":"This might be vague, but what are the most important questions to ask concerning the development of ML?","secure_media_embed":{},"media_embed":{},"subreddit":"MachineLearning","created_utc":"1420050652","author_flair_text":null,"domain":"self.MachineLearning","hide_score":false,"media":null,"selftext":"I realize that I am lack of vision. Despite of learning new \"tricks\" in machine learning, I want to know what to look at or learn to prepare for the next 10 years of development in machine learning and all kinds of trending computer science disciplines.\n\nBesides questions like \"will deep learning lose its popularity?\" and \"in which field will we witness next breakthrough?\", what are the most important questions to ask? If you are to ask someone that is highly involved in learning theory and has great vision on the development of computer science, what questions will you ask?\n\nThanks, I appreciate any insight.","retrieved_on":1440991966,"ups":0,"permalink":"/r/MachineLearning/comments/2qxu8b/this_might_be_vague_but_what_are_the_most/","link_flair_text":null}
{"permalink":"/r/MachineLearning/comments/2qyn4k/what_algorithm_would_be_suit_this_pairing_problem/","link_flair_text":null,"media":null,"selftext":"This is the scenario I am currently in:\n\nI have data on 16 chess players who have each played against some of the other players and they have rated how enjoyable their game was on a 1-10 scale. I am trying to match players to suggest who they should play against.\n\nEach player has their chess proficiency rating and average move time recorded\n\nThey have stated if they are casual, intermediate, or serious players\n\nIn addition people stated if their opponent's proficiency, move time, and seriousness is of low, medium, or high importance to them.\n\nThus far I have only been exposed to Bayesian learning methods. It seems clear that the data should be fitted to maximize a prediction of how enjoyable the players' past games have been. \n\nWhat confuses me is that I've got some numerical (rating/time) and some qualitative (seriousness) data and I don't know how to bring them together. The importance people put on each parameter is not numerical or binomial so would I have to account for this by assigning the levels of importance some constant which becomes another variable in the prediction optimizing?","retrieved_on":1440991591,"ups":0,"subreddit":"MachineLearning","media_embed":{},"author_flair_text":null,"created_utc":"1420067217","hide_score":false,"domain":"self.MachineLearning","distinguished":null,"thumbnail":"self","downs":0,"secure_media_embed":{},"title":"What algorithm would be suit this pairing problem?","created":1420067217,"subreddit_id":"t5_2r3gv","from_kind":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/2qyn4k/what_algorithm_would_be_suit_this_pairing_problem/","from_id":null,"stickied":false,"quarantine":false,"edited":false,"score":0,"over_18":false,"saved":false,"num_comments":8,"author_flair_css_class":null,"archived":true,"id":"2qyn4k","name":"t3_2qyn4k","gilded":0,"link_flair_css_class":null,"author":"thai_tong","from":null,"is_self":true}
