{"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":12,"ups":12,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","domain":"self.MachineLearning","link_flair_text":null,"selftext":"I'm currently taking an advanced Linear Algebra course on Linear Dynamic Systems, and we're covering things like Least-Squares Approximation, Multi-Objective Least Squares, finding the Least-Norm Solution, the Matrix Exponential, and Autonomous Linear Dynamic Systems. \n\nThis is all very overwhelming to a guy whose only taken basic linear algebra before (Null-space, column-space, Eigenvectors, etc.), and I'm having a tough time figuring out how this relates to machine learning and data mining. This all makes perfect sense for electrical engineering, circuits, population dynamics, but I have no idea how any of this plays a role in ML.\n\nCan someone enlighten me on how the concepts above help make you a better ML person?","author":"t_hrow","downs":0,"url":"http://www.reddit.com/r/MachineLearning/comments/1jj21j/what_does_linear_algebra_have_to_do_with_machine/","report_reasons":null,"is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently taking an advanced Linear Algebra course on Linear Dynamic Systems, and we&amp;#39;re covering things like Least-Squares Approximation, Multi-Objective Least Squares, finding the Least-Norm Solution, the Matrix Exponential, and Autonomous Linear Dynamic Systems. &lt;/p&gt;\n\n&lt;p&gt;This is all very overwhelming to a guy whose only taken basic linear algebra before (Null-space, column-space, Eigenvectors, etc.), and I&amp;#39;m having a tough time figuring out how this relates to machine learning and data mining. This all makes perfect sense for electrical engineering, circuits, population dynamics, but I have no idea how any of this plays a role in ML.&lt;/p&gt;\n\n&lt;p&gt;Can someone enlighten me on how the concepts above help make you a better ML person?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","media_embed":{},"id":"1jj21j","permalink":"/r/MachineLearning/comments/1jj21j/what_does_linear_algebra_have_to_do_with_machine/","over_18":false,"created_utc":1375399453,"author_flair_css_class":null,"gilded":0,"title":"What does Linear Algebra have to do with Machine Learning?","banned_by":null,"user_reports":[],"stickied":false,"num_comments":8,"secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411979799}
{"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":22,"ups":22,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","domain":"en.wikipedia.org","link_flair_text":null,"selftext":"","downs":0,"author":"SCombinator","url":"http://en.wikipedia.org/wiki/Kernel_trick","report_reasons":null,"thumbnail":"default","is_self":false,"selftext_html":null,"media_embed":{},"id":"1jiz0j","permalink":"/r/MachineLearning/comments/1jiz0j/improve_your_machine_learning_with_this_one_weird/","over_18":false,"created_utc":1375397033,"author_flair_css_class":null,"gilded":0,"banned_by":null,"title":"Improve your Machine Learning with this one weird trick.","user_reports":[],"stickied":false,"num_comments":12,"secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411979911}
{"gilded":0,"over_18":false,"created_utc":1375388115,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jin31/constrained_independent_component_analysis_cica/","num_comments":2,"user_reports":[],"stickied":false,"banned_by":null,"title":"Constrained Independent Component Analysis (cICA)","mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null,"retrieved_on":1411980354,"score":5,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":5,"downs":0,"author":"cICA1","selftext":"Does anyone have any experience with constrained independent component analysis?  Have been trying to implement cICA in matlab for a while now and cannot get it to work.  Following exactly the algorithm described in the paper: \"Unique ICA solution by eliminating indeterminancy,\" unable to get results similar to their results.  \n\nDoes anyone have any experience with cICA?  Know of any source code that might be helpful? ","link_flair_text":null,"domain":"self.MachineLearning","media_embed":{},"id":"1jin31","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any experience with constrained independent component analysis?  Have been trying to implement cICA in matlab for a while now and cannot get it to work.  Following exactly the algorithm described in the paper: &amp;quot;Unique ICA solution by eliminating indeterminancy,&amp;quot; unable to get results similar to their results.  &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with cICA?  Know of any source code that might be helpful? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/1jin31/constrained_independent_component_analysis_cica/","report_reasons":null}
{"url":"http://engineering.richrelevance.com/bayesian-analysis-of-normal-distributions-with-python/","report_reasons":null,"media_embed":{},"id":"1jile9","thumbnail":"default","is_self":false,"selftext_html":null,"domain":"engineering.richrelevance.com","downs":0,"author":"sergeyfeldman","selftext":"","link_flair_text":null,"subreddit":"MachineLearning","ups":15,"subreddit_id":"t5_2r3gv","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":15,"retrieved_on":1411980415,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"title":"Bayesian Analysis of Normal Distributions with Python","banned_by":null,"num_comments":0,"user_reports":[],"stickied":false,"over_18":false,"created_utc":1375386948,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jile9/bayesian_analysis_of_normal_distributions_with/","gilded":0}
{"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jigip/simple_question_about_gibbs_sampling_with_a/","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I assume the most simple model:&lt;/p&gt;\n\n&lt;p&gt;theta | alpha ~ Dir(alpha + N)&lt;/p&gt;\n\n&lt;p&gt;X_i | Discrete(theta), i = 1, 2, 3&lt;/p&gt;\n\n&lt;p&gt;What would be the update equation, p(x_i, x_-i)? Is it the predictive distribution of a Dirichlet-Multinomial?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"id":"1jigip","media_embed":{},"domain":"self.MachineLearning","selftext":"I assume the most simple model:\n\ntheta | alpha ~ Dir(alpha + N)\n\nX_i | Discrete(theta), i = 1, 2, 3\n\n\nWhat would be the update equation, p(x\\_i, x_-i)? Is it the predictive distribution of a Dirichlet-Multinomial?","link_flair_text":null,"author":"wordsoup","downs":0,"ups":7,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":7,"retrieved_on":1411980596,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"title":"Simple question about Gibbs Sampling with a single Dirichlet-Multinomial","banned_by":null,"stickied":false,"user_reports":[],"num_comments":2,"permalink":"/r/MachineLearning/comments/1jigip/simple_question_about_gibbs_sampling_with_a/","author_flair_css_class":null,"created_utc":1375383750,"over_18":false,"gilded":0}
{"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411980709,"gilded":0,"author_flair_css_class":null,"created_utc":1375381536,"over_18":false,"permalink":"/r/MachineLearning/comments/1jidle/eric_schmidt_says_that_a_computer_will_pass_the/","num_comments":11,"stickied":false,"user_reports":[],"title":"Eric Schmidt says that a computer will Pass the Turing Test in 5 Years","banned_by":null,"author":"wisintel","downs":0,"link_flair_text":null,"selftext":"","domain":"singularityherald.com","id":"1jidle","media_embed":{},"thumbnail":"http://c.thumbs.redditmedia.com/R5gpxs43gSkEexIt.jpg","selftext_html":null,"is_self":false,"report_reasons":null,"url":"http://www.singularityherald.com/2013/08/01/eric-scmidt-turing-test-passed-in-next-5-years/","score":11,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":11}
{"gilded":0,"permalink":"/r/MachineLearning/comments/1jhgds/infographic_supervisedunsupervised_learning/","author_flair_css_class":null,"created_utc":1375350024,"over_18":false,"stickied":false,"user_reports":[],"num_comments":0,"title":"Infographic: Supervised/Unsupervised Learning Classification","banned_by":null,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411982074,"score":1,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":1,"subreddit":"MachineLearning","selftext":"","link_flair_text":null,"author":"saulsherry","downs":0,"domain":"bigdatarepublic.com","thumbnail":"http://a.thumbs.redditmedia.com/uFCZ8aXCsuNMMfJT.jpg","is_self":false,"selftext_html":null,"id":"1jhgds","media_embed":{},"report_reasons":null,"url":"http://www.bigdatarepublic.com/author.asp?section_id=2642&amp;doc_id=266179&amp;"}
{"domain":"self.MachineLearning","link_flair_text":null,"selftext":"In the paper http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf they calculated \"The approximate upper-bounds on the difference required to be statistically significant at the p &lt; 0.05 level are listed in table 1, column∆.\" \n\nCan someone explain me how this can be done?","author":"[deleted]","downs":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jhfgp/calculate_significant_differences_of_classifier/","thumbnail":"default","is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the paper &lt;a href=\"http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf\"&gt;http://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf&lt;/a&gt; they calculated &amp;quot;The approximate upper-bounds on the difference required to be statistically significant at the p &amp;lt; 0.05 level are listed in table 1, column∆.&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;Can someone explain me how this can be done?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","id":"1jhfgp","media_embed":{},"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":2,"ups":2,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411982112,"permalink":"/r/MachineLearning/comments/1jhfgp/calculate_significant_differences_of_classifier/","author_flair_css_class":null,"over_18":false,"created_utc":1375348453,"gilded":0,"title":"Calculate significant differences of classifier results for a corpus","banned_by":null,"stickied":false,"user_reports":[],"num_comments":0}
{"subreddit_id":"t5_2r3gv","ups":1,"subreddit":"MachineLearning","score":1,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"selftext_html":null,"is_self":false,"thumbnail":"default","id":"1jhdoa","media_embed":{},"report_reasons":null,"url":"http://www.machineryshops.com/product/DDC-Conditioner.html","selftext":"","link_flair_text":null,"downs":0,"author":"Niki_Lei","domain":"machineryshops.com","stickied":false,"user_reports":[],"num_comments":0,"title":"SBTZ Series DDC Conditioner","banned_by":null,"gilded":0,"permalink":"/r/MachineLearning/comments/1jhdoa/sbtz_series_ddc_conditioner/","author_flair_css_class":null,"over_18":false,"created_utc":1375345562,"retrieved_on":1411982182,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null}
{"permalink":"/r/MachineLearning/comments/1jkhg2/how_can_an_ensemble_of_predictive_models_provide/","over_18":false,"created_utc":1375457457,"author_flair_css_class":null,"gilded":0,"banned_by":null,"title":"How can an ensemble of predictive models provide better predictions than any individual model in the ensemble?","user_reports":[],"stickied":false,"num_comments":30,"secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411977907,"author_flair_text":null,"edited":1375462516,"media":null,"secure_media_embed":{},"score":17,"ups":17,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","domain":"self.MachineLearning","link_flair_text":null,"selftext":"Consider a situation where we have 99 mediocre predictive models, and 1 good one.  The models are tasked with predicting the probability of a particular classification.  We combine the model's predictions by averaging them to obtain the ensemble's prediction.\n\nWouldn't the 99 bad models (which will produce probabilities closer to the global probability of that classification) drag the prediction of the good model back towards the global mean - making it a worse prediction?\n\nOr should I be using something other than averaging to combine these probabilities?\n\n**edit:** Lots of people are suggesting that I look at boosting.  From what I've read it is sensitive to noisy data, and my data is extremely noisy (I'm predicting the probability that people will click on something).\n\nMy real question is not so much whether there may be better approaches, I'm sure there are, but whether my approach is seriously flawed.","author":"sanity","downs":0,"url":"http://www.reddit.com/r/MachineLearning/comments/1jkhg2/how_can_an_ensemble_of_predictive_models_provide/","report_reasons":null,"is_self":true,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Consider a situation where we have 99 mediocre predictive models, and 1 good one.  The models are tasked with predicting the probability of a particular classification.  We combine the model&amp;#39;s predictions by averaging them to obtain the ensemble&amp;#39;s prediction.&lt;/p&gt;\n\n&lt;p&gt;Wouldn&amp;#39;t the 99 bad models (which will produce probabilities closer to the global probability of that classification) drag the prediction of the good model back towards the global mean - making it a worse prediction?&lt;/p&gt;\n\n&lt;p&gt;Or should I be using something other than averaging to combine these probabilities?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt; Lots of people are suggesting that I look at boosting.  From what I&amp;#39;ve read it is sensitive to noisy data, and my data is extremely noisy (I&amp;#39;m predicting the probability that people will click on something).&lt;/p&gt;\n\n&lt;p&gt;My real question is not so much whether there may be better approaches, I&amp;#39;m sure there are, but whether my approach is seriously flawed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","media_embed":{},"id":"1jkhg2"}
{"author_flair_css_class":null,"over_18":false,"created_utc":1375409336,"permalink":"/r/MachineLearning/comments/1jjdm0/suggestions_for_visualizing_highdimensional/","gilded":0,"banned_by":null,"title":"Suggestions for visualizing high-dimensional clusters","num_comments":3,"stickied":false,"user_reports":[],"link_flair_css_class":null,"secure_media":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411979371,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"score":2,"subreddit":"MachineLearning","ups":2,"subreddit_id":"t5_2r3gv","domain":"self.MachineLearning","author":"WallyMetropolis","downs":0,"selftext":"For example, how would you visualize the results of doing k-means on the 20 newsgroups? Looking for a few ideas for projecting sparse document vectors down to 2- or 3-D, just so I can draw some pretty pictures and impress friends and loved ones. \n\nThanks","link_flair_text":null,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jjdm0/suggestions_for_visualizing_highdimensional/","id":"1jjdm0","media_embed":{},"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example, how would you visualize the results of doing k-means on the 20 newsgroups? Looking for a few ideas for projecting sparse document vectors down to 2- or 3-D, just so I can draw some pretty pictures and impress friends and loved ones. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true}
{"permalink":"/r/MachineLearning/comments/1jj9y0/why_dont_sigmoid_and_tanh_neural_nets_behave/","author_flair_css_class":null,"created_utc":1375406140,"over_18":false,"gilded":0,"banned_by":null,"title":"Why don't sigmoid and tanh neural nets behave equivalently?","stickied":false,"user_reports":[],"num_comments":22,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411979507,"edited":1375411208,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":16,"ups":16,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","domain":"self.MachineLearning","selftext":"A sigmoid net can emulate a tanh net of the same architecture, and vice versa. I calculated the gradient for a tanh net, and used the chain rule to find the corresponding gradient for a sigmoid net that emulated that net, and found the same exact gradient as for a sigmoid net. What am I missing?\n\n**Edit**: It turns out that if learning occurs by following the gradient in the tanh net, and one observes what happens in the corresponding sigmoid net, the gradient of the sigmoid net is not followed. I guess I could calculate the tanh gradient and transform it into updates for a sigmoid net to simulate a tanh net with a sigmoid net. I couldn't find any literature on this, so I'm still suspicious I'm overlooking something.\n\n**Edit**: By sigmoid function, I am referring to 1/(1 + exp(-x)).","link_flair_text":null,"author":"justonium","downs":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jj9y0/why_dont_sigmoid_and_tanh_neural_nets_behave/","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A sigmoid net can emulate a tanh net of the same architecture, and vice versa. I calculated the gradient for a tanh net, and used the chain rule to find the corresponding gradient for a sigmoid net that emulated that net, and found the same exact gradient as for a sigmoid net. What am I missing?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: It turns out that if learning occurs by following the gradient in the tanh net, and one observes what happens in the corresponding sigmoid net, the gradient of the sigmoid net is not followed. I guess I could calculate the tanh gradient and transform it into updates for a sigmoid net to simulate a tanh net with a sigmoid net. I couldn&amp;#39;t find any literature on this, so I&amp;#39;m still suspicious I&amp;#39;m overlooking something.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: By sigmoid function, I am referring to 1/(1 + exp(-x)).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"id":"1jj9y0","media_embed":{}}
{"num_comments":2,"stickied":false,"user_reports":[],"title":"How easy/difficult is it for a programmer with machine learning experience or a statistician with programming experience to get a job in the other's field [X-post AskStatistics]","banned_by":null,"gilded":0,"author_flair_css_class":null,"over_18":false,"created_utc":1375404896,"permalink":"/r/MachineLearning/comments/1jj8ig/how_easydifficult_is_it_for_a_programmer_with/","retrieved_on":1411979559,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":1,"score":1,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"id":"1jj8ig","media_embed":{},"thumbnail":"default","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jj8ig/how_easydifficult_is_it_for_a_programmer_with/","downs":0,"author":"double_snap","link_flair_text":null,"selftext":"","domain":"self.MachineLearning"}
{"num_comments":27,"stickied":false,"user_reports":[],"title":"Who is attending the European Conference on Machine Learning in Prague?","banned_by":null,"gilded":0,"author_flair_css_class":null,"created_utc":1375559060,"over_18":false,"permalink":"/r/MachineLearning/comments/1jn37g/who_is_attending_the_european_conference_on/","retrieved_on":1411974299,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":10,"score":10,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"id":"1jn37g","media_embed":{},"thumbnail":"self","is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious.  I am coming from Canada to present a paper at the Sports Analytics workshop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jn37g/who_is_attending_the_european_conference_on/","author":"ScullerLite","downs":0,"selftext":"I am curious.  I am coming from Canada to present a paper at the Sports Analytics workshop.","link_flair_text":null,"domain":"self.MachineLearning"}
{"retrieved_on":1411972368,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"title":"Can someone explain Kernel Trick intuitively?","banned_by":null,"stickied":false,"user_reports":[],"num_comments":23,"permalink":"/r/MachineLearning/comments/1joh9v/can_someone_explain_kernel_trick_intuitively/","author_flair_css_class":null,"over_18":false,"created_utc":1375624646,"gilded":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1joh9v/can_someone_explain_kernel_trick_intuitively/","is_self":true,"thumbnail":"self","selftext_html":null,"id":"1joh9v","media_embed":{},"domain":"self.MachineLearning","selftext":"","link_flair_text":null,"downs":0,"author":"Intern_MSFT","ups":37,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":37}
{"score":30,"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"subreddit_id":"t5_2r3gv","ups":30,"subreddit":"MachineLearning","link_flair_text":null,"selftext":"I've been trying to wrap my head around the Deep Learning scene for a while now, but I'm merely an enthusiast and get a little lost in the details once in a while. \n \nFollowing the literature over the last few years, it seemed like deep methods like RBMs etc. had a way of leveraging unlabeled data for e.g. a classification task, which is otherwise a task involving labeled data. As far as I understood, pre-training made the network generatively model the data, and when coupled to your classification task (via backpropagating using the labels) this would outperform methods that learnt through the labels alone. \n\nThen Hinton recently gave his formula for DREDNET, a deep network with rectified linear units and dropout, which seems to be a return to form for supervised neural networks. \n\nMy question, then, is: where does this leave semi-supervised models, particularly Bengio's Deep Generative Stochastic Networks? (Important enough to get a Wired article yet the only benchmark I can find is MNIST) ","downs":0,"author":"[deleted]","domain":"self.MachineLearning","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying to wrap my head around the Deep Learning scene for a while now, but I&amp;#39;m merely an enthusiast and get a little lost in the details once in a while. &lt;/p&gt;\n\n&lt;p&gt;Following the literature over the last few years, it seemed like deep methods like RBMs etc. had a way of leveraging unlabeled data for e.g. a classification task, which is otherwise a task involving labeled data. As far as I understood, pre-training made the network generatively model the data, and when coupled to your classification task (via backpropagating using the labels) this would outperform methods that learnt through the labels alone. &lt;/p&gt;\n\n&lt;p&gt;Then Hinton recently gave his formula for DREDNET, a deep network with rectified linear units and dropout, which seems to be a return to form for supervised neural networks. &lt;/p&gt;\n\n&lt;p&gt;My question, then, is: where does this leave semi-supervised models, particularly Bengio&amp;#39;s Deep Generative Stochastic Networks? (Important enough to get a Wired article yet the only benchmark I can find is MNIST) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"media_embed":{},"id":"1jo953","url":"http://www.reddit.com/r/MachineLearning/comments/1jo953/with_hintons_drednets_what_happened_to/","report_reasons":null,"gilded":0,"permalink":"/r/MachineLearning/comments/1jo953/with_hintons_drednets_what_happened_to/","created_utc":1375607514,"over_18":false,"author_flair_css_class":null,"user_reports":[],"stickied":false,"num_comments":17,"title":"With Hinton's DREDNETs, what happened to semi-supervised density modeling?","banned_by":null,"distinguished":null,"mod_reports":[],"secure_media":null,"link_flair_css_class":null,"retrieved_on":1411972685}
{"stickied":false,"user_reports":[],"num_comments":1,"banned_by":null,"title":"Introduction to Recommender Systems","gilded":0,"permalink":"/r/MachineLearning/comments/1jrx4c/introduction_to_recommender_systems/","author_flair_css_class":null,"created_utc":1375746040,"over_18":false,"retrieved_on":1411967347,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"subreddit_id":"t5_2r3gv","ups":7,"subreddit":"MachineLearning","score":7,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"is_self":false,"selftext_html":null,"thumbnail":"http://d.thumbs.redditmedia.com/r-cyl8H1eT-eJ5lo.jpg","id":"1jrx4c","media_embed":{},"report_reasons":null,"url":"https://www.coursera.org/course/recsys","selftext":"","link_flair_text":null,"author":"chocolategirl","downs":0,"domain":"coursera.org"}
{"gilded":0,"author_flair_css_class":null,"created_utc":1375706892,"over_18":false,"permalink":"/r/MachineLearning/comments/1jqj7o/impressions_from_icml_2013/","num_comments":1,"stickied":false,"user_reports":[],"banned_by":null,"title":"Impressions from ICML 2013","mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411969366,"score":20,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":20,"author":"urish","downs":0,"selftext":"","link_flair_text":null,"domain":"compneurobio.wordpress.com","id":"1jqj7o","media_embed":{},"thumbnail":"http://d.thumbs.redditmedia.com/N_XlesLJr_RMRIy3.jpg","selftext_html":null,"is_self":false,"report_reasons":null,"url":"http://compneurobio.wordpress.com/2013/07/30/impressions-from-icml-2013/"}
{"permalink":"/r/MachineLearning/comments/1jqhyv/climin_optimization_straight_forward/","author_flair_css_class":null,"over_18":false,"created_utc":1375705249,"gilded":0,"banned_by":null,"title":"climin; optimization, straight forward","stickied":false,"user_reports":[],"num_comments":1,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411969417,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":19,"ups":19,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","domain":"climin.readthedocs.org","selftext":"","link_flair_text":null,"downs":0,"author":"sieisteinmodel","report_reasons":null,"url":"http://climin.readthedocs.org/en/latest/","is_self":false,"selftext_html":null,"thumbnail":"default","id":"1jqhyv","media_embed":{}}
{"score":0,"media":{"oembed":{"height":450,"author_url":"http://www.youtube.com/user/paulgfy","width":600,"thumbnail_width":480,"provider_name":"YouTube","html":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/9sFYSW6QHCQ?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","description":"I won't tell, that would be ...cheating.","url":"http://www.youtube.com/watch?v=9sFYSW6QHCQ","title":"Willy Wonka Computer","thumbnail_url":"http://i1.ytimg.com/vi/9sFYSW6QHCQ/hqdefault.jpg","type":"video","provider_url":"http://www.youtube.com/","thumbnail_height":360,"version":"1.0","author_name":"paulgfy"},"type":"youtube.com"},"secure_media_embed":{},"author_flair_text":null,"edited":false,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":0,"author":"shaggorama","downs":0,"link_flair_text":null,"selftext":"","domain":"youtube.com","media_embed":{"width":600,"content":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/9sFYSW6QHCQ?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","scrolling":false,"height":450},"id":"1ju8rf","selftext_html":null,"thumbnail":"default","is_self":false,"url":"http://www.youtube.com/watch?v=9sFYSW6QHCQ","report_reasons":null,"gilded":0,"created_utc":1375825258,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1ju8rf/an_innovative_technique_for_handling_models_that/","num_comments":0,"user_reports":[],"stickied":false,"banned_by":null,"title":"An innovative technique for handling models that produce difficult to interpret results","mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null,"retrieved_on":1411963700}
{"retrieved_on":1411963726,"secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"title":"Andrew Ng's Stanford Course Materials","banned_by":null,"user_reports":[],"stickied":false,"num_comments":2,"permalink":"/r/MachineLearning/comments/1ju85h/andrew_ngs_stanford_course_materials/","over_18":false,"created_utc":1375824851,"author_flair_css_class":null,"gilded":0,"url":"http://cs229.stanford.edu","report_reasons":null,"selftext_html":null,"thumbnail":"default","is_self":false,"media_embed":{},"id":"1ju85h","domain":"cs229.stanford.edu","selftext":"","link_flair_text":null,"downs":0,"author":"Ars-Nocendi","ups":14,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":14}
{"stickied":false,"user_reports":[],"num_comments":1,"title":"Theoretical Limits in Machine Learning for the NHL","banned_by":null,"gilded":0,"permalink":"/r/MachineLearning/comments/1jtmeo/theoretical_limits_in_machine_learning_for_the_nhl/","author_flair_css_class":null,"over_18":false,"created_utc":1375809334,"retrieved_on":1411964676,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"subreddit_id":"t5_2r3gv","ups":25,"subreddit":"MachineLearning","score":25,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"selftext_html":null,"thumbnail":"http://e.thumbs.redditmedia.com/b6glfFyyb83psZYX.jpg","is_self":false,"id":"1jtmeo","media_embed":{},"report_reasons":null,"url":"http://nhlnumbers.com/2013/8/6/theoretical-predictions-in-machine-learning-for-the-nhl-part-ii","selftext":"","link_flair_text":null,"downs":0,"author":"ScullerLite","domain":"nhlnumbers.com"}
{"domain":"self.MachineLearning","link_flair_text":null,"selftext":"I was wondering about the way people organize their machine learning projects. Specifically, it is very common to have a pipeline starting with data in some sort of database, which is fed through several sequential algorithms (an example taken from Andrew Ng's course - we start with raw images, then extract locations of digits which appear in them, then feed these to a digit-recognizer).\n\n1. Where do you store intermediary results? \n2. How do you store your trained classifiers?\n3. How do you store the results of different parameterizations or hyper-parametrizations of your algorithms? (for example, assuming that one layer is perfect so that we can see how that affects the final output)\n\n\nThanks!","author":"orangecat99","downs":0,"url":"http://www.reddit.com/r/MachineLearning/comments/1js67c/how_do_you_organize_your_machinelearning_pipeline/","report_reasons":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering about the way people organize their machine learning projects. Specifically, it is very common to have a pipeline starting with data in some sort of database, which is fed through several sequential algorithms (an example taken from Andrew Ng&amp;#39;s course - we start with raw images, then extract locations of digits which appear in them, then feed these to a digit-recognizer).&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Where do you store intermediary results? &lt;/li&gt;\n&lt;li&gt;How do you store your trained classifiers?&lt;/li&gt;\n&lt;li&gt;How do you store the results of different parameterizations or hyper-parametrizations of your algorithms? (for example, assuming that one layer is perfect so that we can see how that affects the final output)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"media_embed":{},"id":"1js67c","author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":29,"ups":29,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411966976,"permalink":"/r/MachineLearning/comments/1js67c/how_do_you_organize_your_machinelearning_pipeline/","over_18":false,"created_utc":1375753429,"author_flair_css_class":null,"gilded":0,"title":"How do you organize your machine-learning pipeline?","banned_by":null,"user_reports":[],"stickied":false,"num_comments":17}
{"downs":0,"author":"reenigne","selftext":"","link_flair_text":null,"domain":"self.MachineLearning","id":"1jx4v0","media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"default","report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jx4v0/using_ml_to_build_a_model_from_large_texts/","score":2,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":2,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411958907,"gilded":0,"author_flair_css_class":null,"over_18":false,"created_utc":1375919254,"permalink":"/r/MachineLearning/comments/1jx4v0/using_ml_to_build_a_model_from_large_texts/","num_comments":13,"stickied":false,"user_reports":[],"banned_by":null,"title":"Using ML to build a model from large texts"}
{"gilded":0,"permalink":"/r/MachineLearning/comments/1jwk4p/sometimes_simplest_learners_are_best_a_short/","author_flair_css_class":null,"over_18":false,"created_utc":1375904067,"stickied":false,"user_reports":[],"num_comments":4,"banned_by":null,"title":"Sometimes simplest learners are best -- a short article reporting results from a training algorithm showdown, plus bonus comic strip","distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411959840,"score":18,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":18,"subreddit":"MachineLearning","selftext":"","link_flair_text":null,"author":"skytomorrownow","downs":0,"domain":"yaroslavvb.blogspot.com","thumbnail":"http://c.thumbs.redditmedia.com/exzZEeZ8aLW4jCSJ.jpg","selftext_html":null,"is_self":false,"id":"1jwk4p","media_embed":{},"report_reasons":null,"url":"http://yaroslavvb.blogspot.com/2010/10/sometimes-simplest-learners-are-best.html"}
{"score":44,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":44,"author":"DrJosh","downs":0,"selftext":"Hello redditors, I'm Josh Bongard, a robotics professor at the University of Vermont.\n\nWe have just launched 'Ludobots', an online evolutionary robotics course. After you've completed all 10 assignments, you can work with us -- and your fellow users -- on research projects, or even create a project of your own. Depending on your contribution, you could end up as a co-author on a research paper.\n\nAny feedback on the course is welcome. Additionally, I'll be doing an AMA at 4pm EST today to answer questions about the site, the field of robotics, and anything else you'd like to ask.\n\nhttp://www.uvm.edu/~ludobots/index.php/Discover/Discover","link_flair_text":null,"domain":"self.MachineLearning","media_embed":{},"id":"1jwie8","thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello redditors, I&amp;#39;m Josh Bongard, a robotics professor at the University of Vermont.&lt;/p&gt;\n\n&lt;p&gt;We have just launched &amp;#39;Ludobots&amp;#39;, an online evolutionary robotics course. After you&amp;#39;ve completed all 10 assignments, you can work with us -- and your fellow users -- on research projects, or even create a project of your own. Depending on your contribution, you could end up as a co-author on a research paper.&lt;/p&gt;\n\n&lt;p&gt;Any feedback on the course is welcome. Additionally, I&amp;#39;ll be doing an AMA at 4pm EST today to answer questions about the site, the field of robotics, and anything else you&amp;#39;d like to ask.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.uvm.edu/%7Eludobots/index.php/Discover/Discover\"&gt;http://www.uvm.edu/~ludobots/index.php/Discover/Discover&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/1jwie8/online_evolutionary_robotics_course_crossposted/","report_reasons":null,"gilded":0,"created_utc":1375902919,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jwie8/online_evolutionary_robotics_course_crossposted/","num_comments":12,"user_reports":[],"stickied":false,"banned_by":null,"title":"Online evolutionary robotics course (crossposted to /r/artificial and /r/robotics)","mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null,"retrieved_on":1411959918}
{"link_flair_text":null,"selftext":"","author":"LightSIDELabs","downs":0,"domain":"lightsidelabs.com","thumbnail":"default","selftext_html":null,"is_self":false,"id":"1jvm8y","media_embed":{},"report_reasons":null,"url":"http://lightsidelabs.com/2013/08/07/lightsides-top-ten-papers-at-acl-2013/","score":8,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":8,"subreddit":"MachineLearning","distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411961374,"gilded":0,"permalink":"/r/MachineLearning/comments/1jvm8y/the_best_new_research_in_computational/","author_flair_css_class":null,"created_utc":1375876807,"over_18":false,"stickied":false,"user_reports":[],"num_comments":1,"banned_by":null,"title":"The best new research in computational linguistics (from ACL 2013)"}
{"gilded":0,"permalink":"/r/MachineLearning/comments/1jvejw/ideas_for_smsmining/","author_flair_css_class":null,"over_18":false,"created_utc":1375864347,"stickied":false,"user_reports":[],"num_comments":12,"banned_by":null,"title":"Ideas for sms-mining.","distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411961723,"score":5,"edited":1375864548,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":5,"subreddit":"MachineLearning","selftext":"I was feeling bored, so I pulled all text-message data from my phone to my computer in a csv file. It contains date, time, whether it was out- or in-going, phone number and name of other person, and the text message itself. Since most separators, like commas, won't work because of confusion with the texts, I used pipes (|) (took a lot of regexes). \n\nNow for the fun part. What could one learn from this?\nOne thing I've thought of is the distribution of waitingtimes between messages. Now when my gf says I never answer, I have data to prove her wrong (or right). I guess it would be some independent poisson mixture, one for waitingtimes of days, and one for rapid back-and-forth messaging.\n\nAnother thing would be text-mining on the messages, like sentiment analysis and looking at trends, but that would take some work, and I'm looking for low-hanging fruit.\n\nHas anyone done something like this before? I'd love to hear your ideas!\n\nPS: I'm using R.","link_flair_text":null,"author":"nuhuskerjegdetmand","downs":0,"domain":"self.MachineLearning","thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was feeling bored, so I pulled all text-message data from my phone to my computer in a csv file. It contains date, time, whether it was out- or in-going, phone number and name of other person, and the text message itself. Since most separators, like commas, won&amp;#39;t work because of confusion with the texts, I used pipes (|) (took a lot of regexes). &lt;/p&gt;\n\n&lt;p&gt;Now for the fun part. What could one learn from this?\nOne thing I&amp;#39;ve thought of is the distribution of waitingtimes between messages. Now when my gf says I never answer, I have data to prove her wrong (or right). I guess it would be some independent poisson mixture, one for waitingtimes of days, and one for rapid back-and-forth messaging.&lt;/p&gt;\n\n&lt;p&gt;Another thing would be text-mining on the messages, like sentiment analysis and looking at trends, but that would take some work, and I&amp;#39;m looking for low-hanging fruit.&lt;/p&gt;\n\n&lt;p&gt;Has anyone done something like this before? I&amp;#39;d love to hear your ideas!&lt;/p&gt;\n\n&lt;p&gt;PS: I&amp;#39;m using R.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"id":"1jvejw","media_embed":{},"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jvejw/ideas_for_smsmining/"}
{"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":42,"score":42,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"id":"1jv2d0","media_embed":{},"selftext_html":null,"thumbnail":"default","is_self":false,"report_reasons":null,"url":"http://isse.sourceforge.net/index.html","downs":0,"author":"Minger","link_flair_text":null,"selftext":"","domain":"isse.sourceforge.net","num_comments":3,"stickied":false,"user_reports":[],"title":"Adobe, CCRMA and Stanford release sound separation software based on machine learning.","banned_by":null,"gilded":0,"author_flair_css_class":null,"over_18":false,"created_utc":1375849378,"permalink":"/r/MachineLearning/comments/1jv2d0/adobe_ccrma_and_stanford_release_sound_separation/","retrieved_on":1411962295,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null}
{"link_flair_text":null,"selftext":"","author":"[deleted]","downs":0,"domain":"isse.sourceforge.net","thumbnail":"default","is_self":false,"selftext_html":null,"id":"1juz8r","media_embed":{},"report_reasons":null,"url":"http://isse.sourceforge.net/index.html","score":1,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":1,"subreddit":"MachineLearning","distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411962446,"gilded":0,"permalink":"/r/MachineLearning/comments/1juz8r/xposted_from_rmashups_adobe_ccrma_and_stanford/","author_flair_css_class":null,"over_18":false,"created_utc":1375846622,"stickied":false,"user_reports":[],"num_comments":0,"banned_by":null,"title":"[x-posted from r/mashups] Adobe, CCRMA and Stanford release sound separation software based on machine learning."}
{"selftext_html":null,"is_self":true,"thumbnail":"default","media_embed":{},"id":"1jzjef","url":"http://www.reddit.com/r/MachineLearning/comments/1jzjef/using_machine_learning_to_predict_a_single_game/","report_reasons":null,"selftext":"","link_flair_text":null,"author":"[deleted]","downs":0,"domain":"self.MachineLearning","subreddit_id":"t5_2r3gv","ups":1,"subreddit":"MachineLearning","score":1,"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"retrieved_on":1411954929,"distinguished":null,"mod_reports":[],"secure_media":null,"link_flair_css_class":null,"user_reports":[],"stickied":false,"num_comments":0,"banned_by":null,"title":"Using Machine Learning to Predict a single game in the NHL - I am presenting this at a Sports Analytics ML workshop in Prague","gilded":0,"permalink":"/r/MachineLearning/comments/1jzjef/using_machine_learning_to_predict_a_single_game/","created_utc":1376000403,"over_18":false,"author_flair_css_class":null}
{"selftext":"","link_flair_text":null,"downs":0,"author":"rrenaud","domain":"zinkov.com","thumbnail":"http://e.thumbs.redditmedia.com/o5bkU-waLZNEL5Up.jpg","is_self":false,"selftext_html":null,"id":"1jzbvt","media_embed":{},"report_reasons":null,"url":"http://zinkov.com/posts/2013-07-28-stop-using-plates/","score":33,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"subreddit_id":"t5_2r3gv","ups":33,"subreddit":"MachineLearning","distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411955277,"gilded":0,"permalink":"/r/MachineLearning/comments/1jzbvt/stop_using_plate_notation/","author_flair_css_class":null,"created_utc":1375994971,"over_18":false,"stickied":false,"user_reports":[],"num_comments":18,"title":"Stop using Plate Notation","banned_by":null}
{"retrieved_on":1411955316,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"title":"A blog post explaining the Cheng and Church biclustering algorithm","banned_by":null,"stickied":false,"user_reports":[],"num_comments":0,"permalink":"/r/MachineLearning/comments/1jzb0k/a_blog_post_explaining_the_cheng_and_church/","author_flair_css_class":null,"created_utc":1375994339,"over_18":false,"gilded":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jzb0k/a_blog_post_explaining_the_cheng_and_church/","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been implementing biclustering algorithms for scikit-learn and blogging about them. Here is the latest, which is on Cheng and Church:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"http://www.kemaleren.com/cheng-and-church.html\"&gt;Cheng and Church&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here are the discussions the previous ones:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"http://www.reddit.com/r/MachineLearning/comments/1hn9gf/a_series_of_blog_posts_about_biclustering/\"&gt;An introduction to biclustering and Spectral Co-Clustering&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"http://www.reddit.com/r/machinelearning/comments/1i6ntd/spectral_biclustering_part_2/\"&gt;Spectral Biclustering&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"id":"1jzb0k","media_embed":{},"domain":"self.MachineLearning","link_flair_text":null,"selftext":"I have been implementing biclustering algorithms for scikit-learn and blogging about them. Here is the latest, which is on Cheng and Church:\n\n* [Cheng and Church](http://www.kemaleren.com/cheng-and-church.html)\n\nHere are the discussions the previous ones:\n\n* [An introduction to biclustering and Spectral Co-Clustering](http://www.reddit.com/r/MachineLearning/comments/1hn9gf/a_series_of_blog_posts_about_biclustering/)\n\n* [Spectral Biclustering](http://www.reddit.com/r/machinelearning/comments/1i6ntd/spectral_biclustering_part_2/)","downs":0,"author":"kemal_eren","ups":10,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":10}
{"ups":7,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":7,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1jyint/is_anyone_else_here_attending_the_mlss_2013_in/","selftext_html":null,"thumbnail":"self","is_self":true,"id":"1jyint","media_embed":{},"domain":"self.MachineLearning","link_flair_text":null,"selftext":"","author":"jamesmcm","downs":0,"banned_by":null,"title":"Is anyone else here attending the MLSS 2013 in Tuebingen, Germany?","stickied":false,"user_reports":[],"num_comments":2,"permalink":"/r/MachineLearning/comments/1jyint/is_anyone_else_here_attending_the_mlss_2013_in/","author_flair_css_class":null,"created_utc":1375973948,"over_18":false,"gilded":0,"retrieved_on":1411956639,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[]}
{"subreddit":"MachineLearning","ups":20,"subreddit_id":"t5_2r3gv","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":20,"url":"http://www.nature.com/nature/journal/vaop/ncurrent/full/nature12160.html","report_reasons":null,"media_embed":{},"id":"1jxy2f","selftext_html":null,"thumbnail":"default","is_self":false,"domain":"nature.com","downs":0,"author":"ha3virus","selftext":"","link_flair_text":null,"title":"New single unit recordings suggest that prefrontal cortex uses something like \"the kernel trick\" of support vector machines. [xpost form /r/neuro]","banned_by":null,"num_comments":16,"user_reports":[],"stickied":false,"over_18":false,"created_utc":1375946265,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jxy2f/new_single_unit_recordings_suggest_that/","gilded":0,"retrieved_on":1411957572,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null}
{"title":"Learning how","banned_by":null,"num_comments":0,"user_reports":[],"stickied":false,"created_utc":1375922735,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jx951/learning_how/","gilded":0,"retrieved_on":1411958717,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"subreddit":"MachineLearning","ups":1,"subreddit_id":"t5_2r3gv","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":1,"url":"http://www.reddit.com/r/MachineLearning/comments/1jx951/learning_how/","report_reasons":null,"media_embed":{},"id":"1jx951","is_self":true,"thumbnail":"default","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://waltherpragerandphilosophy1.blogspot.com/2012/04/note-on-learning.html\"&gt;http://waltherpragerandphilosophy1.blogspot.com/2012/04/note-on-learning.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","downs":0,"author":"[deleted]","selftext":"http://waltherpragerandphilosophy1.blogspot.com/2012/04/note-on-learning.html","link_flair_text":null}
{"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":4,"subreddit":"MachineLearning","ups":4,"subreddit_id":"t5_2r3gv","domain":"newfolder.github.io","author":"peterTorrione","downs":0,"selftext":"","link_flair_text":null,"url":"http://newfolder.github.io/blog/2013/07/29/supervised-learning/","report_reasons":null,"media_embed":{},"id":"1jx8m2","thumbnail":"default","selftext_html":null,"is_self":false,"over_18":false,"created_utc":1375922310,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jx8m2/what_is_supervised_learning_an_introduction_for/","gilded":0,"banned_by":null,"title":"What is supervised learning? An Introduction for Scientists &amp; Engineers","num_comments":0,"user_reports":[],"stickied":false,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411958738}
{"domain":"self.MachineLearning","downs":0,"author":"blackrat47","link_flair_text":null,"selftext":"Apologies if this is inappropiate, but I'm fairly new to ML and having a bit of trouble finding resources for this particular problem.\n\nI have 30 observations in 2 classes (15 in each). Each observation has several thousand variables (this could be reduced in a somewhat hand-wavy way, but I'd rather not). All variables are continuous; some are normally distributed and some aren't; some are most likely redundant; some are highly informative and others aren't/ are misinformative. I'm using SVM with the RBF kernel (from libSVM in Matlab) to build classifiers, using leave-one-out cross validation (or leave-pair-out, removing one from each class, for tests with fewer iterations) to test the feature selection algorithm, but I'm having real trouble finding a feature selection algorithm which is at all stable across the different iterations of the LOOCV. \n\nAt first I tried ranking features in terms of their contrast-to-noise ratio and building a classifier by using the top one, then the top two, then the top n, and finding the optimal classifier out of those, but it meant that a lot of redundant information was included (possibly weighting the classifier in an unhelpful manner), and the results were poor, as well as the choice of features being very unstable- I think because small variations in CNR cause quite large changes in CNR rank. Then I tried greedy forward selection, which was better (80% sensitivity, 87% specificity), but still each classifier was picking up different features (although some were picked more frequently than others). The greedy algorithm used LOOCV within the remaining 29 variables to choose which feature should be added, so it was a sort of (LOO^2). It would be interesting to use the probability of any feature being selected to weight the final classifier, but to test this I'd need to go to a third level of LOO, which is getting absurd.\n\nAt the moment I'm trying to reduce the number of features by combining covarying variables, using PCA. However, it's my understanding that PCA doesn't really work very well with such rectangular data, and so I'd need to heuristically reduce the number of features first for it to effective. In particular, I found that the contrast:noise ratio of each PCC-transformed variable had no correlation with the latent of the PCC (even when the latent was 0). This means that PCA doesn't actually reduce the search space at all. *Edit: Also, none of the features actually seem to have a very high covariance.*\n\nHave I missed some handy redundancy reduction, dimensionality reduction or other feature selection algorithm which is useful for this sort of data? Or is it crazy to be even looking at this rectangular a data set, and I should be trying to massively cut down the number of variables that I'm feeding in to any algorithm?\n\nEDIT: Thanks for the help, guys! In the unlikely event that I can squeeze a publication out of this in the next couple of months I'll do my best to big up /r/machinelearning. ","url":"http://www.reddit.com/r/MachineLearning/comments/1k0op2/help_dealing_with_highvariable_relatively/","report_reasons":null,"media_embed":{},"id":"1k0op2","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is inappropiate, but I&amp;#39;m fairly new to ML and having a bit of trouble finding resources for this particular problem.&lt;/p&gt;\n\n&lt;p&gt;I have 30 observations in 2 classes (15 in each). Each observation has several thousand variables (this could be reduced in a somewhat hand-wavy way, but I&amp;#39;d rather not). All variables are continuous; some are normally distributed and some aren&amp;#39;t; some are most likely redundant; some are highly informative and others aren&amp;#39;t/ are misinformative. I&amp;#39;m using SVM with the RBF kernel (from libSVM in Matlab) to build classifiers, using leave-one-out cross validation (or leave-pair-out, removing one from each class, for tests with fewer iterations) to test the feature selection algorithm, but I&amp;#39;m having real trouble finding a feature selection algorithm which is at all stable across the different iterations of the LOOCV. &lt;/p&gt;\n\n&lt;p&gt;At first I tried ranking features in terms of their contrast-to-noise ratio and building a classifier by using the top one, then the top two, then the top n, and finding the optimal classifier out of those, but it meant that a lot of redundant information was included (possibly weighting the classifier in an unhelpful manner), and the results were poor, as well as the choice of features being very unstable- I think because small variations in CNR cause quite large changes in CNR rank. Then I tried greedy forward selection, which was better (80% sensitivity, 87% specificity), but still each classifier was picking up different features (although some were picked more frequently than others). The greedy algorithm used LOOCV within the remaining 29 variables to choose which feature should be added, so it was a sort of (LOO&lt;sup&gt;2).&lt;/sup&gt; It would be interesting to use the probability of any feature being selected to weight the final classifier, but to test this I&amp;#39;d need to go to a third level of LOO, which is getting absurd.&lt;/p&gt;\n\n&lt;p&gt;At the moment I&amp;#39;m trying to reduce the number of features by combining covarying variables, using PCA. However, it&amp;#39;s my understanding that PCA doesn&amp;#39;t really work very well with such rectangular data, and so I&amp;#39;d need to heuristically reduce the number of features first for it to effective. In particular, I found that the contrast:noise ratio of each PCC-transformed variable had no correlation with the latent of the PCC (even when the latent was 0). This means that PCA doesn&amp;#39;t actually reduce the search space at all. &lt;em&gt;Edit: Also, none of the features actually seem to have a very high covariance.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Have I missed some handy redundancy reduction, dimensionality reduction or other feature selection algorithm which is useful for this sort of data? Or is it crazy to be even looking at this rectangular a data set, and I should be trying to massively cut down the number of variables that I&amp;#39;m feeding in to any algorithm?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Thanks for the help, guys! In the unlikely event that I can squeeze a publication out of this in the next couple of months I&amp;#39;ll do my best to big up &lt;a href=\"/r/machinelearning\"&gt;/r/machinelearning&lt;/a&gt;. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":1376064747,"score":18,"subreddit":"MachineLearning","ups":18,"subreddit_id":"t5_2r3gv","secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411953075,"created_utc":1376045283,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k0op2/help_dealing_with_highvariable_relatively/","gilded":0,"title":"[Help] Dealing with high-variable, (relatively) low-observation data","banned_by":null,"num_comments":25,"user_reports":[],"stickied":false}
{"link_flair_css_class":null,"secure_media":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411953531,"author_flair_css_class":null,"created_utc":1376028105,"over_18":false,"permalink":"/r/MachineLearning/comments/1k0ei1/why_do_texture_filter_banks_change_the_size_of/","gilded":0,"title":"Why do texture filter banks change the size of the data?","banned_by":null,"num_comments":5,"stickied":false,"user_reports":[],"domain":"self.MachineLearning","downs":0,"author":"logrech","link_flair_text":null,"selftext":"I'm new to texture analysis. I was experimenting with some pretty standard filter banks when I came across something strange. \n\nThe data I'm working with is a 2D matrix of dimensions: 379x422. \n\nWhen I put the data through the filterbank, the dimensions of the responses were 427x470. \n\nThe filterbank I'm working with is the The Leung-Malik (LM) Filter Bank. It consists of first and second derivatives of Gaussians at 6 orientations and 3 scales making a total of 36; 8 Laplacian of Gaussian (LOG) filters; and 4 Gaussians.\n\nEssentially, I'm just wondering why the responses have more data than the original? What's going on here? ","report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k0ei1/why_do_texture_filter_banks_change_the_size_of/","id":"1k0ei1","media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to texture analysis. I was experimenting with some pretty standard filter banks when I came across something strange. &lt;/p&gt;\n\n&lt;p&gt;The data I&amp;#39;m working with is a 2D matrix of dimensions: 379x422. &lt;/p&gt;\n\n&lt;p&gt;When I put the data through the filterbank, the dimensions of the responses were 427x470. &lt;/p&gt;\n\n&lt;p&gt;The filterbank I&amp;#39;m working with is the The Leung-Malik (LM) Filter Bank. It consists of first and second derivatives of Gaussians at 6 orientations and 3 scales making a total of 36; 8 Laplacian of Gaussian (LOG) filters; and 4 Gaussians.&lt;/p&gt;\n\n&lt;p&gt;Essentially, I&amp;#39;m just wondering why the responses have more data than the original? What&amp;#39;s going on here? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"self","secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"score":2,"subreddit":"MachineLearning","ups":2,"subreddit_id":"t5_2r3gv"}
{"subreddit":"MachineLearning","ups":0,"subreddit_id":"t5_2r3gv","secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"score":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k0clo/has_reddit_become_a_delivery_vehicle_for_the_nyt/","id":"1k0clo","media_embed":{},"thumbnail":"self","is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some customization algorithm is wreaking havoc with my reddit -- the first 150 articles are from the New York Times (I haven&amp;#39;t checked further down). Help appreciated. I can provide screenshots is that would help, or my preferences pane, but it&amp;#39;s pretty vanilla and I haven&amp;#39;t changed it in ages. Or am I a victim of location-awareness? I&amp;#39;m reading reddit from a hotel room in Bulgaria...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","downs":0,"author":"fooazma","link_flair_text":null,"selftext":"Some customization algorithm is wreaking havoc with my reddit -- the first 150 articles are from the New York Times (I haven't checked further down). Help appreciated. I can provide screenshots is that would help, or my preferences pane, but it's pretty vanilla and I haven't changed it in ages. Or am I a victim of location-awareness? I'm reading reddit from a hotel room in Bulgaria...","banned_by":null,"title":"Has reddit become a delivery vehicle for the NYT?","num_comments":1,"stickied":false,"user_reports":[],"author_flair_css_class":null,"over_18":false,"created_utc":1376025828,"permalink":"/r/MachineLearning/comments/1k0clo/has_reddit_become_a_delivery_vehicle_for_the_nyt/","gilded":0,"retrieved_on":1411953618,"link_flair_css_class":null,"secure_media":null,"mod_reports":[],"distinguished":null}
{"title":"Using Machine Learning to predict games in hockey - I am presenting this at a Sports Analytics / ML workshop in September","banned_by":null,"num_comments":6,"user_reports":[],"stickied":false,"created_utc":1376007214,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1jzrs5/using_machine_learning_to_predict_games_in_hockey/","gilded":0,"retrieved_on":1411954554,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"subreddit":"MachineLearning","ups":7,"subreddit_id":"t5_2r3gv","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":7,"url":"http://www.scribd.com/doc/158486700/Use-of-Performance-Metrics-to-Forecast-Success-in-the-National-Hockey-League","report_reasons":null,"media_embed":{},"id":"1jzrs5","selftext_html":null,"thumbnail":"http://a.thumbs.redditmedia.com/peoYrYiR6icjf26f.jpg","is_self":false,"domain":"scribd.com","author":"ScullerLite","downs":0,"link_flair_text":null,"selftext":""}
{"retrieved_on":1411947529,"distinguished":null,"mod_reports":[],"secure_media":null,"link_flair_css_class":null,"user_reports":[],"stickied":false,"num_comments":3,"banned_by":null,"title":"The Variational Approximation for Bayesian Inference: Life after the EM algorithm","gilded":0,"permalink":"/r/MachineLearning/comments/1k41k1/the_variational_approximation_for_bayesian/","created_utc":1376173205,"over_18":false,"author_flair_css_class":null,"selftext_html":null,"thumbnail":"default","is_self":false,"media_embed":{},"id":"1k41k1","url":"http://www.cs.uoi.gr/~arly/papers/SPM08.pdf","report_reasons":null,"link_flair_text":null,"selftext":"","downs":0,"author":"alfonsoeromero","domain":"cs.uoi.gr","subreddit_id":"t5_2r3gv","ups":23,"subreddit":"MachineLearning","score":23,"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{}}
{"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k3c6z/struggling_to_learn_machine_learning_on_my_own/","is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m studying Machine Learning on my own, but with some difficulties. I tried many books on Machine Learning but it wasn&amp;#39;t easy to find the right one.&lt;/p&gt;\n\n&lt;p&gt;Bishop&amp;#39;s &amp;quot;Pattern Recognition and Machine Learning&amp;quot; is a very hard read. In my opinion, the problem is not the material but the exposition. Many derivations are left to the reader and there are too many &amp;quot;it&amp;#39;s trivial to see&amp;quot;, &amp;quot;it can be readily seen&amp;quot; and &amp;quot;after some straightforward algebra&amp;quot;. In the end, I gave up.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The Elements of Statistical Learning&amp;quot; (Hastie et al.) suffers from the same problems. Moreover, some explanations I couldn&amp;#39;t follow because they referred to concepts I wasn&amp;#39;t familiar with. All these books claim that you just need to know some calculus, probability and linear algebra, but that&amp;#39;s a lie. Bishop even tries to teach you basic probability, suggesting that his book can be read by one who doesn&amp;#39;t know probability, which is absurd.&lt;/p&gt;\n\n&lt;p&gt;The lessons by Andrew Ng are easy to follow but they&amp;#39;re not very deep. They show you a collection of techniques but they don&amp;#39;t provide the theory that should guide you in using these techniques. Also, many techniques are presented in their simplest form.\nThe lectures by Tom Mitchell are also very easy to follow but I think he oversimplifies things. By the way, he says that after taking his course one can do research and read papers without problems. I wish it was that easy.&lt;/p&gt;\n\n&lt;p&gt;The same material can be presented at very different levels and I found out that there is a noticeable gap between elementary texts and lessons, and advanced ones. I was looking for something advanced but at the same time accessible. In my opinion, a text can be advanced and at the same time introductory. Introductory should mean that no prior knowledge of the topic is assumed. Many books claim to be introductory but they&amp;#39;re not. Some explanations are so cryptic that only one with a prior exposure to the material would benefit from them.&lt;/p&gt;\n\n&lt;p&gt;Finally, I found the right book for me: Pattern Classification (Duda, Hart, Stork, 2ed.). The book doesn&amp;#39;t shy away from advanced material and the explanations are great. Finally, a book that I can study on my own without having to rely on somebody else for additional explanations!&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your experience with Machine Learning books, textbooks and lectures?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","id":"1k3c6z","media_embed":{},"domain":"self.MachineLearning","link_flair_text":null,"selftext":"I'm studying Machine Learning on my own, but with some difficulties. I tried many books on Machine Learning but it wasn't easy to find the right one.\n\nBishop's \"Pattern Recognition and Machine Learning\" is a very hard read. In my opinion, the problem is not the material but the exposition. Many derivations are left to the reader and there are too many \"it's trivial to see\", \"it can be readily seen\" and \"after some straightforward algebra\". In the end, I gave up.\n\n\"The Elements of Statistical Learning\" (Hastie et al.) suffers from the same problems. Moreover, some explanations I couldn't follow because they referred to concepts I wasn't familiar with. All these books claim that you just need to know some calculus, probability and linear algebra, but that's a lie. Bishop even tries to teach you basic probability, suggesting that his book can be read by one who doesn't know probability, which is absurd.\n\nThe lessons by Andrew Ng are easy to follow but they're not very deep. They show you a collection of techniques but they don't provide the theory that should guide you in using these techniques. Also, many techniques are presented in their simplest form.\nThe lectures by Tom Mitchell are also very easy to follow but I think he oversimplifies things. By the way, he says that after taking his course one can do research and read papers without problems. I wish it was that easy.\n\nThe same material can be presented at very different levels and I found out that there is a noticeable gap between elementary texts and lessons, and advanced ones. I was looking for something advanced but at the same time accessible. In my opinion, a text can be advanced and at the same time introductory. Introductory should mean that no prior knowledge of the topic is assumed. Many books claim to be introductory but they're not. Some explanations are so cryptic that only one with a prior exposure to the material would benefit from them.\n\nFinally, I found the right book for me: Pattern Classification (Duda, Hart, Stork, 2ed.). The book doesn't shy away from advanced material and the explanations are great. Finally, a book that I can study on my own without having to rely on somebody else for additional explanations!\n\nWhat's your experience with Machine Learning books, textbooks and lectures?","downs":0,"author":"Kiuhnm","ups":58,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":58,"retrieved_on":1411948733,"link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"banned_by":null,"title":"Struggling to learn Machine Learning on my own","stickied":false,"user_reports":[],"num_comments":37,"permalink":"/r/MachineLearning/comments/1k3c6z/struggling_to_learn_machine_learning_on_my_own/","author_flair_css_class":null,"over_18":false,"created_utc":1376149222,"gilded":0}
{"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411945276,"over_18":false,"created_utc":1376243446,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k5jtf/pystruct_01_released_structured_prediction_and/","gilded":0,"banned_by":null,"title":"PyStruct 0.1 released! Structured prediction and learning in Python.","num_comments":5,"user_reports":[],"stickied":false,"domain":"pystruct.github.io","downs":0,"author":"t3kcit","link_flair_text":null,"selftext":"","url":"http://pystruct.github.io","report_reasons":null,"media_embed":{},"id":"1k5jtf","is_self":false,"selftext_html":null,"thumbnail":"default","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":22,"subreddit":"MachineLearning","ups":22,"subreddit_id":"t5_2r3gv"}
{"gilded":0,"author_flair_css_class":null,"over_18":false,"created_utc":1376183122,"permalink":"/r/MachineLearning/comments/1k4bk8/what_are_some_good_resources_to_learn_and/","num_comments":9,"stickied":false,"user_reports":[],"title":"What are some good resources to learn and practice machine learning that offer step-by-step instructions?","banned_by":null,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null,"retrieved_on":1411947128,"score":4,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":4,"author":"mrlovell","downs":0,"selftext":"","link_flair_text":null,"domain":"self.MachineLearning","id":"1k4bk8","media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"default","report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k4bk8/what_are_some_good_resources_to_learn_and/"}
{"retrieved_on":1411941421,"mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null,"num_comments":0,"user_reports":[],"stickied":false,"banned_by":null,"title":"Distributed Word Representation for all languages (Online Demo)","gilded":0,"created_utc":1376334782,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k83ys/distributed_word_representation_for_all_languages/","media_embed":{},"id":"1k83ys","thumbnail":"default","selftext_html":null,"is_self":false,"url":"https://sites.google.com/site/rmyeid/projects/polyglot","report_reasons":null,"downs":0,"author":"rmyeid","link_flair_text":null,"selftext":"","domain":"sites.google.com","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":1,"score":1,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false}
{"subreddit":"MachineLearning","ups":0,"subreddit_id":"t5_2r3gv","secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"score":0,"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k7pz6/testing_an_autoencoder_application/","id":"1k7pz6","media_embed":{},"thumbnail":"default","is_self":true,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","downs":0,"author":"eubarch","selftext":"","link_flair_text":null,"title":"Testing an autoencoder application","banned_by":null,"num_comments":3,"stickied":false,"user_reports":[],"author_flair_css_class":null,"created_utc":1376324471,"over_18":false,"permalink":"/r/MachineLearning/comments/1k7pz6/testing_an_autoencoder_application/","gilded":0,"retrieved_on":1411942023,"link_flair_css_class":null,"secure_media":null,"mod_reports":[],"distinguished":null}
{"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":25,"score":25,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"media_embed":{},"id":"1k7o48","is_self":false,"thumbnail":"default","selftext_html":null,"url":"http://videolectures.net/colt2013_lecun_theory/","report_reasons":null,"downs":0,"author":"rrenaud","link_flair_text":null,"selftext":"","domain":"videolectures.net","num_comments":1,"user_reports":[],"stickied":false,"title":"Learning Representations: Yann LeCun's Challenge to the Learning Theory community","banned_by":null,"gilded":0,"over_18":false,"created_utc":1376322961,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k7o48/learning_representations_yann_lecuns_challenge_to/","retrieved_on":1411942103,"mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null}
{"subreddit":"MachineLearning","ups":69,"subreddit_id":"t5_2r3gv","secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"score":69,"report_reasons":null,"url":"http://www.autonlab.org/tutorials/prob18.pdf","id":"1k74we","media_embed":{},"selftext_html":null,"thumbnail":"default","is_self":false,"domain":"autonlab.org","downs":0,"author":"advait","selftext":"","link_flair_text":null,"banned_by":null,"title":"Andrew Moore's slides on Bayesian classifiers — the most accessible (yet mathematically rigorous) introduction to the topic I have ever read.","num_comments":6,"stickied":false,"user_reports":[],"author_flair_css_class":null,"over_18":false,"created_utc":1376299907,"permalink":"/r/MachineLearning/comments/1k74we/andrew_moores_slides_on_bayesian_classifiers_the/","gilded":0,"retrieved_on":1411942894,"link_flair_css_class":null,"secure_media":null,"mod_reports":[],"distinguished":null}
{"retrieved_on":1411936513,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"banned_by":null,"title":"Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning","num_comments":21,"user_reports":[],"stickied":false,"over_18":false,"created_utc":1376435950,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1kb977/andrew_ng_deep_learning_selftaught_learning_and/","gilded":0,"url":"https://www.youtube.com/watch?v=n1ViNeWhC24","report_reasons":null,"media_embed":{"height":338,"scrolling":false,"content":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/n1ViNeWhC24?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600},"id":"1kb977","thumbnail":"http://f.thumbs.redditmedia.com/AVv6OW45ZYYWUDd_.jpg","selftext_html":null,"is_self":false,"domain":"youtube.com","downs":0,"author":"Slartibartfastibast","selftext":"","link_flair_text":null,"subreddit":"MachineLearning","ups":92,"subreddit_id":"t5_2r3gv","media":{"type":"youtube.com","oembed":{"author_url":"http://www.youtube.com/channel/UCCDVzRMhcdTQXzzoLcIE17A","height":338,"html":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/n1ViNeWhC24?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","provider_name":"YouTube","description":"Graduate Summer School: Deep Learning, Feature Learning \"Deep Learning, Self-Taught Learning and Unsupervised Feature Learning (Part 1 Slides1-68; Part 2 Slides 69-109)\" https://www.ipam.ucla.edu/publications/gss2012/gss2012_10595.pdf","width":600,"thumbnail_width":480,"thumbnail_url":"http://i1.ytimg.com/vi/n1ViNeWhC24/hqdefault.jpg","type":"video","provider_url":"http://www.youtube.com/","title":"Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning","url":"http://www.youtube.com/watch?v=n1ViNeWhC24","author_name":"鑫 黄","thumbnail_height":360,"version":"1.0"}},"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":92}
{"over_18":false,"created_utc":1376430836,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1kb2nw/brains_and_machine_intelligence_a_long_time_coming/","gilded":0,"banned_by":null,"title":"Brains and Machine Intelligence, A Long Time Coming","num_comments":3,"user_reports":[],"stickied":false,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411936815,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":10,"subreddit":"MachineLearning","ups":10,"subreddit_id":"t5_2r3gv","domain":"numenta.org","author":"numenta","downs":0,"link_flair_text":null,"selftext":"","url":"http://numenta.org/news/2013/08/13/brains-and-machine-intelligence-a-long-time-coming.html","report_reasons":null,"media_embed":{},"id":"1kb2nw","is_self":false,"selftext_html":null,"thumbnail":"http://a.thumbs.redditmedia.com/vH5V8smbwg_qhoer.jpg"}
{"domain":"self.MachineLearning","selftext":"I saw that Eric Siegel released this new \"popular science\" book and it is difficult for me to assess whether it's a relevant read to a layperson. Did anyone with background in the field read the book and can comment?","link_flair_text":null,"downs":0,"author":"dtelad11","url":"http://www.reddit.com/r/MachineLearning/comments/1kb2ex/anyone_familiar_with_eric_siegels_latest_book/","report_reasons":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw that Eric Siegel released this new &amp;quot;popular science&amp;quot; book and it is difficult for me to assess whether it&amp;#39;s a relevant read to a layperson. Did anyone with background in the field read the book and can comment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"media_embed":{},"id":"1kb2ex","author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":4,"ups":4,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411936827,"permalink":"/r/MachineLearning/comments/1kb2ex/anyone_familiar_with_eric_siegels_latest_book/","created_utc":1376430656,"over_18":false,"author_flair_css_class":null,"gilded":0,"title":"Anyone familiar with Eric Siegel's latest book? (Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die)","banned_by":null,"user_reports":[],"stickied":false,"num_comments":0}
{"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":1,"score":1,"secure_media_embed":{},"media":null,"edited":false,"author_flair_text":null,"id":"1kar5k","media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Machine learning is the task of finding a concise set of rules to generalize some behavior in data. I&amp;#39;ve long wondered if one could create a learning algorithm based on regular expressions, which are meant to do something similar (explain a whole bunch of complicated features with highly general constructs).  As simple contrived examples, regular expressions would make a nice classifier to discriminate numbers from words, or tell whether a string is an email address.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there&amp;#39;s some deep connection with encoding, compression, manifolds, etc. between ML and the way regexps work.  Anyone else thought about this or seen work along these lines? I imagine there is some way to extend regexps to numerical data?&lt;/p&gt;\n\n&lt;p&gt;(For clarity, I am talking about using regexps to do ML, not using ML to find regexps)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"self","report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1kar5k/a_connection_between_regular_expressions_and/","author":"willis77","downs":0,"link_flair_text":null,"selftext":"Machine learning is the task of finding a concise set of rules to generalize some behavior in data. I've long wondered if one could create a learning algorithm based on regular expressions, which are meant to do something similar (explain a whole bunch of complicated features with highly general constructs).  As simple contrived examples, regular expressions would make a nice classifier to discriminate numbers from words, or tell whether a string is an email address.\n\nI'm sure there's some deep connection with encoding, compression, manifolds, etc. between ML and the way regexps work.  Anyone else thought about this or seen work along these lines? I imagine there is some way to extend regexps to numerical data?\n\n(For clarity, I am talking about using regexps to do ML, not using ML to find regexps)","domain":"self.MachineLearning","num_comments":16,"stickied":false,"user_reports":[],"title":"A connection between regular expressions and machine learning?","banned_by":null,"gilded":0,"author_flair_css_class":null,"created_utc":1376422565,"over_18":false,"permalink":"/r/MachineLearning/comments/1kar5k/a_connection_between_regular_expressions_and/","retrieved_on":1411937326,"mod_reports":[],"distinguished":null,"link_flair_css_class":null,"secure_media":null}
{"domain":"bizjournals.com","author":"Seboskovitch","downs":0,"link_flair_text":null,"selftext":"","url":"http://www.bizjournals.com/seattle/blog/techflash/2013/08/google-scientist-jeff-dean-on-how.html?ana=rdt","report_reasons":null,"media_embed":{},"id":"1k9w4l","is_self":false,"selftext_html":null,"thumbnail":"http://d.thumbs.redditmedia.com/pU2ZhUWrU3KRD2A9.jpg","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":18,"subreddit":"MachineLearning","ups":18,"subreddit_id":"t5_2r3gv","secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"retrieved_on":1411938744,"over_18":false,"created_utc":1376398010,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k9w4l/google_scientist_jeff_dean_on_how_neural_networks/","gilded":0,"title":"Google scientist Jeff Dean on how neural networks are improving everything Google does","banned_by":null,"num_comments":0,"user_reports":[],"stickied":false}
{"stickied":false,"user_reports":[],"num_comments":0,"banned_by":null,"title":"Hierarchical models, seasonality and frequency domain","gilded":0,"permalink":"/r/MachineLearning/comments/1k9vvd/hierarchical_models_seasonality_and_frequency/","author_flair_css_class":null,"over_18":false,"created_utc":1376397690,"retrieved_on":1411938759,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"subreddit_id":"t5_2r3gv","ups":2,"subreddit":"MachineLearning","score":2,"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"thumbnail":"default","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of the main challenges in my line of work is detecting and controlling (the latter part has more standard methods) periodic behavior in short samples of notionally very large time-series. For example, we&amp;#39;ll have two years&amp;#39; worth of sales data, when sales have been going forever and respond to a number of seasonality-inducing economic factors that didn&amp;#39;t just appear in two years.&lt;/p&gt;\n\n&lt;p&gt;My standard approach is filtering with regression-based estimated of the Fourier transform (the Christiano-Fitzgerald filters). This gives me an estimate of a frequency v. power domain, which is useful for selecting cut-off frequencies from the ones which have significant power. Where &amp;quot;significant power&amp;quot; means &amp;quot;reading spikes off the chart&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So I was reading a tutorial on decision trees posted here (maybe on &lt;a href=\"/r/math\"&gt;/r/math&lt;/a&gt;?) yesterday and it dawned on me that a such a model should be able to &amp;quot;read&amp;quot; the frequency profiles of time series prioritizing according to information gain so to classify them into previously-known &amp;quot;kinds of seasonality&amp;quot; (depending on crops, on retail sales, etc.). But given the nature of my work and the data we wrangle, I can&amp;#39;t just hack ahead :/ I need published literature precedents that can I can point to and say &amp;quot;see? this is not some crazy idea I had, this is actually done.&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Anyone knows anything like this in the literature?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"id":"1k9vvd","media_embed":{},"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k9vvd/hierarchical_models_seasonality_and_frequency/","link_flair_text":null,"selftext":"One of the main challenges in my line of work is detecting and controlling (the latter part has more standard methods) periodic behavior in short samples of notionally very large time-series. For example, we'll have two years' worth of sales data, when sales have been going forever and respond to a number of seasonality-inducing economic factors that didn't just appear in two years.\n\nMy standard approach is filtering with regression-based estimated of the Fourier transform (the Christiano-Fitzgerald filters). This gives me an estimate of a frequency v. power domain, which is useful for selecting cut-off frequencies from the ones which have significant power. Where \"significant power\" means \"reading spikes off the chart\".\n\nSo I was reading a tutorial on decision trees posted here (maybe on /r/math?) yesterday and it dawned on me that a such a model should be able to \"read\" the frequency profiles of time series prioritizing according to information gain so to classify them into previously-known \"kinds of seasonality\" (depending on crops, on retail sales, etc.). But given the nature of my work and the data we wrangle, I can't just hack ahead :/ I need published literature precedents that can I can point to and say \"see? this is not some crazy idea I had, this is actually done.\".\n\nAnyone knows anything like this in the literature?","downs":0,"author":"[deleted]","domain":"self.MachineLearning"}
{"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to predict resource utilisation of grid (1000+ servers) using history information about tasks and grid utilisation while it was running. It&amp;#39;s not clear for me - how to use ML algos to get predicted utilization curve using curves and task parameters from training sets available.&lt;/p&gt;\n\n&lt;p&gt;Could you please give me any ideas to start from? Any papers\\articles\\books\\links?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","is_self":true,"id":"1k9rgv","media_embed":{},"report_reasons":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1k9rgv/ml_in_capacity_management/","link_flair_text":null,"selftext":"I need to predict resource utilisation of grid (1000+ servers) using history information about tasks and grid utilisation while it was running. It's not clear for me - how to use ML algos to get predicted utilization curve using curves and task parameters from training sets available.\n\nCould you please give me any ideas to start from? Any papers\\articles\\books\\links?","downs":0,"author":"Stas911","domain":"self.MachineLearning","subreddit_id":"t5_2r3gv","ups":3,"subreddit":"MachineLearning","score":3,"edited":1376401434,"author_flair_text":null,"secure_media_embed":{},"media":null,"retrieved_on":1411938938,"distinguished":null,"mod_reports":[],"link_flair_css_class":null,"secure_media":null,"stickied":false,"user_reports":[],"num_comments":5,"title":"ML in capacity management","banned_by":null,"gilded":0,"permalink":"/r/MachineLearning/comments/1k9rgv/ml_in_capacity_management/","author_flair_css_class":null,"created_utc":1376391265,"over_18":false}
{"url":"http://www.reddit.com/r/MachineLearning/comments/1k8w2y/who_is_the_neil_degrasse_tyson_of_machine_learning/","report_reasons":null,"media_embed":{},"id":"1k8w2y","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"default","domain":"self.MachineLearning","author":"sanity","downs":0,"selftext":"","link_flair_text":null,"subreddit":"MachineLearning","ups":0,"subreddit_id":"t5_2r3gv","media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"score":0,"retrieved_on":1411940258,"secure_media":null,"link_flair_css_class":null,"mod_reports":[],"distinguished":null,"banned_by":null,"title":"Who is the Neil deGrasse Tyson of Machine Learning?","num_comments":0,"user_reports":[],"stickied":false,"created_utc":1376355921,"over_18":false,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1k8w2y/who_is_the_neil_degrasse_tyson_of_machine_learning/","gilded":0}
{"url":"http://www.reddit.com/r/MachineLearning/comments/1kdpjo/where_are_some_topnotch_universities_for/","author":"Fa1l3r","title":"Where are some top-notch universities for undergraduate education in machine learning?","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"media":null,"link_flair_css_class":null,"ups":4,"author_flair_css_class":null,"selftext":"As a high school student, I have two career goals: a professor of computer science (or mathematics) in statistics or machine learning, or a engineer building upon or starting projects (i.e. the Google self-driving car) about artificial intelligence. What are some top-notch universities to jump-start (i.e. research for undergraduates) me (in my undergraduate career) toward these goals? \n\nAlso, if you can, I would like colleges with good financial aid, and I do not trust any rankings of colleges.\n\nedit: It would also be in the US for financial reasons.","created_utc":1376519918,"is_self":true,"thumbnail":"self","downs":0,"report_reasons":null,"distinguished":null,"edited":1376525253,"retrieved_on":1411932579,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a high school student, I have two career goals: a professor of computer science (or mathematics) in statistics or machine learning, or a engineer building upon or starting projects (i.e. the Google self-driving car) about artificial intelligence. What are some top-notch universities to jump-start (i.e. research for undergraduates) me (in my undergraduate career) toward these goals? &lt;/p&gt;\n\n&lt;p&gt;Also, if you can, I would like colleges with good financial aid, and I do not trust any rankings of colleges.&lt;/p&gt;\n\n&lt;p&gt;edit: It would also be in the US for financial reasons.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"score":4,"permalink":"/r/MachineLearning/comments/1kdpjo/where_are_some_topnotch_universities_for/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"num_comments":9,"id":"1kdpjo","user_reports":[],"over_18":false}
{"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"url":"http://richardminerich.com/2013/08/all-machine-learning-platforms-are-terrible-but-some-less-so/","subreddit_id":"t5_2r3gv","title":"All Machine Learning Platforms are Terrible (but some less so)","author":"Rickasaurus","subreddit":"MachineLearning","stickied":false,"selftext":"","created_utc":1376515162,"is_self":false,"media":null,"link_flair_css_class":null,"ups":7,"author_flair_css_class":null,"retrieved_on":1411932847,"selftext_html":null,"mod_reports":[],"thumbnail":"default","downs":0,"distinguished":null,"report_reasons":null,"edited":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"id":"1kdj0u","num_comments":25,"user_reports":[],"over_18":false,"score":7,"permalink":"/r/MachineLearning/comments/1kdj0u/all_machine_learning_platforms_are_terrible_but/","domain":"richardminerich.com"}
{"url":"http://www.reddit.com/r/MachineLearning/comments/1kc8o7/understanding_qlearning_in_neural_networks/","report_reasons":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;ve been struggling to learn how to apply Q-learning to ANN&amp;#39;s. I understand that they work mostly by using MLP feed forward neural nets using gradient descent back propagation. My problem is understanding the right way to use the Q-values I get to update the neural network.&lt;/p&gt;\n\n&lt;p&gt;Take for instance the mountain car problem, it is continuous states with 3 actions&lt;/p&gt;\n\n&lt;p&gt;Car_position = [-1.2 0.6]\nCar_velocity= [-0.07 0.07]\nPossible actions =[Rev, Neutral(do nothing), Fwd]\nthe car starts every episode  in state -0.5 position and 0.0 velocity&lt;/p&gt;\n\n&lt;p&gt;Now the idea is to create a neural network to replace the Q-table that I would normally have right?\nTherefor a neural network with 2 inputs(real numbers for position and velocity), a hidden layer of nodes( 5-25 or so) and 3 output nodes corresponding to the actions seems like a good idea. &lt;/p&gt;\n\n&lt;p&gt;Is this the right process now:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Run the network( Feed forward the state -0.5, 0.0 ) to get 3 q values, one for each action . These are Q-values for state s (the current state)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Choose an action a using E-greedy, either pick the highest Q-value or random&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Simulate the Mountain Car one step and obtain a reward and new state s&amp;#39; from the executed action&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run the network with state s&amp;#39; to get 3 new Q-values for s&amp;#39;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Calculate QTarget = reward + gamma* Max Q-value for s&amp;#39;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The Target pattern for the update of the weights is then either 0;0;QTarget or 0;QTarget or QTarget;0;0 since we don&amp;#39;t know how good the Q-values are of the actions we did not take, and we want to move the Q-value of s corresponding to action taken&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;set s = s&amp;#39; and repeat the process until # of learning episodes elapsed&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m using Matlab with NN toolbox to create, init and update the weights. So therefore I use the newff with sigmoid in the hidden and linear in the output.&lt;/p&gt;\n\n&lt;p&gt;Updating is done using the net=train(net,s,Targets) function? The parameter s is a matrix like so [-0.5; 0.0]. I selected the traingdm as the training function&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","is_self":true,"thumbnail":"self","media_embed":{},"id":"1kc8o7","domain":"self.MachineLearning","link_flair_text":null,"selftext":"Hey all, I've been struggling to learn how to apply Q-learning to ANN's. I understand that they work mostly by using MLP feed forward neural nets using gradient descent back propagation. My problem is understanding the right way to use the Q-values I get to update the neural network.\n\nTake for instance the mountain car problem, it is continuous states with 3 actions\n\nCar_position = [-1.2 0.6]\nCar_velocity= [-0.07 0.07]\nPossible actions =[Rev, Neutral(do nothing), Fwd]\nthe car starts every episode  in state -0.5 position and 0.0 velocity\n\nNow the idea is to create a neural network to replace the Q-table that I would normally have right?\nTherefor a neural network with 2 inputs(real numbers for position and velocity), a hidden layer of nodes( 5-25 or so) and 3 output nodes corresponding to the actions seems like a good idea. \n\nIs this the right process now:\n\n1. Run the network( Feed forward the state -0.5, 0.0 ) to get 3 q values, one for each action . These are Q-values for state s (the current state)\n\n2. Choose an action a using E-greedy, either pick the highest Q-value or random\n\n3. Simulate the Mountain Car one step and obtain a reward and new state s' from the executed action\n\n4. Run the network with state s' to get 3 new Q-values for s'\n\n5. Calculate QTarget = reward + gamma* Max Q-value for s'\n\n6. The Target pattern for the update of the weights is then either 0;0;QTarget or 0;QTarget or QTarget;0;0 since we don't know how good the Q-values are of the actions we did not take, and we want to move the Q-value of s corresponding to action taken\n\n7. set s = s' and repeat the process until # of learning episodes elapsed\n\nI'm using Matlab with NN toolbox to create, init and update the weights. So therefore I use the newff with sigmoid in the hidden and linear in the output.\n\nUpdating is done using the net=train(net,s,Targets) function? The parameter s is a matrix like so [-0.5; 0.0]. I selected the traingdm as the training function\n\nThanks","downs":0,"author":"SevrenBG","ups":10,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"score":10,"retrieved_on":1411934917,"secure_media":null,"link_flair_css_class":null,"distinguished":null,"mod_reports":[],"banned_by":null,"title":"Understanding Q-learning in Neural networks","user_reports":[],"stickied":false,"num_comments":4,"permalink":"/r/MachineLearning/comments/1kc8o7/understanding_qlearning_in_neural_networks/","over_18":false,"created_utc":1376474308,"author_flair_css_class":null,"gilded":0}
{"link_flair_text":null,"selftext":"","downs":0,"author":"[deleted]","domain":"self.MachineLearning","selftext_html":null,"thumbnail":"default","is_self":true,"media_embed":{},"id":"1kc37k","url":"http://www.reddit.com/r/MachineLearning/comments/1kc37k/learning/","report_reasons":null,"score":1,"author_flair_text":null,"edited":false,"media":null,"secure_media_embed":{},"subreddit_id":"t5_2r3gv","ups":1,"subreddit":"MachineLearning","distinguished":null,"mod_reports":[],"secure_media":null,"link_flair_css_class":null,"retrieved_on":1411935172,"gilded":0,"permalink":"/r/MachineLearning/comments/1kc37k/learning/","created_utc":1376464701,"over_18":false,"author_flair_css_class":null,"user_reports":[],"stickied":false,"num_comments":0,"title":"Learning","banned_by":null}
{"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","ups":12,"score":12,"media":null,"secure_media_embed":{},"author_flair_text":null,"edited":false,"media_embed":{},"id":"1kbr0l","is_self":true,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As someone coming from a Computer Science background, how do I get up to speed on statistics beyond basic stats and probability?  &lt;/p&gt;\n\n&lt;p&gt;I mean, I have a vague recollection of linear regression and hypothesis testing from way back in undergrad, but what is next?  &lt;/p&gt;\n\n&lt;p&gt;It is fairly easy now to teach yourself the coding angle of machine learning and data science online (through documentation and things like coursera), but where should the autodidact go for the statistics?&lt;/p&gt;\n\n&lt;p&gt;I assume a good first step would be to plow through a textbook... Does anyone have a recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","url":"http://www.reddit.com/r/MachineLearning/comments/1kbr0l/how_can_i_a_compsci_grad_student_selfteach/","report_reasons":null,"author":"watch_me_roll","downs":0,"link_flair_text":null,"selftext":"As someone coming from a Computer Science background, how do I get up to speed on statistics beyond basic stats and probability?  \n\nI mean, I have a vague recollection of linear regression and hypothesis testing from way back in undergrad, but what is next?  \n\nIt is fairly easy now to teach yourself the coding angle of machine learning and data science online (through documentation and things like coursera), but where should the autodidact go for the statistics?\n\nI assume a good first step would be to plow through a textbook... Does anyone have a recommendation?","domain":"self.MachineLearning","num_comments":6,"user_reports":[],"stickied":false,"title":"How can I (a CompSci grad student) self-teach statistics?","banned_by":null,"gilded":0,"over_18":false,"created_utc":1376451142,"author_flair_css_class":null,"permalink":"/r/MachineLearning/comments/1kbr0l/how_can_i_a_compsci_grad_student_selfteach/","retrieved_on":1411935724,"mod_reports":[],"distinguished":null,"secure_media":null,"link_flair_css_class":null}
{"domain":"scribd.com","link_flair_text":null,"selftext":"","author":"Badoosker","downs":0,"report_reasons":null,"url":"http://www.scribd.com/doc/160122111/Robots-in-Software","selftext_html":null,"thumbnail":"http://b.thumbs.redditmedia.com/xZ0yuMylpyoOl4uU.jpg","is_self":false,"id":"1kbjq3","media_embed":{},"edited":false,"author_flair_text":null,"secure_media_embed":{},"media":null,"score":14,"ups":14,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","link_flair_css_class":null,"secure_media":null,"distinguished":null,"mod_reports":[],"retrieved_on":1411936052,"permalink":"/r/MachineLearning/comments/1kbjq3/reddit_here_is_the_startings_of_the_book_that_was/","author_flair_css_class":null,"over_18":false,"created_utc":1376444783,"gilded":0,"banned_by":null,"title":"Reddit, here is the startings of the book that was asked of me. There is much work to do, and I would like your feedback as so far.","stickied":false,"user_reports":[],"num_comments":7}
{"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":25,"selftext":"To be presented at the Machine Learning Summer School in Tübingen (Germany), August 2013.\n\n[The MultiSkill Tennis Model: Estimating player skills on serve and return with dynamic Bayesian networks](https://github.com/danielkorzekwa/tennis-player-compare/blob/master/doc/mlss2013/tennis_skills_poster.pdf?raw=true)","created_utc":1376593321,"is_self":true,"title":"The MultiSkill Tennis Model: Estimating player skills on serve and return with dynamic Bayesian networks","subreddit_id":"t5_2r3gv","author":"danielkorzekwa","url":"http://www.reddit.com/r/MachineLearning/comments/1kfpsk/the_multiskill_tennis_model_estimating_player/","stickied":false,"subreddit":"MachineLearning","author_flair_text":null,"banned_by":null,"secure_media":null,"link_flair_text":null,"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1kfpsk/the_multiskill_tennis_model_estimating_player/","score":25,"media_embed":{},"gilded":0,"secure_media_embed":{},"over_18":false,"num_comments":7,"id":"1kfpsk","user_reports":[],"thumbnail":"self","edited":false,"distinguished":null,"report_reasons":null,"downs":0,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To be presented at the Machine Learning Summer School in Tübingen (Germany), August 2013.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/danielkorzekwa/tennis-player-compare/blob/master/doc/mlss2013/tennis_skills_poster.pdf?raw=true\"&gt;The MultiSkill Tennis Model: Estimating player skills on serve and return with dynamic Bayesian networks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411929605}
{"score":3,"permalink":"/r/MachineLearning/comments/1kfeam/any_ucsd_dmml_certificate_students_or_grads/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"num_comments":1,"id":"1kfeam","user_reports":[],"over_18":false,"thumbnail":"default","downs":0,"distinguished":null,"report_reasons":null,"edited":false,"retrieved_on":1411930080,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone completed/ enrolled in the UCSD Data Mining certificate program? Am looking for some 3rd party feedback before I discuss it with some of my coworkers and my boss.&lt;/p&gt;\n\n&lt;p&gt;Does the online set-up work well? Is it easy to understand?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://extension.ucsd.edu/programs/index.cfm?vAction=certDetail&amp;amp;vCertificateID=128&amp;amp;vStudyAreaID=14\"&gt;UCSD DM info here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"media":null,"ups":3,"author_flair_css_class":null,"created_utc":1376584707,"selftext":"Has anyone completed/ enrolled in the UCSD Data Mining certificate program? Am looking for some 3rd party feedback before I discuss it with some of my coworkers and my boss.\n  \nDoes the online set-up work well? Is it easy to understand?\n\n[UCSD DM info here](http://extension.ucsd.edu/programs/index.cfm?vAction=certDetail&amp;vCertificateID=128&amp;vStudyAreaID=14)","is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/1kfeam/any_ucsd_dmml_certificate_students_or_grads/","subreddit_id":"t5_2r3gv","title":"Any UCSD DM/ML Certificate students or grads?","author":"[deleted]","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null}
{"stickied":false,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","title":"NMF via weighted least squares?","author":"thrope","url":"http://www.reddit.com/r/MachineLearning/comments/1keqv3/nmf_via_weighted_least_squares/","secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"author_flair_css_class":null,"ups":3,"link_flair_css_class":null,"media":null,"is_self":true,"selftext":"Cross-post from CrossValidated:\n\nhttp://stats.stackexchange.com/questions/67462/weighted-least-squares-for-nmf-on-concatenated-images-of-different-sizes\n\nBriefly I want to do NMF on a set of 5 images of different sizes, but I want to weight it so each component has the same effect on the NMF (rather than being dominated by the largest one).","created_utc":1376557871,"edited":false,"distinguished":null,"report_reasons":null,"downs":0,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cross-post from CrossValidated:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://stats.stackexchange.com/questions/67462/weighted-least-squares-for-nmf-on-concatenated-images-of-different-sizes\"&gt;http://stats.stackexchange.com/questions/67462/weighted-least-squares-for-nmf-on-concatenated-images-of-different-sizes&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Briefly I want to do NMF on a set of 5 images of different sizes, but I want to weight it so each component has the same effect on the NMF (rather than being dominated by the largest one).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411931048,"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1keqv3/nmf_via_weighted_least_squares/","score":3,"over_18":false,"num_comments":0,"user_reports":[],"id":"1keqv3","media_embed":{},"gilded":0,"secure_media_embed":{}}
{"is_self":false,"created_utc":1376536548,"selftext":"","author_flair_css_class":null,"ups":47,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null,"stickied":false,"subreddit":"MachineLearning","url":"https://code.google.com/p/word2vec/","title":"word2vec - Google research tool for computing vector representations of words.","subreddit_id":"t5_2r3gv","author":"rrenaud","num_comments":15,"user_reports":[],"id":"1ke9w3","over_18":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"permalink":"/r/MachineLearning/comments/1ke9w3/word2vec_google_research_tool_for_computing/","score":47,"domain":"code.google.com","retrieved_on":1411931743,"mod_reports":[],"selftext_html":null,"report_reasons":null,"distinguished":null,"downs":0,"edited":false,"thumbnail":"default"}
{"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interested in building an autonomous mobile robot and I want it to recognize visited places. At this point I have found the vocabulary search tree and FAB-MAP but I don&amp;#39;t know if those are widely used or are state of the art techniques&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411925148,"thumbnail":"self","edited":false,"downs":0,"report_reasons":null,"distinguished":null,"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":3,"id":"1kipok","user_reports":[],"domain":"self.MachineLearning","score":6,"permalink":"/r/MachineLearning/comments/1kipok/question_which_techniques_are_preferred_or/","author_flair_text":null,"banned_by":null,"secure_media":null,"link_flair_text":null,"title":"Question: which techniques are preferred or popular in place recognition tasks?","author":"RaulPL","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kipok/question_which_techniques_are_preferred_or/","subreddit":"MachineLearning","stickied":false,"created_utc":1376695247,"selftext":"I'm interested in building an autonomous mobile robot and I want it to recognize visited places. At this point I have found the vocabulary search tree and FAB-MAP but I don't know if those are widely used or are state of the art techniques","is_self":true,"media":null,"link_flair_css_class":null,"ups":6,"author_flair_css_class":null}
{"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"author":"ismaelc","title":"List of 30+ summarizer APIs, libraries, and software","subreddit_id":"t5_2r3gv","url":"http://blog.mashape.com/post/58164039983/list-of-30-summarizer-apis-libraries-and-software","subreddit":"MachineLearning","stickied":false,"selftext":"","created_utc":1376676935,"is_self":false,"link_flair_css_class":null,"media":null,"ups":3,"author_flair_css_class":null,"selftext_html":null,"mod_reports":[],"retrieved_on":1411926108,"thumbnail":"http://d.thumbs.redditmedia.com/ON1tQD_ppByNw01i.jpg","edited":false,"downs":0,"distinguished":null,"report_reasons":null,"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":0,"id":"1ki3a9","user_reports":[],"domain":"blog.mashape.com","score":3,"permalink":"/r/MachineLearning/comments/1ki3a9/list_of_30_summarizer_apis_libraries_and_software/"}
{"permalink":"/r/MachineLearning/comments/1khq4d/random_forest_per_prediction_confidence/","score":33,"domain":"opinions5.blogspot.com","num_comments":9,"id":"1khq4d","user_reports":[],"over_18":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"distinguished":null,"report_reasons":null,"downs":0,"edited":false,"thumbnail":"http://e.thumbs.redditmedia.com/lwRp3aVTUbAF4-hv.jpg","retrieved_on":1411926647,"selftext_html":null,"mod_reports":[],"author_flair_css_class":null,"ups":33,"link_flair_css_class":null,"media":null,"is_self":false,"selftext":"","created_utc":1376666551,"stickied":false,"subreddit":"MachineLearning","url":"http://opinions5.blogspot.com/2013/08/random-forest-confidence.html","title":"Random Forest: Per prediction confidence","author":"hirak99","subreddit_id":"t5_2r3gv","link_flair_text":null,"secure_media":null,"author_flair_text":null,"banned_by":null}
{"author_flair_css_class":null,"ups":61,"media":null,"link_flair_css_class":null,"is_self":false,"created_utc":1376771182,"selftext":"","stickied":false,"subreddit":"MachineLearning","title":"Interesting insight into differences between music and movie recommendations by the guy building the recommendation system at Spotify","author":"rrenaud","subreddit_id":"t5_2r3gv","url":"http://www.quora.com/Machine-Learning/What-if-any-are-the-differences-in-the-algorithms-that-are-behind-recommendations-for-music-and-movies?share=1","secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"domain":"quora.com","permalink":"/r/MachineLearning/comments/1kkfkv/interesting_insight_into_differences_between/","score":61,"over_18":false,"num_comments":5,"id":"1kkfkv","user_reports":[],"media_embed":{},"gilded":0,"secure_media_embed":{},"edited":false,"distinguished":null,"report_reasons":null,"downs":0,"thumbnail":"http://d.thumbs.redditmedia.com/UC50_AZayHTgXMVW.jpg","mod_reports":[],"selftext_html":null,"retrieved_on":1411922656}
{"permalink":"/r/MachineLearning/comments/1kk86g/cant_we_all_just_get_along_stanford_has_2_great/","score":15,"domain":"normaldeviate.wordpress.com","secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":6,"user_reports":[],"id":"1kk86g","over_18":false,"thumbnail":"http://d.thumbs.redditmedia.com/N_XlesLJr_RMRIy3.jpg","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"retrieved_on":1411922938,"mod_reports":[],"selftext_html":null,"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":15,"created_utc":1376764321,"selftext":"","is_self":false,"url":"http://normaldeviate.wordpress.com/2013/04/13/data-science-the-end-of-statistics/","subreddit_id":"t5_2r3gv","title":"Can't we all just get along? Stanford has 2 great ML groups - 1 in stat and 1 in comp sci - separate books, separate classes - WHY?","author":"scottedwards2000","stickied":false,"subreddit":"MachineLearning","author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null}
{"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"author":"dhammack","title":"Using word2vec to determine which word isn't like the others.","subreddit_id":"t5_2r3gv","url":"https://github.com/dhammack/Word2VecExample","subreddit":"MachineLearning","stickied":false,"selftext":"","created_utc":1376855835,"is_self":false,"media":null,"link_flair_css_class":null,"ups":44,"author_flair_css_class":null,"selftext_html":null,"mod_reports":[],"retrieved_on":1411919695,"thumbnail":"http://e.thumbs.redditmedia.com/3uZxQqqmOYer9hAj.jpg","edited":false,"downs":0,"distinguished":null,"report_reasons":null,"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":17,"id":"1kme15","user_reports":[],"domain":"github.com","score":44,"permalink":"/r/MachineLearning/comments/1kme15/using_word2vec_to_determine_which_word_isnt_like/"}
{"is_self":false,"created_utc":1376854669,"selftext":"","ups":3,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"author_flair_text":null,"banned_by":null,"subreddit":"MachineLearning","stickied":false,"url":"http://google-opensource.blogspot.com/2013/08/learning-meaning-behind-words.html","author":"Seboskovitch","title":"Learning the meaning behind words","subreddit_id":"t5_2r3gv","num_comments":0,"id":"1kmcq5","user_reports":[],"over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"score":3,"permalink":"/r/MachineLearning/comments/1kmcq5/learning_the_meaning_behind_words/","domain":"google-opensource.blogspot.com","retrieved_on":1411919899,"selftext_html":null,"mod_reports":[],"downs":0,"report_reasons":null,"distinguished":null,"edited":false,"thumbnail":"http://a.thumbs.redditmedia.com/gpeg6AqZ_9VIqMzf.jpg"}
{"retrieved_on":1411921090,"mod_reports":[],"selftext_html":null,"thumbnail":"default","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"id":"1klirr","num_comments":3,"user_reports":[],"over_18":false,"score":15,"permalink":"/r/MachineLearning/comments/1klirr/api_design_for_machine_learning_software/","domain":"orbi.ulg.ac.be","banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"url":"http://orbi.ulg.ac.be/bitstream/2268/154357/1/paper.pdf","author":"glouppe","title":"API design for machine learning software: experiences from the scikit-learn project","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","stickied":false,"created_utc":1376814892,"selftext":"","is_self":false,"media":null,"link_flair_css_class":null,"ups":15,"author_flair_css_class":null}
{"retrieved_on":1411916924,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am on a classification problem that the data is a lot and the intra-class variation is highly changing. Therefore I aim to cluster the data to capture those variations and create classifiers for each clusters to be merged at the final decision on a novel instance. However I also hesitate that a simple model might be able to discriminate this data as well as the my purposed approach. What you think for the case?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"thumbnail":"self","distinguished":null,"report_reasons":null,"downs":0,"edited":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":0,"user_reports":[],"id":"1kobhz","over_18":false,"permalink":"/r/MachineLearning/comments/1kobhz/question_do_you_think_is_it_possibly_a_better/","score":1,"domain":"self.MachineLearning","banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1kobhz/question_do_you_think_is_it_possibly_a_better/","title":"QUESTION: Do you think is it possibly a better approach to cluster the data and create a ensemble of classifiers modelled by each cluster?","subreddit_id":"t5_2r3gv","author":"erogol","stickied":false,"subreddit":"MachineLearning","created_utc":1376930864,"selftext":"I am on a classification problem that the data is a lot and the intra-class variation is highly changing. Therefore I aim to cluster the data to capture those variations and create classifiers for each clusters to be merged at the final decision on a novel instance. However I also hesitate that a simple model might be able to discriminate this data as well as the my purposed approach. What you think for the case?","is_self":true,"media":null,"link_flair_css_class":null,"author_flair_css_class":null,"ups":1}
{"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":3,"id":"1knn3w","user_reports":[],"domain":"static.googleusercontent.com","score":21,"permalink":"/r/MachineLearning/comments/1knn3w/ad_click_prediction_a_view_from_the_trenches/","mod_reports":[],"selftext_html":null,"retrieved_on":1411917889,"thumbnail":"default","edited":false,"downs":0,"report_reasons":null,"distinguished":null,"created_utc":1376901590,"selftext":"","is_self":false,"link_flair_css_class":null,"media":null,"ups":21,"author_flair_css_class":null,"author_flair_text":null,"banned_by":null,"secure_media":null,"link_flair_text":null,"author":"urish","title":"Ad Click Prediction: a View from the Trenches (Google Research, PDF)","subreddit_id":"t5_2r3gv","url":"http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//pubs/archive/41159.pdf","subreddit":"MachineLearning","stickied":false}
{"is_self":true,"selftext":"I am currently a MS degree student and research assistant. I am actively researching and cannot find enough time to devote myself to learn optimization stuffs, even I tried to read the Boyd's book after some period its technical details are so complex to grasp with the research work. What do you suggest to me? How can I learn optimization in a lightweight way in parallel to my research work?","created_utc":1376899667,"ups":1,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null,"subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1knm2s/how_can_i_selfteach_optimization_for_machine/","title":"How can I self-teach optimization for machine learning ?","subreddit_id":"t5_2r3gv","author":"erogol","num_comments":0,"id":"1knm2s","user_reports":[],"over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"score":1,"permalink":"/r/MachineLearning/comments/1knm2s/how_can_i_selfteach_optimization_for_machine/","domain":"self.MachineLearning","retrieved_on":1411917931,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a MS degree student and research assistant. I am actively researching and cannot find enough time to devote myself to learn optimization stuffs, even I tried to read the Boyd&amp;#39;s book after some period its technical details are so complex to grasp with the research work. What do you suggest to me? How can I learn optimization in a lightweight way in parallel to my research work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"report_reasons":null,"distinguished":null,"edited":1376930632,"thumbnail":"self"}
{"score":15,"permalink":"/r/MachineLearning/comments/1knfoc/is_there_a_good_website_that_critiques_ml_research/","domain":"self.MachineLearning","num_comments":9,"id":"1knfoc","user_reports":[],"over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"downs":0,"report_reasons":null,"distinguished":null,"edited":false,"thumbnail":"self","retrieved_on":1411918196,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am personally writing code to recreate some ML research and I am getting very different results from what the paper describes. Is there a community that likes to vet this type of work to help divulge validity?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":15,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"is_self":true,"created_utc":1376890532,"selftext":"I am personally writing code to recreate some ML research and I am getting very different results from what the paper describes. Is there a community that likes to vet this type of work to help divulge validity?","subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1knfoc/is_there_a_good_website_that_critiques_ml_research/","author":"MentalMasochist","title":"Is there a good website that critiques ML research?","subreddit_id":"t5_2r3gv","link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null}
{"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1kn208/deep_sparse_autoencoder_with_fixed_output_for/","title":"Deep Sparse Autoencoder with fixed output for Image Registration","author":"andrewff","subreddit_id":"t5_2r3gv","stickied":false,"subreddit":"MachineLearning","created_utc":1376877669,"selftext":"Hi /r/MachineLearning\n\nI've just begun my PhD and my work is heavily in computer vision.  One of the tasks that we do commonly is image registration.  Basically this is the process of taking an input image and mapping it to a fixed space.  One of the classic examples of this is fitting images of the brain via CT, MRI, PET to Talaraich Coordinates, which are strictly defined coordinates that map the brain to a consistent space that is rotationally, translationally, and scale invariant.\n\nI was wondering if anyone thought it would be reasonable to use an autoencoder for this process.  Basically, I would want to train the autoencoder with the input being the imaging modality, CT, MRI, etc. and the output as the expected results in Talaraich space instead of the input like the typical autoencoder problem.  I've done a decent bit of reading into deep learning and how autoencoders work, but I've never seen work done with a fixed output.\n\nIf anyone is familiar with work related to this, I would be ecstatic if you could pass it on and if anyone has any ideas, thoughts, opinions etc. I would really appreciate hearing them!","is_self":true,"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":6,"retrieved_on":1411918740,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/MachineLearning\"&gt;/r/MachineLearning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just begun my PhD and my work is heavily in computer vision.  One of the tasks that we do commonly is image registration.  Basically this is the process of taking an input image and mapping it to a fixed space.  One of the classic examples of this is fitting images of the brain via CT, MRI, PET to Talaraich Coordinates, which are strictly defined coordinates that map the brain to a consistent space that is rotationally, translationally, and scale invariant.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone thought it would be reasonable to use an autoencoder for this process.  Basically, I would want to train the autoencoder with the input being the imaging modality, CT, MRI, etc. and the output as the expected results in Talaraich space instead of the input like the typical autoencoder problem.  I&amp;#39;ve done a decent bit of reading into deep learning and how autoencoders work, but I&amp;#39;ve never seen work done with a fixed output.&lt;/p&gt;\n\n&lt;p&gt;If anyone is familiar with work related to this, I would be ecstatic if you could pass it on and if anyone has any ideas, thoughts, opinions etc. I would really appreciate hearing them!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"thumbnail":"self","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":8,"user_reports":[],"id":"1kn208","over_18":false,"permalink":"/r/MachineLearning/comments/1kn208/deep_sparse_autoencoder_with_fixed_output_for/","score":6,"domain":"self.MachineLearning"}
{"secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null,"stickied":false,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","title":"Statistical Pattern Recognition Toolbox [a nice little menagerie of classifiers and demos, from linear, to k-means]","author":"skytomorrownow","url":"http://cmp.felk.cvut.cz/cmp/software/stprtool/examples.html","is_self":false,"created_utc":1377041583,"selftext":"","author_flair_css_class":null,"ups":19,"link_flair_css_class":null,"media":null,"mod_reports":[],"selftext_html":null,"retrieved_on":1411912149,"edited":false,"distinguished":null,"report_reasons":null,"downs":0,"thumbnail":"http://f.thumbs.redditmedia.com/Lhjl0r9dTU_hji8w.jpg","over_18":false,"num_comments":0,"user_reports":[],"id":"1krnwz","media_embed":{},"gilded":0,"secure_media_embed":{},"domain":"cmp.felk.cvut.cz","permalink":"/r/MachineLearning/comments/1krnwz/statistical_pattern_recognition_toolbox_a_nice/","score":19}
{"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"url":"http://www.quora.com/Data/Where-can-I-find-large-datasets-open-to-the-public","title":"Public large datasets - Quora.","subreddit_id":"t5_2r3gv","author":"imsome1","stickied":false,"subreddit":"MachineLearning","created_utc":1377027407,"selftext":"","is_self":false,"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":22,"retrieved_on":1411912886,"selftext_html":null,"mod_reports":[],"thumbnail":"http://d.thumbs.redditmedia.com/WPxXwH6aUBUNo_6E.jpg","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":2,"user_reports":[],"id":"1kr564","over_18":false,"permalink":"/r/MachineLearning/comments/1kr564/public_large_datasets_quora/","score":22,"domain":"quora.com"}
{"stickied":false,"subreddit":"MachineLearning","url":"http://www.reddit.com/r/MachineLearning/comments/1kqv5w/new_datasets_for_biglearning/","title":"New datasets for big-learning?","subreddit_id":"t5_2r3gv","author":"hadian","link_flair_text":null,"secure_media":null,"author_flair_text":null,"banned_by":null,"author_flair_css_class":null,"ups":2,"media":null,"link_flair_css_class":null,"is_self":true,"selftext":"I am searching for datasts in big-data learning to work on as my PhD thesis. I seek for big-learning datasets that have the following properties:\n\n* The problem is **new** and is hot for academic society. The corresponding dataset should be **recent** and **publicly available**.\n\n*  Results of state-of-the-art methods are not satisfactory, due to high amount of data/computation. **The main concern about the dataset should be its huge size, not the difficulty of the task itself**. E.g., Naive-bayes has low accuracy and no one has been able to test SVM on it, because maybe no SVM package can handle that much data.\n \n~~* More importantly, The dataset is preferably out of the computer-science community, i.e. few cs/ml researchers have tried to solve it. For example, NLP problems are too hard for contribution, because most NLP researchers are expert in ML &amp; Algorithms, so it is extremely difficult for me to outperform their works! I think there should be easy-to-outperform datasets in bioinformatics, but I do not know which tasks are of the big-learning scheme. It would be grateful if you suggest datasets from other fields as well :)~~","created_utc":1377020309,"distinguished":null,"report_reasons":null,"downs":0,"edited":1377052065,"thumbnail":"self","retrieved_on":1411913275,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am searching for datasts in big-data learning to work on as my PhD thesis. I seek for big-learning datasets that have the following properties:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The problem is &lt;strong&gt;new&lt;/strong&gt; and is hot for academic society. The corresponding dataset should be &lt;strong&gt;recent&lt;/strong&gt; and &lt;strong&gt;publicly available&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Results of state-of-the-art methods are not satisfactory, due to high amount of data/computation. &lt;strong&gt;The main concern about the dataset should be its huge size, not the difficulty of the task itself&lt;/strong&gt;. E.g., Naive-bayes has low accuracy and no one has been able to test SVM on it, because maybe no SVM package can handle that much data.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;del&gt;* More importantly, The dataset is preferably out of the computer-science community, i.e. few cs/ml researchers have tried to solve it. For example, NLP problems are too hard for contribution, because most NLP researchers are expert in ML &amp;amp; Algorithms, so it is extremely difficult for me to outperform their works! I think there should be easy-to-outperform datasets in bioinformatics, but I do not know which tasks are of the big-learning scheme. It would be grateful if you suggest datasets from other fields as well :)&lt;/del&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"permalink":"/r/MachineLearning/comments/1kqv5w/new_datasets_for_biglearning/","score":2,"domain":"self.MachineLearning","num_comments":4,"id":"1kqv5w","user_reports":[],"over_18":false,"secure_media_embed":{},"media_embed":{},"gilded":0}
{"created_utc":1377019841,"selftext":"","is_self":false,"link_flair_css_class":null,"media":null,"ups":17,"author_flair_css_class":null,"author_flair_text":null,"banned_by":null,"secure_media":null,"link_flair_text":null,"author":"_dexter","title":"Read the Web :: Carnegie Mellon University - NELL: Never-Ending Language Learning","subreddit_id":"t5_2r3gv","url":"http://rtw.ml.cmu.edu/rtw/index.php","subreddit":"MachineLearning","stickied":false,"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":1,"id":"1kquhw","user_reports":[],"domain":"rtw.ml.cmu.edu","score":17,"permalink":"/r/MachineLearning/comments/1kquhw/read_the_web_carnegie_mellon_university_nell/","selftext_html":null,"mod_reports":[],"retrieved_on":1411913306,"thumbnail":"http://e.thumbs.redditmedia.com/2P9UTIChJJzljcyd.jpg","edited":false,"downs":0,"distinguished":null,"report_reasons":null}
{"secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"stickied":false,"subreddit":"MachineLearning","author":"Jonny5ive","title":"Anybody have an example, in any industry/application, of an ML scheme for predictive maintenance? E.g., the machine learns to predict when equipment will fail.","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kqp1n/anybody_have_an_example_in_any/","is_self":true,"created_utc":1377015729,"selftext":"Is this just an idea or has someone worked it out?","author_flair_css_class":null,"ups":5,"media":null,"link_flair_css_class":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is this just an idea or has someone worked it out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411913524,"edited":false,"report_reasons":null,"distinguished":null,"downs":0,"thumbnail":"self","over_18":false,"num_comments":10,"id":"1kqp1n","user_reports":[],"media_embed":{},"gilded":0,"secure_media_embed":{},"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1kqp1n/anybody_have_an_example_in_any/","score":5}
{"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":0,"created_utc":1377013504,"selftext":"","is_self":false,"url":"http://www.singularityweblog.com/geordie-rose-d-wave-quantum-computing/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+SingularityBlog+%28Singularity+Weblog%29&amp;utm_content=Google+Feedfetcher","subreddit_id":"t5_2r3gv","title":"D-Wave CTO Geordie Rose: Machine Learning is Progressing Faster Than You Think","author":"Buck-Nasty","stickied":false,"subreddit":"MachineLearning","author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"permalink":"/r/MachineLearning/comments/1kqm50/dwave_cto_geordie_rose_machine_learning_is/","score":0,"domain":"singularityweblog.com","secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":5,"id":"1kqm50","user_reports":[],"over_18":false,"thumbnail":"default","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"retrieved_on":1411913635,"mod_reports":[],"selftext_html":null}
{"score":10,"permalink":"/r/MachineLearning/comments/1kq9ma/how_can_i_update_a_bayesian_network_model_given/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"id":"1kq9ma","num_comments":5,"user_reports":[],"over_18":false,"thumbnail":"self","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411914116,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are several methods for inferring network structure in Bayesian networks, given data.&lt;/p&gt;\n\n&lt;p&gt;In my case I have a Bayesian network model built from old data, and I have a new source of data that I want to use to update the model, both in terms of structure and parameters.  This new data source has a number of observations, much higher than the past data, making it ideal for structure inference.  The problem is that the new data covers only a subset of the variables in the original model.  Has anyone heard of a way to update BN model network structure (and parameters) given only a subset of the variables in the model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"media":null,"link_flair_css_class":null,"ups":10,"author_flair_css_class":null,"selftext":"There are several methods for inferring network structure in Bayesian networks, given data.\n\nIn my case I have a Bayesian network model built from old data, and I have a new source of data that I want to use to update the model, both in terms of structure and parameters.  This new data source has a number of observations, much higher than the past data, making it ideal for structure inference.  The problem is that the new data covers only a subset of the variables in the original model.  Has anyone heard of a way to update BN model network structure (and parameters) given only a subset of the variables in the model?","created_utc":1377001631,"is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/1kq9ma/how_can_i_update_a_bayesian_network_model_given/","subreddit_id":"t5_2r3gv","title":"How can I update a Bayesian network model given new data on only a subset of the variables in the original model?","author":"osazuwa","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null}
{"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1kq4ho/hot_problems_in_bigdata_with_openaccess_datasets/","score":0,"media_embed":{},"gilded":0,"secure_media_embed":{},"over_18":false,"num_comments":0,"user_reports":[],"id":"1kq4ho","thumbnail":"default","edited":false,"distinguished":null,"report_reasons":null,"downs":0,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am searching for datasts in big-data learning to work on as my PhD thesis. I seek for learning tasks that have the following conditions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The problem is new and is acceptable for academic society.&lt;/li&gt;\n&lt;li&gt;The dataset is too large for usual single-machine packages, so it need to be processed in parallel/distributed/streaming mode.&lt;/li&gt;\n&lt;li&gt;The dataset is recent and publicly available.&lt;/li&gt;\n&lt;li&gt;State-of-the-art results are not satisfactory (It takes too much time or have poor predictions).&lt;/li&gt;\n&lt;li&gt;More importantly, The dataset is preferably out of the computer-science community, i.e. few cs/ml researchers have tried to solve it. For example, NLP problems are too hard for contribution, because most NLP researchers are expert in ML &amp;amp; Algorithms, so it is extremely difficult for me to outperform their works! I think there should be easy-to-outperform datasets in bioinformatics, but I do not know which tasks are of the big-learning scheme. It would be grateful if you suggest datasets from other fields as well&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in Advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411914312,"media":null,"link_flair_css_class":null,"author_flair_css_class":null,"ups":0,"created_utc":1376993432,"selftext":"I am searching for datasts in big-data learning to work on as my PhD thesis. I seek for learning tasks that have the following conditions:\n\n* The problem is new and is acceptable for academic society.\n* The dataset is too large for usual single-machine packages, so it need to be processed in parallel/distributed/streaming mode.\n* The dataset is recent and publicly available.\n* State-of-the-art results are not satisfactory (It takes too much time or have poor predictions).\n* More importantly, The dataset is preferably out of the computer-science community, i.e. few cs/ml researchers have tried to solve it. For example, NLP problems are too hard for contribution, because most NLP researchers are expert in ML &amp; Algorithms, so it is extremely difficult for me to outperform their works! I think there should be easy-to-outperform datasets in bioinformatics, but I do not know which tasks are of the big-learning scheme. It would be grateful if you suggest datasets from other fields as well\n\n\nThanks in Advance!","is_self":true,"author":"[deleted]","title":"Hot problems in big-data with open-access datasets","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kq4ho/hot_problems_in_bigdata_with_openaccess_datasets/","stickied":false,"subreddit":"MachineLearning","banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null}
{"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1kq2ae/weighted_least_squares_for_nmf_on_concatenated/","score":6,"over_18":false,"num_comments":1,"user_reports":[],"id":"1kq2ae","media_embed":{},"gilded":0,"secure_media_embed":{},"edited":1377079938,"distinguished":null,"report_reasons":null,"downs":0,"thumbnail":"self","mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large set of images decomposed into 5 spatial frequency bands, with each band downsampled appropriately.\nSo for each image I have 64x64, 32x32, 16x16, 8x8 and 4x4 [5 spatial frequencies].&lt;/p&gt;\n\n&lt;p&gt;Now I want to do NMF on the combined image set, considering all spatial frequencies together. The first option would be to upsample all spatial frequencies to 64x64, but then I have ~20,000 features and I do not have a machine with enough RAM to do that (I have &amp;gt;200k items). Doing NMF on the 5456 downsampled vectors works, but obviously it&amp;#39;s biased very much towards to the high spatial frequency. &lt;/p&gt;\n\n&lt;p&gt;I saw NMF uses alternating least squares algorithm, so I thought if I could weight the components of the different spatial frequencies appropriately, this could correct the imbalance in number of pixels and result in equal overall weight to each SF component.&lt;/p&gt;\n\n&lt;p&gt;The key part of the ALS algorithm consists of the following lines:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;h = max(0, w0\\a);\nw = max(0, a/h);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So I modified this to:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;h = max(0, lscov(w0, a, weight));\nw = max(0, a/h);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I also updated the error term norm calcualtion to be a weighted Frobenius:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;d = bsxfun(@times, d.^2, weight);\ndnorm = sqrt(sum(sum(d))/nm);\n% dnorm = sqrt(sum(sum(d.^2))/nm);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Based on my first thought about how to weight to normalise the number of elements I thought I would weight each SF (high to low) by [1 4 16 64 256] respectively (because each lower SF has 4x less elements). But this produced solutions weighted far too much to the low SF. Instead weights of [1 2 4 8 16] give solutions that look great - but obviously I have to come to this in a completely ad hoc way so I am a bit worried if it is justifiable. &lt;/p&gt;\n\n&lt;p&gt;So the questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is my modification to the NMF ALS algorithm valid? I couldn&amp;#39;t find a way to add weights to the second iteration for w - because it is along the other axis so needs weights along the number of different images direction.&lt;/li&gt;\n&lt;li&gt;Why does 2x weights seem to be correct rather than 4x weights (is there a square somewhere that I am missing - I thought weights were applied like XWX so would not be squared)?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411914398,"author_flair_css_class":null,"ups":6,"link_flair_css_class":null,"media":null,"is_self":true,"selftext":"I have a large set of images decomposed into 5 spatial frequency bands, with each band downsampled appropriately.\nSo for each image I have 64x64, 32x32, 16x16, 8x8 and 4x4 [5 spatial frequencies].\n\nNow I want to do NMF on the combined image set, considering all spatial frequencies together. The first option would be to upsample all spatial frequencies to 64x64, but then I have ~20,000 features and I do not have a machine with enough RAM to do that (I have &gt;200k items). Doing NMF on the 5456 downsampled vectors works, but obviously it's biased very much towards to the high spatial frequency. \n\nI saw NMF uses alternating least squares algorithm, so I thought if I could weight the components of the different spatial frequencies appropriately, this could correct the imbalance in number of pixels and result in equal overall weight to each SF component.\n\nThe key part of the ALS algorithm consists of the following lines:\n\n    h = max(0, w0\\a);\n    w = max(0, a/h);\n\nSo I modified this to:\n\n    h = max(0, lscov(w0, a, weight));\n    w = max(0, a/h);\n\nI also updated the error term norm calcualtion to be a weighted Frobenius:\n\n    d = bsxfun(@times, d.^2, weight);\n    dnorm = sqrt(sum(sum(d))/nm);\n    % dnorm = sqrt(sum(sum(d.^2))/nm);\n\nBased on my first thought about how to weight to normalise the number of elements I thought I would weight each SF (high to low) by [1 4 16 64 256] respectively (because each lower SF has 4x less elements). But this produced solutions weighted far too much to the low SF. Instead weights of [1 2 4 8 16] give solutions that look great - but obviously I have to come to this in a completely ad hoc way so I am a bit worried if it is justifiable. \n\nSo the questions:\n\n - Is my modification to the NMF ALS algorithm valid? I couldn't find a way to add weights to the second iteration for w - because it is along the other axis so needs weights along the number of different images direction.\n - Why does 2x weights seem to be correct rather than 4x weights (is there a square somewhere that I am missing - I thought weights were applied like XWX so would not be squared)?","created_utc":1376989352,"stickied":false,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","title":"Weighted Least Squares for NMF on concatenated images of different sizes","author":"thrope","url":"http://www.reddit.com/r/MachineLearning/comments/1kq2ae/weighted_least_squares_for_nmf_on_concatenated/","secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null}
{"domain":"jeremykun.com","score":15,"permalink":"/r/MachineLearning/comments/1kq13n/linear_regression/","gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":0,"user_reports":[],"id":"1kq13n","thumbnail":"http://d.thumbs.redditmedia.com/TE7wLWpVD_0J3b2E.jpg","edited":false,"downs":0,"distinguished":null,"report_reasons":null,"selftext_html":null,"mod_reports":[],"retrieved_on":1411914444,"link_flair_css_class":null,"media":null,"ups":15,"author_flair_css_class":null,"selftext":"","created_utc":1376987135,"is_self":false,"subreddit_id":"t5_2r3gv","title":"Linear Regression","author":"zaega","url":"http://jeremykun.com/2013/08/18/linear-regression/","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null}
{"num_comments":1,"user_reports":[],"id":"1kpkz6","over_18":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"permalink":"/r/MachineLearning/comments/1kpkz6/any_video_lecture_on_structured_svm/","score":7,"domain":"self.MachineLearning","retrieved_on":1411915084,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The one on videolectures.net by Thornsten Joachim is hard to understand because illegible slides.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","distinguished":null,"report_reasons":null,"downs":0,"edited":false,"thumbnail":"self","is_self":true,"created_utc":1376968171,"selftext":"The one on videolectures.net by Thornsten Joachim is hard to understand because illegible slides.","author_flair_css_class":null,"ups":7,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"author_flair_text":null,"banned_by":null,"stickied":false,"subreddit":"MachineLearning","url":"http://www.reddit.com/r/MachineLearning/comments/1kpkz6/any_video_lecture_on_structured_svm/","title":"Any video lecture on Structured SVM ?","subreddit_id":"t5_2r3gv","author":"bakarr"}
{"edited":false,"downs":0,"distinguished":null,"report_reasons":null,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;I was wondering if you could give me some advice on negotiating a salary for a senior machine learning/ software engineer in San Francisco. What&amp;#39;s a good starting point for someone with a PhD and 1-2 years of experience? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m switching from academia (postdoc) to industry. As a postdoc I&amp;#39;m getting about $50k.  &lt;/p&gt;\n\n&lt;p&gt;Let me know if I should post this in &lt;a href=\"/r/datascience\"&gt;/r/datascience&lt;/a&gt; or &lt;a href=\"/r/bigdata\"&gt;/r/bigdata&lt;/a&gt; instead. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411915330,"domain":"self.MachineLearning","score":13,"permalink":"/r/MachineLearning/comments/1kpevm/senior_mlsoftware_engineer_salary_with_phd_12/","over_18":false,"num_comments":32,"id":"1kpevm","user_reports":[],"gilded":0,"media_embed":{},"secure_media_embed":{},"subreddit":"MachineLearning","stickied":false,"subreddit_id":"t5_2r3gv","title":"Senior ML/software engineer salary with PhD + 1-2 years experience?","author":"brownck","url":"http://www.reddit.com/r/MachineLearning/comments/1kpevm/senior_mlsoftware_engineer_salary_with_phd_12/","secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null,"ups":13,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"is_self":true,"created_utc":1376963161,"selftext":"Hi All, \n\nI was wondering if you could give me some advice on negotiating a salary for a senior machine learning/ software engineer in San Francisco. What's a good starting point for someone with a PhD and 1-2 years of experience? \n\nI'm switching from academia (postdoc) to industry. As a postdoc I'm getting about $50k.  \n\nLet me know if I should post this in /r/datascience or /r/bigdata instead. \n\nThanks!"}
{"thumbnail":"self","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411909350,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suppose I want to predict the profit of a company given a number of inputs (e.g. number of employees, industry and location of headquarters). &lt;/p&gt;\n\n&lt;p&gt;I know that profit = revenue - expenses.&lt;/p&gt;\n\n&lt;p&gt;Would it be possible to force the learning algorithm to predict the revenue feature, and the expenses feature such that &amp;quot;revenue - expenses&amp;quot; predicts the profit of the company?&lt;/p&gt;\n\n&lt;p&gt;Please note:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I only know the profit for each labelled company, I don&amp;#39;t know their revenue or expenses.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I do not want to predict the revenue and expenses of each company, I only want to predict the profit. In other words, I do not care about the accuracy of the revenue and expenses, I only care about the profit.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The problem explained in this message is strictly for illustrative purposes.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;More generally I want:\nGiven an input vector X, I want to find the vector Y such that f(Y) predicts what I want to predict. The function f is designed by a human.&lt;/p&gt;\n\n&lt;p&gt;Is this doable? Has this been studied?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"score":0,"permalink":"/r/MachineLearning/comments/1kthtm/assisted_supervised_learning/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"num_comments":2,"user_reports":[],"id":"1kthtm","over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1kthtm/assisted_supervised_learning/","author":"TheCatelier","title":"Assisted supervised learning?","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"media":null,"link_flair_css_class":null,"ups":0,"author_flair_css_class":null,"selftext":"Suppose I want to predict the profit of a company given a number of inputs (e.g. number of employees, industry and location of headquarters). \n\nI know that profit = revenue - expenses.\n\nWould it be possible to force the learning algorithm to predict the revenue feature, and the expenses feature such that \"revenue - expenses\" predicts the profit of the company?\n\nPlease note:\n\n1. I only know the profit for each labelled company, I don't know their revenue or expenses.\n\n2. I do not want to predict the revenue and expenses of each company, I only want to predict the profit. In other words, I do not care about the accuracy of the revenue and expenses, I only care about the profit.\n \n3. The problem explained in this message is strictly for illustrative purposes.\n\nMore generally I want:\nGiven an input vector X, I want to find the vector Y such that f(Y) predicts what I want to predict. The function f is designed by a human.\n\nIs this doable? Has this been studied?\n","created_utc":1377108878,"is_self":true}
{"title":"¿Software for causal inference?","author":"jamesmcm","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kt310/software_for_causal_inference/","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"link_flair_css_class":null,"media":null,"ups":4,"author_flair_css_class":null,"created_utc":1377097675,"selftext":"Does anyone know of software for causal inference?\n\nThe R package _pcalg_ seemed like the best bet, but various changes to R have made it (and its dependencies) a nightmare to install (seems like it isn't possible on the most recent version of R) :/\n\nSo I was wondering what other alternatives there are? Something in Python would be ideal.","is_self":true,"thumbnail":"self","edited":false,"downs":0,"distinguished":null,"report_reasons":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of software for causal inference?&lt;/p&gt;\n\n&lt;p&gt;The R package &lt;em&gt;pcalg&lt;/em&gt; seemed like the best bet, but various changes to R have made it (and its dependencies) a nightmare to install (seems like it isn&amp;#39;t possible on the most recent version of R) :/&lt;/p&gt;\n\n&lt;p&gt;So I was wondering what other alternatives there are? Something in Python would be ideal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411909979,"domain":"self.MachineLearning","score":4,"permalink":"/r/MachineLearning/comments/1kt310/software_for_causal_inference/","gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":4,"id":"1kt310","user_reports":[]}
{"edited":1377204510,"report_reasons":null,"distinguished":null,"downs":0,"thumbnail":"self","mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any specific languages I should learn, books to read? Anything is appreciated. I already do tons of ML at my job daily (most statisticians don&amp;#39;t think of it as ML stuff though), but can&amp;#39;t help feel there is something I am missing that would qualify me as data scientist.&lt;/p&gt;\n\n&lt;p&gt;I should mention, I am also pretty good at Octave/Matlab and am ok at Python (would be better but I never get a chance to use it). My masters thesis was on non-Gaussian time series and multivariate predictive methods, I also took a few courses in Bayesian econometrics/analysis.&lt;/p&gt;\n\n&lt;p&gt;edit: Thanks a ton for the advice so far, the consensus seems to be start learning how to deal with big data using Hadoop or some other similar app and learn some software engineering, however that part seems a little ambiguous still.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411905645,"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1kvxm3/i_have_an_ms_in_statistics_am_a_good_rsassql/","score":26,"over_18":false,"num_comments":42,"user_reports":[],"id":"1kvxm3","media_embed":{},"gilded":0,"secure_media_embed":{},"stickied":false,"subreddit":"MachineLearning","title":"I have an M.S. in statistics, am a good R/SAS/SQL programmer, I have taken the coursera ML course, and am currently working as principal statistician for an energy company. What else should I do to become qualified for 'Data Scientist' positions?","author":"bstockton","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kvxm3/i_have_an_ms_in_statistics_am_a_good_rsassql/","secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"author_flair_css_class":null,"ups":26,"link_flair_css_class":null,"media":null,"is_self":true,"created_utc":1377192945,"selftext":"Any specific languages I should learn, books to read? Anything is appreciated. I already do tons of ML at my job daily (most statisticians don't think of it as ML stuff though), but can't help feel there is something I am missing that would qualify me as data scientist.\n\nI should mention, I am also pretty good at Octave/Matlab and am ok at Python (would be better but I never get a chance to use it). My masters thesis was on non-Gaussian time series and multivariate predictive methods, I also took a few courses in Bayesian econometrics/analysis.\n\n\nedit: Thanks a ton for the advice so far, the consensus seems to be start learning how to deal with big data using Hadoop or some other similar app and learn some software engineering, however that part seems a little ambiguous still."}
{"score":8,"permalink":"/r/MachineLearning/comments/1kvdss/shortcircuit_predictors/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"num_comments":9,"user_reports":[],"id":"1kvdss","over_18":false,"thumbnail":"self","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411906487,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On &lt;a href=\"/r/science\"&gt;/r/science&lt;/a&gt; there&amp;#39;s a link to a physorg article on successfully predicting movie revenues by analyzing wikipedia posts: &lt;a href=\"http://phys.org/news/2013-08-math-movies-office.html\"&gt;Link to phys.org&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Maybe someone can conjure the actual actual math paper, but there&amp;#39;s an interesting twist apparent in the newsy article about it:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The predicting power of the Wikipedia-based model, despite its simplicity compared with Twitter, is that many of the editors of the Wikipedia pages about the movies are committed movie-goers who gather and edit relevant material well before the release date. By contrast, the &amp;quot;mass&amp;quot; production of tweets occurs very close to the release time, and often these can be spun by marketing agencies rather than reflecting the feelings of the public.&amp;#39;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;If this predictor were adopted, there&amp;#39;s an obvious short-circuit that can render it useless. Producers can spoof the signal by unleashing hoards of wiki editors to amp up the projections of revenue and thereby make it easier to raise money for the film. This is close to the idea of &lt;a href=\"http://alife.co.uk/essays/the_wirehead_problem/\"&gt;&amp;#39;wireheading&amp;#39;&lt;/a&gt; in AI. &lt;/p&gt;\n\n&lt;p&gt;I have thought and &lt;a href=\"http://ieet.org/index.php/IEET/more/eubanks20120611\"&gt;written&lt;/a&gt; about this problem for a while, and wonder if there are examples from ML where self-reference interferes with models. More interestingly: what does an intelligence machine/organization do to prevent this kind of short circuit?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"media":null,"link_flair_css_class":null,"ups":8,"author_flair_css_class":null,"created_utc":1377176760,"selftext":"On /r/science there's a link to a physorg article on successfully predicting movie revenues by analyzing wikipedia posts: [Link to phys.org](http://phys.org/news/2013-08-math-movies-office.html). \n\nMaybe someone can conjure the actual actual math paper, but there's an interesting twist apparent in the newsy article about it:\n\n&gt;The predicting power of the Wikipedia-based model, despite its simplicity compared with Twitter, is that many of the editors of the Wikipedia pages about the movies are committed movie-goers who gather and edit relevant material well before the release date. By contrast, the \"mass\" production of tweets occurs very close to the release time, and often these can be spun by marketing agencies rather than reflecting the feelings of the public.'\n\nIf this predictor were adopted, there's an obvious short-circuit that can render it useless. Producers can spoof the signal by unleashing hoards of wiki editors to amp up the projections of revenue and thereby make it easier to raise money for the film. This is close to the idea of ['wireheading'](http://alife.co.uk/essays/the_wirehead_problem/) in AI. \n\nI have thought and [written](http://ieet.org/index.php/IEET/more/eubanks20120611) about this problem for a while, and wonder if there are examples from ML where self-reference interferes with models. More interestingly: what does an intelligence machine/organization do to prevent this kind of short circuit?","is_self":true,"url":"http://www.reddit.com/r/MachineLearning/comments/1kvdss/shortcircuit_predictors/","subreddit_id":"t5_2r3gv","title":"Short-Circuit predictors","author":"szza","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null}
{"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":19,"selftext":"","created_utc":1377139421,"is_self":false,"url":"http://blog.yhathq.com/posts/estimating-user-lifetimes-with-pymc.html","author":"hernamesbarbara","title":"Estimating User Lifetimes w/ PyMC, a python package for Bayesian analysis","subreddit_id":"t5_2r3gv","stickied":false,"subreddit":"MachineLearning","banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"permalink":"/r/MachineLearning/comments/1kukke/estimating_user_lifetimes_w_pymc_a_python_package/","score":19,"domain":"blog.yhathq.com","secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":0,"user_reports":[],"id":"1kukke","over_18":false,"thumbnail":"default","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"retrieved_on":1411907710,"mod_reports":[],"selftext_html":null}
{"thumbnail":"self","edited":false,"downs":0,"report_reasons":null,"distinguished":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In Literature it is often stated that SVM need no Feature Selection i.e. Joachims 1998 &lt;a href=\"http://www.cs.iastate.edu/%7Ejtian/cs573/Papers/Joachims-ECML-98.pdf\"&gt;http://www.cs.iastate.edu/~jtian/cs573/Papers/Joachims-ECML-98.pdf&lt;/a&gt;. On the other hand feature selection is still studied 5 years after it i.e. &lt;a href=\"http://www.machine-learning.martinsewell.com/feature-selection/Forman2003.pdf\"&gt;http://www.machine-learning.martinsewell.com/feature-selection/Forman2003.pdf&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;So if I apply SVM why should I still use feature selection? Shouldn&amp;#39;t the cost parameter &amp;quot;reduce&amp;quot; the feature set much better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411901737,"domain":"self.MachineLearning","score":6,"permalink":"/r/MachineLearning/comments/1kyplr/if_i_use_svm_is_there_a_situation_in_which/","gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":11,"user_reports":[],"id":"1kyplr","title":"If I use SVM, is there a situation in which feature selection is better than using the Cost-Parameter?","author":"ComplexIt","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1kyplr/if_i_use_svm_is_there_a_situation_in_which/","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"secure_media":null,"link_flair_text":null,"link_flair_css_class":null,"media":null,"ups":6,"author_flair_css_class":null,"selftext":"In Literature it is often stated that SVM need no Feature Selection i.e. Joachims 1998 http://www.cs.iastate.edu/~jtian/cs573/Papers/Joachims-ECML-98.pdf. On the other hand feature selection is still studied 5 years after it i.e. http://www.machine-learning.martinsewell.com/feature-selection/Forman2003.pdf. \n\nSo if I apply SVM why should I still use feature selection? Shouldn't the cost parameter \"reduce\" the feature set much better?","created_utc":1377289219,"is_self":true}
{"media":null,"link_flair_css_class":null,"ups":8,"author_flair_css_class":null,"selftext":"So I'm playing around with sparse autoencoders, and I'm trying to train a simple example with conjugate gradient descent.  I just witnessed some behavior I can't explain and I'm hoping someone here can help me understand what's going on.\n\n\n\nThe neural network I'm training is small, and meant to solve the XOR problem.  It has two inputs plus a bias on the input layer, two hidden units (plus a bias), and a single output.  This creates 3*2 + 3 = 9 total weights to be trained.  I have confidence that my gradient calculations are correct, because they pass the gradient estimation check described [here](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization), and are used to generate edge detectors for natural images with the backpropagation algorithm as described [here](http://ufldl.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder).  It should be a short couple of steps to train this network to solve XOR with conjugate gradient descent using my already-coded gradient calculation plus an erf() function that calculates overall network error.  I'm using the Polak-Ribiere method to generate the Beta coefficient.    My erf() function is more of less exactly as described at the UFLDL site.\n\n\n\nFinally, the problem:  My CGD algorithm seems to be sensitive to the magnitude of the weights that I initialize the network with.  When I initialize the weights with uniform random numbers in the range of [-0.1 0.1], the algorithm reliably converges on a bad local minima (all inputs result in an output of 0.5).  If I hange the weight initialization to uniform random numbers of the range [-0.3 0.3], then the network converges to a state that solves XOR.  \n\n\n\nWhat's the principle at work here?  Is this kind of weight sensitivity something specific to CGD?  \n\n\n\n\nThanks!","created_utc":1377364032,"is_self":true,"title":"Question about the behavior of conjugate gradient descent optimization","subreddit_id":"t5_2r3gv","author":"eubarch","url":"http://www.reddit.com/r/MachineLearning/comments/1l0dkv/question_about_the_behavior_of_conjugate_gradient/","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"domain":"self.MachineLearning","score":8,"permalink":"/r/MachineLearning/comments/1l0dkv/question_about_the_behavior_of_conjugate_gradient/","gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"user_reports":[],"num_comments":14,"id":"1l0dkv","thumbnail":"self","edited":false,"downs":0,"distinguished":null,"report_reasons":null,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m playing around with sparse autoencoders, and I&amp;#39;m trying to train a simple example with conjugate gradient descent.  I just witnessed some behavior I can&amp;#39;t explain and I&amp;#39;m hoping someone here can help me understand what&amp;#39;s going on.&lt;/p&gt;\n\n&lt;p&gt;The neural network I&amp;#39;m training is small, and meant to solve the XOR problem.  It has two inputs plus a bias on the input layer, two hidden units (plus a bias), and a single output.  This creates 3*2 + 3 = 9 total weights to be trained.  I have confidence that my gradient calculations are correct, because they pass the gradient estimation check described &lt;a href=\"http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\"&gt;here&lt;/a&gt;, and are used to generate edge detectors for natural images with the backpropagation algorithm as described &lt;a href=\"http://ufldl.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder\"&gt;here&lt;/a&gt;.  It should be a short couple of steps to train this network to solve XOR with conjugate gradient descent using my already-coded gradient calculation plus an erf() function that calculates overall network error.  I&amp;#39;m using the Polak-Ribiere method to generate the Beta coefficient.    My erf() function is more of less exactly as described at the UFLDL site.&lt;/p&gt;\n\n&lt;p&gt;Finally, the problem:  My CGD algorithm seems to be sensitive to the magnitude of the weights that I initialize the network with.  When I initialize the weights with uniform random numbers in the range of [-0.1 0.1], the algorithm reliably converges on a bad local minima (all inputs result in an output of 0.5).  If I hange the weight initialization to uniform random numbers of the range [-0.3 0.3], then the network converges to a state that solves XOR.  &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the principle at work here?  Is this kind of weight sensitivity something specific to CGD?  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411899480}
{"thumbnail":"self","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411900439,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I took the NLP Course in Coursera and sometimes they referred GLM as CRF. I have been doing some reading and I&amp;#39;m not really sure if they are the same or one is a generalization of the other one, and if the method for parameter estimation, objective function, etc, is a little bit different or not.&lt;/p&gt;\n\n&lt;p&gt;If they are in fact different. Could you elaborate on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":1,"permalink":"/r/MachineLearning/comments/1kznxl/are_conditional_random_fields_the_same_thing_as/","domain":"self.MachineLearning","secure_media_embed":{},"gilded":0,"media_embed":{},"user_reports":[],"num_comments":4,"id":"1kznxl","over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1kznxl/are_conditional_random_fields_the_same_thing_as/","title":"Are Conditional Random Fields the same thing as Global Linear Models","author":"blackhattrick","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"media":null,"link_flair_css_class":null,"ups":1,"author_flair_css_class":null,"created_utc":1377322976,"selftext":"I took the NLP Course in Coursera and sometimes they referred GLM as CRF. I have been doing some reading and I'm not really sure if they are the same or one is a generalization of the other one, and if the method for parameter estimation, objective function, etc, is a little bit different or not.\n\nIf they are in fact different. Could you elaborate on this?\n","is_self":true}
{"url":"http://engineering.richrelevance.com/bayesian-ab-testing-with-a-log-normal-model/","author":"sergeyfeldman","title":"Bayesian A/B Tests with Log-normal Models (w/ Python code)","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"media":null,"link_flair_css_class":null,"ups":20,"author_flair_css_class":null,"selftext":"","created_utc":1377552087,"is_self":false,"thumbnail":"http://b.thumbs.redditmedia.com/6jVSF7vM2xHrHt9g.jpg","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411892485,"selftext_html":null,"mod_reports":[],"score":20,"permalink":"/r/MachineLearning/comments/1l55kh/bayesian_ab_tests_with_lognormal_models_w_python/","domain":"engineering.richrelevance.com","secure_media_embed":{},"gilded":0,"media_embed":{},"user_reports":[],"num_comments":0,"id":"1l55kh","over_18":false}
{"over_18":false,"num_comments":7,"id":"1l4hw6","user_reports":[],"media_embed":{},"gilded":0,"secure_media_embed":{},"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1l4hw6/classification_data_set_with_timeseries_revelation/","score":7,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a project and need a data set to stand in for the &amp;quot;real&amp;quot; problem. &lt;/p&gt;\n\n&lt;p&gt;The data should look like:&lt;br/&gt;\n1) Each instance should be one classification. Meaning the label is simply one yes/no(multi-class is fine too, with a small n) over the entire series &lt;/p&gt;\n\n&lt;p&gt;2) It should have a time-series aspect to it, so trained models can continually update their prediction on the same decision question. &lt;/p&gt;\n\n&lt;p&gt;Or could this just be faked?   &lt;/p&gt;\n\n&lt;p&gt;3) There should be a way to have two different &amp;quot;views&amp;quot; of the data. Most likely this could just be something really simple, like being able to with-holding certain features, train one model, and with-hold another set, and train another. Even better would be data from two different sources, like two different sensors or views.  &lt;/p&gt;\n\n&lt;p&gt;I looked around a bit but am having trouble finding data sets that hold to all of these qualities. I was thinking something like cancer data with time-series bio samples or something?  Fraud detection?&lt;/p&gt;\n\n&lt;p&gt;Any help/suggestions are appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411893462,"edited":1377536066,"report_reasons":null,"distinguished":null,"downs":0,"thumbnail":"self","is_self":true,"created_utc":1377534267,"selftext":"I'm working on a project and need a data set to stand in for the \"real\" problem. \n\nThe data should look like:  \n1) Each instance should be one classification. Meaning the label is simply one yes/no(multi-class is fine too, with a small n) over the entire series \n \n2) It should have a time-series aspect to it, so trained models can continually update their prediction on the same decision question. \n\nOr could this just be faked?   \n\n3) There should be a way to have two different \"views\" of the data. Most likely this could just be something really simple, like being able to with-holding certain features, train one model, and with-hold another set, and train another. Even better would be data from two different sources, like two different sensors or views.  \n\nI looked around a bit but am having trouble finding data sets that hold to all of these qualities. I was thinking something like cancer data with time-series bio samples or something?  Fraud detection?\n\nAny help/suggestions are appreciated.","author_flair_css_class":null,"ups":7,"media":null,"link_flair_css_class":null,"secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null,"stickied":false,"subreddit":"MachineLearning","author":"GibbsSamplePlatter","title":"Classification Data Set with time-series revelation","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1l4hw6/classification_data_set_with_timeseries_revelation/"}
{"secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":8,"user_reports":[],"id":"1l4be6","over_18":false,"permalink":"/r/MachineLearning/comments/1l4be6/machine_learning_and_pcap_files/","score":8,"domain":"self.MachineLearning","retrieved_on":1411893735,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking for resources on applying ML to PCAPs, but I&amp;#39;m having a devil of a time.  I am interested in finding outliers as well as correlating the various devices.  I&amp;#39;ve rigged up a few statistical tests to identify unusual traffic and attempt to pair devices to owners, but I believe it could be done better with ML as I have time to analyze the data after capture.  The trouble is, I can&amp;#39;t seem to find any good papers or advice for applying ML to PCAPs.  Are there any good resources you would recommend for analyzing PCAP/network data with ML techniques? Any advice from personal experience?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"thumbnail":"self","report_reasons":null,"distinguished":null,"downs":0,"edited":false,"created_utc":1377528859,"selftext":"Hello,\n\nI've been looking for resources on applying ML to PCAPs, but I'm having a devil of a time.  I am interested in finding outliers as well as correlating the various devices.  I've rigged up a few statistical tests to identify unusual traffic and attempt to pair devices to owners, but I believe it could be done better with ML as I have time to analyze the data after capture.  The trouble is, I can't seem to find any good papers or advice for applying ML to PCAPs.  Are there any good resources you would recommend for analyzing PCAP/network data with ML techniques? Any advice from personal experience?","is_self":true,"media":null,"link_flair_css_class":null,"author_flair_css_class":null,"ups":8,"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1l4be6/machine_learning_and_pcap_files/","title":"Machine Learning and PCAP Files","subreddit_id":"t5_2r3gv","author":"PyFun","stickied":false,"subreddit":"MachineLearning"}
{"domain":"fastml.com","score":12,"permalink":"/r/MachineLearning/comments/1l3z6j/a_bag_of_words_and_a_nice_little_network/","over_18":false,"num_comments":1,"id":"1l3z6j","user_reports":[],"gilded":0,"media_embed":{},"secure_media_embed":{},"edited":false,"downs":0,"report_reasons":null,"distinguished":null,"thumbnail":"default","selftext_html":null,"mod_reports":[],"retrieved_on":1411894235,"ups":12,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"is_self":false,"created_utc":1377514037,"selftext":"","subreddit":"MachineLearning","stickied":false,"subreddit_id":"t5_2r3gv","title":"A bag of words and a nice little network","author":"Foxtr0t","url":"http://fastml.com/a-bag-of-words-and-a-nice-little-network/","secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null}
{"ups":3,"author_flair_css_class":null,"media":null,"link_flair_css_class":null,"is_self":true,"created_utc":1377622533,"selftext":"I'm trying to use the ClassificationKNN class in matlab with DTW distance. I'm passing the DTW function as a custom function handle. The problem is that matlab is expecting the input X (feature vectors) to be a matrix, which I cannot put in because the input vectors are of different lengths. It is not taking in a cell array as an input. I'm stumped here and hoping for some help from you guys.\n\nCheers\n","subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1l74ff/knn_with_dtw_in_matlab_xpost_rmatlab/","title":"KNN with DTW in matlab (x-post /r/matlab)","author":"rorschach122","subreddit_id":"t5_2r3gv","link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null,"score":3,"permalink":"/r/MachineLearning/comments/1l74ff/knn_with_dtw_in_matlab_xpost_rmatlab/","domain":"self.MachineLearning","num_comments":0,"user_reports":[],"id":"1l74ff","over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"downs":0,"distinguished":null,"report_reasons":null,"edited":false,"thumbnail":"self","retrieved_on":1411889564,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to use the ClassificationKNN class in matlab with DTW distance. I&amp;#39;m passing the DTW function as a custom function handle. The problem is that matlab is expecting the input X (feature vectors) to be a matrix, which I cannot put in because the input vectors are of different lengths. It is not taking in a cell array as an input. I&amp;#39;m stumped here and hoping for some help from you guys.&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[]}
{"thumbnail":"http://a.thumbs.redditmedia.com/1mMBNKXX4onRqtxx.jpg","distinguished":null,"report_reasons":null,"downs":0,"edited":false,"retrieved_on":1411889573,"mod_reports":[],"selftext_html":null,"permalink":"/r/MachineLearning/comments/1l748w/machine_learning_talks_a_youtube_channel_with/","score":34,"domain":"youtube.com","secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":4,"user_reports":[],"id":"1l748w","over_18":false,"url":"http://www.youtube.com/channel/UCHhbDEKA7BP58mq1wfTBQNQ/videos","subreddit_id":"t5_2r3gv","title":"Machine Learning Talks: a YouTube channel with... machine learning talks.","author":"skytomorrownow","stickied":false,"subreddit":"MachineLearning","banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":34,"created_utc":1377622405,"selftext":"","is_self":false}
{"link_flair_css_class":null,"media":{"oembed":{"author_name":"ML talks","provider_url":"http://www.youtube.com/","thumbnail_url":"http://i1.ytimg.com/vi/HZQOvm0fkLA/hqdefault.jpg","type":"video","description":"Machine Learning Summer School 2012: Lecture 3: Graph based semi-supervised learning (Part 1) - Zoubin Ghahramani (University of Cambridge) http://mlss2012.tsc.uc3m.es/","width":600,"height":450,"version":"1.0","thumbnail_height":360,"author_url":"http://www.youtube.com/channel/UCHhbDEKA7BP58mq1wfTBQNQ","provider_name":"YouTube","thumbnail_width":480,"url":"http://www.youtube.com/watch?v=HZQOvm0fkLA","title":"MLSS 2012: Z. Ghahramani - Lecture 3: Graph based semi-supervised learning (Part 1)","html":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/HZQOvm0fkLA?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"type":"youtube.com"},"ups":11,"author_flair_css_class":null,"created_utc":1377619832,"selftext":"","is_self":false,"url":"http://www.youtube.com/watch?v=HZQOvm0fkLA","subreddit_id":"t5_2r3gv","title":"Z. Ghahramani: Graph-based semi-supervised learning","author":"skytomorrownow","subreddit":"MachineLearning","stickied":false,"author_flair_text":null,"banned_by":null,"link_flair_text":null,"secure_media":null,"score":11,"permalink":"/r/MachineLearning/comments/1l70wp/z_ghahramani_graphbased_semisupervised_learning/","domain":"youtube.com","secure_media_embed":{},"gilded":0,"media_embed":{"scrolling":false,"content":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/HZQOvm0fkLA?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":450,"width":600},"num_comments":1,"id":"1l70wp","user_reports":[],"over_18":false,"thumbnail":"http://e.thumbs.redditmedia.com/GW9l_mEdOtBPuicv.jpg","downs":0,"distinguished":null,"report_reasons":null,"edited":false,"retrieved_on":1411889713,"selftext_html":null,"mod_reports":[]}
{"ups":92,"author_flair_css_class":null,"media":null,"link_flair_css_class":null,"is_self":false,"created_utc":1377601830,"selftext":"","subreddit":"MachineLearning","stickied":false,"subreddit_id":"t5_2r3gv","title":"The authors of a machine learning research paper have been awarded a prize for their work - ten years after the initial paper had been rejected.","author":"_dexter","url":"http://www.eng.cam.ac.uk/news/stories/2013/10_year_award/","secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"domain":"eng.cam.ac.uk","score":92,"permalink":"/r/MachineLearning/comments/1l6iir/the_authors_of_a_machine_learning_research_paper/","over_18":false,"num_comments":10,"id":"1l6iir","user_reports":[],"gilded":0,"media_embed":{},"secure_media_embed":{},"edited":false,"downs":0,"report_reasons":null,"distinguished":null,"thumbnail":"http://a.thumbs.redditmedia.com/Di2vySGBqsB2p51l.jpg","selftext_html":null,"mod_reports":[],"retrieved_on":1411890441}
{"link_flair_css_class":null,"media":null,"ups":18,"author_flair_css_class":null,"selftext":"","created_utc":1377731581,"is_self":false,"url":"http://dlib.net/ml.html#structural_svm_problem","subreddit_id":"t5_2r3gv","title":"Easy to use structural SVM solver with C++ and Python tutorials/examples","author":"davis685","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"score":18,"permalink":"/r/MachineLearning/comments/1lakce/easy_to_use_structural_svm_solver_with_c_and/","domain":"dlib.net","secure_media_embed":{},"gilded":0,"media_embed":{},"user_reports":[],"num_comments":4,"id":"1lakce","over_18":false,"thumbnail":"http://a.thumbs.redditmedia.com/M9zcuQQiNaFknKyr.jpg","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"retrieved_on":1411884336,"mod_reports":[],"selftext_html":null}
{"media":null,"link_flair_css_class":null,"ups":6,"author_flair_css_class":null,"created_utc":1377719632,"selftext":"","is_self":false,"subreddit_id":"t5_2r3gv","title":"To Be or Not To Be IID by William M. Pottenger (Higher Order Naive Bayes Explained)","author":"Rickasaurus","url":"http://g33ktalk.com/intuidex-to-be-or-not-to-be-iid-by-william-m-pottenger/","subreddit":"MachineLearning","stickied":false,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"domain":"g33ktalk.com","score":6,"permalink":"/r/MachineLearning/comments/1la4nu/to_be_or_not_to_be_iid_by_william_m_pottenger/","gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":2,"user_reports":[],"id":"1la4nu","thumbnail":"http://b.thumbs.redditmedia.com/Ddf-EeH_bM7d4ium.jpg","edited":false,"downs":0,"report_reasons":null,"distinguished":null,"mod_reports":[],"selftext_html":null,"retrieved_on":1411885015}
{"gilded":0,"media_embed":{},"secure_media_embed":{},"over_18":false,"num_comments":17,"id":"1la3cf","user_reports":[],"domain":"self.MachineLearning","score":7,"permalink":"/r/MachineLearning/comments/1la3cf/what_does_it_mean_exactly_to_build_a_statistical/","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So this is a straight forward question, I would like to know, what it means exactly, when someone says &amp;quot;We built a statistical model of all our images&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I overheard this, (and keep overhearing that phrase), but I am not sure how/what that entails exactly.&lt;/p&gt;\n\n&lt;p&gt;What does it mean, for someone to &amp;#39;build a statistical model&amp;#39;?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;EDIT: I am very well aware of regression, PCA, EM algorithm, GMMs, etc etc. When I hear &amp;quot;statistical model&amp;quot;, I think &amp;quot;Ok, he has a PDF&amp;quot;. This is what I am trying to confirm/deny. Does &amp;quot;building a statistical model&amp;quot; mean &amp;quot;I came up with a new PDF&amp;quot;, or does it mean &amp;quot;We figured out &lt;em&gt;parameters&lt;/em&gt; of a given PDF&amp;quot;, or what? This is what I am trying to determine. Literally, you have a set of images, you state, &amp;quot;I built a statistical model&amp;quot;... &lt;strong&gt;what does that mean exactly&lt;/strong&gt;? What do you have now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"retrieved_on":1411885072,"thumbnail":"self","edited":1377719986,"downs":0,"distinguished":null,"report_reasons":null,"created_utc":1377718724,"selftext":"\nSo this is a straight forward question, I would like to know, what it means exactly, when someone says \"We built a statistical model of all our images\".\n\nI overheard this, (and keep overhearing that phrase), but I am not sure how/what that entails exactly.\n\nWhat does it mean, for someone to 'build a statistical model'?\n\nThanks\n\nEDIT: I am very well aware of regression, PCA, EM algorithm, GMMs, etc etc. When I hear \"statistical model\", I think \"Ok, he has a PDF\". This is what I am trying to confirm/deny. Does \"building a statistical model\" mean \"I came up with a new PDF\", or does it mean \"We figured out *parameters* of a given PDF\", or what? This is what I am trying to determine. Literally, you have a set of images, you state, \"I built a statistical model\"... **what does that mean exactly**? What do you have now?","is_self":true,"media":null,"link_flair_css_class":null,"ups":7,"author_flair_css_class":null,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"author":"Ayakalam","title":"What does it mean exactly, to “build a statistical model” of, say, a series of images?","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1la3cf/what_does_it_mean_exactly_to_build_a_statistical/","subreddit":"MachineLearning","stickied":false}
{"over_18":false,"num_comments":1,"id":"1lbtwa","user_reports":[],"gilded":0,"media_embed":{"width":600,"content":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/36IT9VgGr0g?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":450,"scrolling":false},"secure_media_embed":{},"domain":"youtube.com","score":20,"permalink":"/r/MachineLearning/comments/1lbtwa/defending_networks_with_incomplete_information_a/","mod_reports":[],"selftext_html":null,"retrieved_on":1411882385,"edited":false,"downs":0,"report_reasons":null,"distinguished":null,"thumbnail":"http://f.thumbs.redditmedia.com/UkTfILlAW2iNVz7k.jpg","is_self":false,"selftext":"","created_utc":1377782187,"ups":20,"author_flair_css_class":null,"link_flair_css_class":null,"media":{"oembed":{"type":"video","description":"Let's face it: we may win some battles, but we are losing the war pretty badly. Regardless of the advances in malware and targeted attacks detection technologies, our top security practitioners can only do so much in a 24 hour day. Even less, if you let them eat and sleep.","thumbnail_url":"http://i1.ytimg.com/vi/36IT9VgGr0g/hqdefault.jpg","provider_url":"http://www.youtube.com/","author_name":"Alexandre Pinto","author_url":"http://www.youtube.com/channel/UCPCgMNnbWm5udC1b6Fd-E5g","thumbnail_height":360,"version":"1.0","height":450,"width":600,"thumbnail_width":480,"provider_name":"YouTube","html":"&lt;iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/36IT9VgGr0g?feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","title":"DEFCON 21: Defending Networks with Incomplete Information - A Machine Learning Approach","url":"http://www.youtube.com/watch?v=36IT9VgGr0g"},"type":"youtube.com"},"secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"subreddit":"MachineLearning","stickied":false,"author":"ezrakh","title":"Defending Networks with Incomplete Information - A Machine Learning Approach","subreddit_id":"t5_2r3gv","url":"http://www.youtube.com/watch?v=36IT9VgGr0g"}
{"num_comments":6,"user_reports":[],"id":"1lbmni","over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"score":7,"permalink":"/r/MachineLearning/comments/1lbmni/progol_ilp_relational_learning/","domain":"self.MachineLearning","retrieved_on":1411882697,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to understand how to use relational and ilp tools for machine learning. So I have constructed an example problem:&lt;/p&gt;\n\n&lt;p&gt;I have two tables and I want progol to learn a rule along the lines of &amp;quot;class(A,big_spender) if salary(A,high)&amp;quot; or &amp;quot;class(A, big_spender) if married(A,B), salary(B,high)\nTable1&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Name    Profession  Salary  Education   Class: Big  Spender\nBob      Doctor         High            Private          Yes\nJeff    Unemployed  Low         Private          No\nDonald  Artist          Low         State                    Yes\nJenny   Unemployed  Low         State                    Yes\nAnn     Lawyer  High            Private          Yes\nEmily   TA          Low         State                    No\nSoph    Unemployed  Low         State                    No\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Table 2&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Married \nBob         Jenny\nDonald          Ann\nSophie          Jeff\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The input file I have made follows, but progol is unable to learn the correct rule. I am confused about how to set out my modeb declarations and the other settings that progols uses to guide the search. Anyone able to help me understand what I need to put in these settings and why?&lt;/p&gt;\n\n&lt;p&gt;The input file I have made:&lt;/p&gt;\n\n&lt;p&gt;%%%%%%%%%%%%%%\n%This data is a made up big spender by spouse examples&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;%I need to set inflation etc\n%:-set(inflate,500)?\n%:-set(r, 100000)?\n%:-set(h, 60)?\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%Then I need my mode declarations&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;:-modeh(*, class(+person, #spender))?\n\n:-modeb(*, profession(+person, #job))?\n:-modeb(*, salary(+person,#amount))?\n:-modeb(*, education(+person, #schooling))?\n:-modeb(*, married(+person, -person))?\n-modeb(*, married(-person, +person))?\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%Then I need my Integrity constraints. This states that A can not be a big and not big spender at the same time&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;:- class(A, B), class(A, C), B\\=C. \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%Now my examples&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class(bob, big_spender).\nclass(jeff, not_big_spender).\nclass(donald, big_spender).\nclass(jenny, big_spender).\nclass(ann, big_spender).\n\nclass(emily, not_big_spender).\nclass(sophie, not_big_spender).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%My background knowledge&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;person(bob).\nperson(jeff).\nperson(donald).\nperson(jenny).\nperson(ann).\nperson(emily).\nperson(sophie).\n\nspender(big_spender).\nspender(not_big_spender).\n\nschooling(state).\nschooling(private).\n\njob(doctor).\njob(lawyer).\njob(unemployed).\njob(artist).\njob(ta).\n\namount(high).\namount(low).\n\n\nmarried(bob, jenny).\nmarried(donald, ann).\nmarried(jeff, sophie).\n\n%example1\nprofession(bob, doctor).\nsalary(bob, high).\neducation(bob, private).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example2&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(jeff, unemployed).\nsalary(jeff, low).\neducation(jeff, private).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example3&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(donald, artist).\nsalary(donald, low).\neducation(donald, state).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example4&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(jenny, unemployed).\nsalary(jenny, low).\neducation(jenny, state).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example5&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(ann, lawyer).\nsalary(ann, high).\neducation(ann, private).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example6&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(emily, ta).\nsalary(emily, low).\neducation(emily, state).\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;%example7&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;profession(sophie, unemployed).\nsalary(sophie, low).\neducation(sophie, state).\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"downs":0,"report_reasons":null,"distinguished":null,"edited":1377773743,"thumbnail":"self","is_self":true,"created_utc":1377773019,"selftext":"I am trying to understand how to use relational and ilp tools for machine learning. So I have constructed an example problem:\n\nI have two tables and I want progol to learn a rule along the lines of \"class(A,big_spender) if salary(A,high)\" or \"class(A, big_spender) if married(A,B), salary(B,high)\nTable1\n\n    Name    Profession\tSalary\tEducation\tClass: Big  Spender\n    Bob      Doctor\t        High\t        Private\t         Yes\n    Jeff    Unemployed\tLow\t        Private\t         No\n    Donald  Artist\t        Low\t        State\t                 Yes\n    Jenny   Unemployed\tLow\t        State\t                 Yes\n    Ann     Lawyer\tHigh\t        Private\t         Yes\n    Emily   TA\t        Low\t        State\t                 No\n    Soph    Unemployed  Low\t        State\t                 No\n\nTable 2\n\n    Married\t\n    Bob\t        Jenny\n    Donald\t        Ann\n    Sophie\t        Jeff\n\nThe input file I have made follows, but progol is unable to learn the correct rule. I am confused about how to set out my modeb declarations and the other settings that progols uses to guide the search. Anyone able to help me understand what I need to put in these settings and why?\n\nThe input file I have made:\n\n%%%%%%%%%%%%%%\n%This data is a made up big spender by spouse examples\n\n\n    %I need to set inflation etc\n    %:-set(inflate,500)?\n    %:-set(r, 100000)?\n    %:-set(h, 60)?\n\n\n%Then I need my mode declarations\n\n    :-modeh(*, class(+person, #spender))?\n\n    :-modeb(*, profession(+person, #job))?\n    :-modeb(*, salary(+person,#amount))?\n    :-modeb(*, education(+person, #schooling))?\n    :-modeb(*, married(+person, -person))?\n    -modeb(*, married(-person, +person))?\n\n\n%Then I need my Integrity constraints. This states that A can not be a big and not big spender at the same time\n    \n    :- class(A, B), class(A, C), B\\=C. \n\n\n%Now my examples\n\n    class(bob, big_spender).\n    class(jeff, not_big_spender).\n    class(donald, big_spender).\n    class(jenny, big_spender).\n    class(ann, big_spender).\n\n    class(emily, not_big_spender).\n    class(sophie, not_big_spender).\n\n\n%My background knowledge\n\n    person(bob).\n    person(jeff).\n    person(donald).\n    person(jenny).\n    person(ann).\n    person(emily).\n    person(sophie).\n\n    spender(big_spender).\n    spender(not_big_spender).\n\n    schooling(state).\n    schooling(private).\n\n    job(doctor).\n    job(lawyer).\n    job(unemployed).\n    job(artist).\n    job(ta).\n\n    amount(high).\n    amount(low).\n\n\n    married(bob, jenny).\n    married(donald, ann).\n    married(jeff, sophie).\n\n    %example1\n    profession(bob, doctor).\n    salary(bob, high).\n    education(bob, private).\n\n\n%example2\n\n    profession(jeff, unemployed).\n    salary(jeff, low).\n    education(jeff, private).\n\n%example3\n\n    profession(donald, artist).\n    salary(donald, low).\n    education(donald, state).\n\n%example4\n\n    profession(jenny, unemployed).\n    salary(jenny, low).\n    education(jenny, state).\n\n%example5\n\n    profession(ann, lawyer).\n    salary(ann, high).\n    education(ann, private).\n\n%example6\n\n    profession(emily, ta).\n    salary(emily, low).\n    education(emily, state).\n\n%example7\n\n    profession(sophie, unemployed).\n    salary(sophie, low).\n    education(sophie, state).","ups":7,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null,"subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1lbmni/progol_ilp_relational_learning/","title":"Progol ILP relational learning","subreddit_id":"t5_2r3gv","author":"walrusesarecool"}
{"is_self":true,"selftext":"I would like to gain some insight into what kind of process goes into using machine learning in industry. This question is aimed at those of you who have experience in industry or those who have looked for jobs or might have any insight for whatever reason.\n\nSome background- I am currently a year into a Masters program with a thesis on applying Bayesian Networks in the biomedical field. For the past two years I carried out some research into SVMs as well. I have no intention to continue into a PhD program and would like to get a job after graduating.\n\nMy deep knowledge only involves a very narrow slice of the entire machine learning spectrum and I am sure that I would have a hard time trying to find a position related to my research topic without having a PhD. The fields that seem to use machine learning the most (e.g. finance) involve different branches of machine learning that I have no working knowledge of. I have played around with various ML libraries and taken some online courses, but I feel like I have only scratched the surface of what is out there.\n\nOf course this will depend on the nature of the position, but ***what kind of knowledge is generally expected when applying for positions that use ML in a general sense, but may not directly to relate to what I am most comfortable with***? Are positions like this common? Is it possible to get a position in an industry which I have no experience in (like finance)?\n\nThanks in advance for the input. ","created_utc":1377746658,"ups":3,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"link_flair_text":null,"secure_media":null,"author_flair_text":null,"banned_by":null,"subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1lb2tw/question_on_machine_learning_in_industry/","title":"Question on machine learning in industry","subreddit_id":"t5_2r3gv","author":"mashito","num_comments":7,"user_reports":[],"id":"1lb2tw","over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"score":3,"permalink":"/r/MachineLearning/comments/1lb2tw/question_on_machine_learning_in_industry/","domain":"self.MachineLearning","retrieved_on":1411883536,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to gain some insight into what kind of process goes into using machine learning in industry. This question is aimed at those of you who have experience in industry or those who have looked for jobs or might have any insight for whatever reason.&lt;/p&gt;\n\n&lt;p&gt;Some background- I am currently a year into a Masters program with a thesis on applying Bayesian Networks in the biomedical field. For the past two years I carried out some research into SVMs as well. I have no intention to continue into a PhD program and would like to get a job after graduating.&lt;/p&gt;\n\n&lt;p&gt;My deep knowledge only involves a very narrow slice of the entire machine learning spectrum and I am sure that I would have a hard time trying to find a position related to my research topic without having a PhD. The fields that seem to use machine learning the most (e.g. finance) involve different branches of machine learning that I have no working knowledge of. I have played around with various ML libraries and taken some online courses, but I feel like I have only scratched the surface of what is out there.&lt;/p&gt;\n\n&lt;p&gt;Of course this will depend on the nature of the position, but &lt;strong&gt;&lt;em&gt;what kind of knowledge is generally expected when applying for positions that use ML in a general sense, but may not directly to relate to what I am most comfortable with&lt;/em&gt;&lt;/strong&gt;? Are positions like this common? Is it possible to get a position in an industry which I have no experience in (like finance)?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for the input. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"report_reasons":null,"edited":false,"thumbnail":"self"}
{"retrieved_on":1411877123,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m mulling over the idea of working my way through a data science text and self-teaching. I&amp;#39;d probably create a blog to document my efforts and to serve as notes to myself. I think I&amp;#39;d learn more and have more fun if I had a research project that I could work through as I learn.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very much interested in finance and economics. Additionally, professionally I work in commercial real estate. However, I don&amp;#39;t know how well these subjects would lend themselves to research projects. Generally trying to predict the markets is a fool&amp;#39;s game. So I&amp;#39;m wondering what unexplored, worthwhile areas of research might exist. I&amp;#39;m reaching out to the community to see if you guys have any interesting ideas. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"thumbnail":"self","distinguished":null,"report_reasons":null,"downs":0,"edited":false,"secure_media_embed":{},"media_embed":{},"gilded":0,"num_comments":0,"id":"1lf8q9","user_reports":[],"over_18":false,"permalink":"/r/MachineLearning/comments/1lf8q9/help_me_choose_a_data_science_research_project/","score":1,"domain":"self.MachineLearning","banned_by":null,"author_flair_text":null,"link_flair_text":null,"secure_media":null,"url":"http://www.reddit.com/r/MachineLearning/comments/1lf8q9/help_me_choose_a_data_science_research_project/","title":"Help me choose a data science research project","subreddit_id":"t5_2r3gv","author":"[deleted]","stickied":false,"subreddit":"MachineLearning","created_utc":1377895712,"selftext":"I'm mulling over the idea of working my way through a data science text and self-teaching. I'd probably create a blog to document my efforts and to serve as notes to myself. I think I'd learn more and have more fun if I had a research project that I could work through as I learn.\n\nI'm very much interested in finance and economics. Additionally, professionally I work in commercial real estate. However, I don't know how well these subjects would lend themselves to research projects. Generally trying to predict the markets is a fool's game. So I'm wondering what unexplored, worthwhile areas of research might exist. I'm reaching out to the community to see if you guys have any interesting ideas. Thanks!","is_self":true,"link_flair_css_class":null,"media":null,"author_flair_css_class":null,"ups":1}
{"media_embed":{},"gilded":0,"secure_media_embed":{},"over_18":false,"user_reports":[],"num_comments":0,"id":"1leupx","domain":"nada.kth.se","permalink":"/r/MachineLearning/comments/1leupx/erik_bernhardsson_spotify_implementing_a_scalable/","score":18,"mod_reports":[],"selftext_html":null,"retrieved_on":1411877695,"thumbnail":"default","edited":false,"distinguished":null,"report_reasons":null,"downs":0,"created_utc":1377884602,"selftext":"","is_self":false,"media":null,"link_flair_css_class":null,"author_flair_css_class":null,"ups":18,"banned_by":null,"author_flair_text":null,"secure_media":null,"link_flair_text":null,"author":"chocolategirl","title":"Erik Bernhardsson (Spotify) - Implementing a Scalable Music Recommender System [PDF]","subreddit_id":"t5_2r3gv","url":"http://www.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2009/rapporter09/bernhardsson_erik_09071.pdf","stickied":false,"subreddit":"MachineLearning"}
{"secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null,"subreddit":"MachineLearning","stickied":false,"title":"Learning to rank with scikit-learn: the pairwise transform","author":"rrenaud","subreddit_id":"t5_2r3gv","url":"http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/","is_self":false,"selftext":"","created_utc":1377880259,"ups":27,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"mod_reports":[],"selftext_html":null,"retrieved_on":1411877919,"edited":false,"downs":0,"distinguished":null,"report_reasons":null,"thumbnail":"http://a.thumbs.redditmedia.com/7DqMIzj_K_I-HCzT.jpg","over_18":false,"id":"1lep6q","num_comments":0,"user_reports":[],"gilded":0,"media_embed":{},"secure_media_embed":{},"domain":"fa.bianp.net","score":27,"permalink":"/r/MachineLearning/comments/1lep6q/learning_to_rank_with_scikitlearn_the_pairwise/"}
{"is_self":true,"created_utc":1377831313,"selftext":"I want to make a thesis named \"Using Machine Learning to monitor QOS in wireless networks\", but I only had a  bit of machine learning in my course, and it consisted of using a trainer in python to classify news articles. So I'm pretty much at the beggining, and I've learn that there are LOTS of methods to choose from. Can you help me by saying what should I read and study mostly, and what  I should not spend time on? I think my data will be all the traffic made my people using the Access Points, as well as the typical data provided by the Acess Points. I want to become excellent at it, but I'm kinda lost for now.\nThank you very much guys!  Sorry if I made any mistake, english isn't my first language.","ups":0,"author_flair_css_class":null,"media":null,"link_flair_css_class":null,"link_flair_text":null,"secure_media":null,"banned_by":null,"author_flair_text":null,"subreddit":"MachineLearning","stickied":false,"url":"http://www.reddit.com/r/MachineLearning/comments/1ldjzs/help_me_study_for_my_thesisusing_machine_learning/","title":"Help me study for my thesis:\"Using Machine Learning to monitor QOS in wireless networks\"","subreddit_id":"t5_2r3gv","author":"learning_ML","num_comments":1,"user_reports":[],"id":"1ldjzs","over_18":false,"secure_media_embed":{},"gilded":0,"media_embed":{},"score":0,"permalink":"/r/MachineLearning/comments/1ldjzs/help_me_study_for_my_thesisusing_machine_learning/","domain":"self.MachineLearning","retrieved_on":1411879720,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a thesis named &amp;quot;Using Machine Learning to monitor QOS in wireless networks&amp;quot;, but I only had a  bit of machine learning in my course, and it consisted of using a trainer in python to classify news articles. So I&amp;#39;m pretty much at the beggining, and I&amp;#39;ve learn that there are LOTS of methods to choose from. Can you help me by saying what should I read and study mostly, and what  I should not spend time on? I think my data will be all the traffic made my people using the Access Points, as well as the typical data provided by the Acess Points. I want to become excellent at it, but I&amp;#39;m kinda lost for now.\nThank you very much guys!  Sorry if I made any mistake, english isn&amp;#39;t my first language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"report_reasons":null,"distinguished":null,"edited":false,"thumbnail":"self"}
{"over_18":false,"num_comments":40,"id":"1lgrd3","user_reports":[],"gilded":0,"media_embed":{},"secure_media_embed":{},"domain":"self.MachineLearning","score":15,"permalink":"/r/MachineLearning/comments/1lgrd3/stochastic_gradient_descent_outperforming_lbfgs/","mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been hitting my head against this problem for a while now, and I&amp;#39;m about to give up and just use the method which I have that performs.  However, I think I also have evidence that something about my implementation is broken.  I&amp;#39;m asking for help here because my options for soliciting feedback/advice are pretty limited, so apologies for the multiple posts on the same subject matter.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an album of some simple experimental results based on building a 25 hidden layer unit autoencoder, and training it with 8x8 grayscale images from Bruno Olshausen&amp;#39;s whitened natural images dataset:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://imgur.com/a/zuzJO\"&gt;http://imgur.com/a/zuzJO&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ideally, such an autoencoder should resolve 25 edge detectors in this configuration.    The first image shows this, and it&amp;#39;s the result of training the network with &amp;quot;stochastic gradient descent&amp;quot;, i.e. simple fixed-step gradient descent wherein the batch size is low (100 training examples), and only one step is taken per batch.  The second figure shows the objective function versus the training iteration, and you can see the random walk downwards over 24,000 batch iterations.  This took a little over 2 minutes to run.  &lt;/p&gt;\n\n&lt;p&gt;The last picture is a typical example of the results I get from running any of three algorithms in a more typical fashion (i.e. with a batch size equal to the training set size, with multiple steps taken on the batch).  Both L-BFGS and Conjugate Gradient Descent manage to quickly (within 50 iterations) find a minima on the order of 0.5 (equivalent to the finishing value of stochastic gradient descent), but the result looks like the third figure.  Standard gradient descent with a large batch also does this.  L-BFGS in particular (I&amp;#39;m using the implementation from the RISO project) will iterate a few times and then fail when it has a nonzero gradient but ends up taking a step of length 0.  &lt;/p&gt;\n\n&lt;p&gt;My gradient calculation has been tested and I have high confidence that it is working properly.  My objective function calculation seems to be the only thing separating CGD and L-BFGS from fixed-step gradient descent, but I&amp;#39;ve been staring at it for many hours now and it just isn&amp;#39;t complex enough to convince me that there&amp;#39;s a bug hidden in there.  I would blame the data, but this exact experiment is solved using L-BFGS in Andrew Ng&amp;#39;s tutorial &lt;a href=\"http://ufldl.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder\"&gt;here&lt;/a&gt;.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to use this code on some much larger experiments and I don&amp;#39;t want to start off with a buggy implementation, but I can&amp;#39;t nail down where my method might be diverging from Ng&amp;#39;s example.  Any thoughts or suggestions would be appreciated.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411874729,"edited":false,"downs":0,"distinguished":null,"report_reasons":null,"thumbnail":"self","is_self":true,"created_utc":1377963602,"selftext":"I've been hitting my head against this problem for a while now, and I'm about to give up and just use the method which I have that performs.  However, I think I also have evidence that something about my implementation is broken.  I'm asking for help here because my options for soliciting feedback/advice are pretty limited, so apologies for the multiple posts on the same subject matter.\n\n\n\nHere's an album of some simple experimental results based on building a 25 hidden layer unit autoencoder, and training it with 8x8 grayscale images from Bruno Olshausen's whitened natural images dataset:\n\n\n\nhttp://imgur.com/a/zuzJO\n\n\n\n\nIdeally, such an autoencoder should resolve 25 edge detectors in this configuration.    The first image shows this, and it's the result of training the network with \"stochastic gradient descent\", i.e. simple fixed-step gradient descent wherein the batch size is low (100 training examples), and only one step is taken per batch.  The second figure shows the objective function versus the training iteration, and you can see the random walk downwards over 24,000 batch iterations.  This took a little over 2 minutes to run.  \n\n\n\n\nThe last picture is a typical example of the results I get from running any of three algorithms in a more typical fashion (i.e. with a batch size equal to the training set size, with multiple steps taken on the batch).  Both L-BFGS and Conjugate Gradient Descent manage to quickly (within 50 iterations) find a minima on the order of 0.5 (equivalent to the finishing value of stochastic gradient descent), but the result looks like the third figure.  Standard gradient descent with a large batch also does this.  L-BFGS in particular (I'm using the implementation from the RISO project) will iterate a few times and then fail when it has a nonzero gradient but ends up taking a step of length 0.  \n\n\n\nMy gradient calculation has been tested and I have high confidence that it is working properly.  My objective function calculation seems to be the only thing separating CGD and L-BFGS from fixed-step gradient descent, but I've been staring at it for many hours now and it just isn't complex enough to convince me that there's a bug hidden in there.  I would blame the data, but this exact experiment is solved using L-BFGS in Andrew Ng's tutorial [here](http://ufldl.stanford.edu/wiki/index.php/Exercise:Sparse_Autoencoder).  \n\n\n\nI'm about to use this code on some much larger experiments and I don't want to start off with a buggy implementation, but I can't nail down where my method might be diverging from Ng's example.  Any thoughts or suggestions would be appreciated.  \n\n","ups":15,"author_flair_css_class":null,"link_flair_css_class":null,"media":null,"secure_media":null,"link_flair_text":null,"banned_by":null,"author_flair_text":null,"subreddit":"MachineLearning","stickied":false,"title":"Stochastic gradient descent outperforming L-BFGS","subreddit_id":"t5_2r3gv","author":"eubarch","url":"http://www.reddit.com/r/MachineLearning/comments/1lgrd3/stochastic_gradient_descent_outperforming_lbfgs/"}
{"domain":"self.MachineLearning","permalink":"/r/MachineLearning/comments/1lgokv/is_there_some_exercises_for_practice/","score":3,"over_18":false,"user_reports":[],"num_comments":3,"id":"1lgokv","media_embed":{},"gilded":0,"secure_media_embed":{},"edited":false,"distinguished":null,"report_reasons":null,"downs":0,"thumbnail":"self","mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So you can download some data, and descriptions of problems you need to solve. Maybe something like kaggle, but more learning oriented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","retrieved_on":1411874849,"author_flair_css_class":null,"ups":3,"media":null,"link_flair_css_class":null,"is_self":true,"created_utc":1377960661,"selftext":"So you can download some data, and descriptions of problems you need to solve. Maybe something like kaggle, but more learning oriented.","stickied":false,"subreddit":"MachineLearning","author":"dotneter","title":"Is there some exercises for practice?","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1lgokv/is_there_some_exercises_for_practice/","secure_media":null,"link_flair_text":null,"author_flair_text":null,"banned_by":null}
