{"score":14,"gilded":0,"selftext_html":null,"domain":"cs.toronto.edu","author":"blindConjecture","user_reports":[],"num_comments":13,"mod_reports":[],"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354393584,"subreddit_id":"t5_2r3gv","url":"http://www.cs.toronto.edu/~hinton/adi/index.htm","retrieved_on":1413328680,"link_flair_text":null,"distinguished":null,"title":"Interactive Demonstration of a Digit-Recognizing Neural Network in Action (Explanation in the comments)","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"secure_media":null,"thumbnail":"default","permalink":"/r/MachineLearning/comments/1447uj/interactive_demonstration_of_a_digitrecognizing/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"media":null,"ups":14,"report_reasons":null,"is_self":false,"edited":false,"id":"1447uj"}
{"edited":false,"is_self":true,"id":"143iq5","permalink":"/r/MachineLearning/comments/143iq5/how_do_you_approach_a_data_mining_contest/","ups":2,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"thumbnail":"self","secure_media":null,"subreddit":"MachineLearning","title":"How do you approach a data mining contest? ","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1354352380,"downs":0,"author_flair_css_class":null,"selftext":"I'm just wondering what's the process each of you use when you are faced with a new Kaggle/other contest. Most of the winners seem to have a hybrid/kitchen sink approach to things. How do you get to there? Where do you start? I'd like to hear about how you work. ","retrieved_on":1413329738,"url":"http://www.reddit.com/r/MachineLearning/comments/143iq5/how_do_you_approach_a_data_mining_contest/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":0,"gilded":0,"score":2,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just wondering what&amp;#39;s the process each of you use when you are faced with a new Kaggle/other contest. Most of the winners seem to have a hybrid/kitchen sink approach to things. How do you get to there? Where do you start? I&amp;#39;d like to hear about how you work. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"naive_babes","user_reports":[],"domain":"self.MachineLearning"}
{"is_self":true,"edited":false,"id":"143cdo","permalink":"/r/MachineLearning/comments/143cdo/is_it_possible_to_learn_how_to_cluster_data_points/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":0,"report_reasons":null,"media":null,"secure_media":null,"thumbnail":"default","link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Is it possible to \"learn how to cluster\" data points?","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354340756,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/143cdo/is_it_possible_to_learn_how_to_cluster_data_points/","retrieved_on":1413330001,"num_comments":4,"mod_reports":[],"gilded":0,"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"saprian"}
{"edited":false,"is_self":true,"id":"145i2f","permalink":"/r/MachineLearning/comments/145i2f/best_algorithms_for_twitter_sentiment_analysis_in/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":2,"report_reasons":null,"media":null,"secure_media":null,"thumbnail":"self","link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Best Algorithms for twitter sentiment analysis in WEKA.","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"selftext":"Hello everyone, I am trying to implement something and i need your help. I want to perform a sentiment analysis based on some hundreds of tweets and i use Weka. I am parsing the information through twitter4j and then analysing by hand the tweets. However i am at a total loss regarding the algorithms to use in Weka. Have you ever implemented something like that and do you have any ides for some of the best algorithms? Thanks in advance!","author_flair_css_class":null,"downs":0,"created_utc":1354462193,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/145i2f/best_algorithms_for_twitter_sentiment_analysis_in/","retrieved_on":1413326625,"num_comments":9,"mod_reports":[],"gilded":0,"score":2,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I am trying to implement something and i need your help. I want to perform a sentiment analysis based on some hundreds of tweets and i use Weka. I am parsing the information through twitter4j and then analysing by hand the tweets. However i am at a total loss regarding the algorithms to use in Weka. Have you ever implemented something like that and do you have any ides for some of the best algorithms? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"Lilykos","user_reports":[]}
{"permalink":"/r/MachineLearning/comments/145cdg/help_implementing_semantic_hashing/","media_embed":{},"over_18":false,"stickied":false,"report_reasons":null,"ups":15,"media":null,"secure_media_embed":{},"edited":false,"is_self":true,"id":"145cdg","secure_media":null,"thumbnail":"self","downs":0,"selftext":"Recently I'm confused with semantic hashing. I want to, given a collect of documents, get the semantic hash codes for each document(semantic hashing is introduced here: http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf). I downloaded the source code of Deep AutoEncoder from http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html. It seems helpful but slightly different from semantic hashing especially in the fine-tuning part. That is, I don't know how to get binary codes for each document after fine-tuning. Anyone knows that? Any suggestions on learning semantic hashing are welcomed! Thank you!","author_flair_css_class":null,"created_utc":1354449481,"url":"http://www.reddit.com/r/MachineLearning/comments/145cdg/help_implementing_semantic_hashing/","subreddit_id":"t5_2r3gv","retrieved_on":1413326882,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Help implementing semantic hashing","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"gilded":0,"score":15,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I&amp;#39;m confused with semantic hashing. I want to, given a collect of documents, get the semantic hash codes for each document(semantic hashing is introduced here: &lt;a href=\"http://www.cs.toronto.edu/%7Ersalakhu/papers/semantic_final.pdf\"&gt;http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf&lt;/a&gt;). I downloaded the source code of Deep AutoEncoder from &lt;a href=\"http://www.cs.toronto.edu/%7Ehinton/MatlabForSciencePaper.html\"&gt;http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html&lt;/a&gt;. It seems helpful but slightly different from semantic hashing especially in the fine-tuning part. That is, I don&amp;#39;t know how to get binary codes for each document after fine-tuning. Anyone knows that? Any suggestions on learning semantic hashing are welcomed! Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"gynnash","user_reports":[],"domain":"self.MachineLearning","num_comments":9,"mod_reports":[]}
{"id":"147j6s","is_self":true,"edited":false,"ups":0,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/147j6s/do_i_have_to_normalize_data_andor_weight_vectors/","thumbnail":"self","secure_media":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Do I have to normalize data and/or weight vectors in SOMs?...","distinguished":null,"link_flair_text":null,"retrieved_on":1413323396,"url":"http://www.reddit.com/r/MachineLearning/comments/147j6s/do_i_have_to_normalize_data_andor_weight_vectors/","subreddit_id":"t5_2r3gv","created_utc":1354555919,"downs":0,"selftext":"\nSo I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. \n\nAs I understand it:\n\n1) The data used in SOMs must be normalized, to have unit length. \n\n2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt 'blow up'.\n\nThis is confusing to me however. If you look [here](http://imgur.com/bN7TA), this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. \n\nIf I normalize it however, that means I am projecting it unto the unit circle as shown [here](http://imgur.com/NoFGl). However if I do that... I lose the information about those 3 clusters from the above image... \n\nDoes this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...","author_flair_css_class":null,"mod_reports":[],"num_comments":0,"author":"Ayakalam","user_reports":[],"domain":"self.MachineLearning","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. &lt;/p&gt;\n\n&lt;p&gt;As I understand it:&lt;/p&gt;\n\n&lt;p&gt;1) The data used in SOMs must be normalized, to have unit length. &lt;/p&gt;\n\n&lt;p&gt;2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt &amp;#39;blow up&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;This is confusing to me however. If you look &lt;a href=\"http://imgur.com/bN7TA\"&gt;here&lt;/a&gt;, this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. &lt;/p&gt;\n\n&lt;p&gt;If I normalize it however, that means I am projecting it unto the unit circle as shown &lt;a href=\"http://imgur.com/NoFGl\"&gt;here&lt;/a&gt;. However if I do that... I lose the information about those 3 clusters from the above image... &lt;/p&gt;\n\n&lt;p&gt;Does this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":0,"gilded":0}
{"id":"147eaa","edited":false,"is_self":true,"media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":0,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/147eaa/why_should_one_normalize_input_data_and_outputs/","secure_media":null,"thumbnail":"self","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Why should one normalize input data and outputs weight vectors of a SOM?...","banned_by":null,"url":"http://www.reddit.com/r/MachineLearning/comments/147eaa/why_should_one_normalize_input_data_and_outputs/","subreddit_id":"t5_2r3gv","retrieved_on":1413323612,"downs":0,"selftext":"So I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. \n\nAs I understand it:\n\n1) The data used in SOMs must be normalized, to have unit length. \n\n2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt 'blow up'.\n\nThis is confusing to me however. If you look [here](http://imgur.com/bN7TA), this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. \n\nIf I normalize it however, that means I am projecting it unto the unit circle as shown [here](http://imgur.com/NoFGl). However if I do that... I lose the information about those 3 clusters from the above image... \n\nDoes this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...","author_flair_css_class":null,"created_utc":1354551346,"num_comments":0,"mod_reports":[],"user_reports":[],"author":"Ayakalam","domain":"self.MachineLearning","score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. &lt;/p&gt;\n\n&lt;p&gt;As I understand it:&lt;/p&gt;\n\n&lt;p&gt;1) The data used in SOMs must be normalized, to have unit length. &lt;/p&gt;\n\n&lt;p&gt;2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt &amp;#39;blow up&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;This is confusing to me however. If you look &lt;a href=\"http://imgur.com/bN7TA\"&gt;here&lt;/a&gt;, this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. &lt;/p&gt;\n\n&lt;p&gt;If I normalize it however, that means I am projecting it unto the unit circle as shown &lt;a href=\"http://imgur.com/NoFGl\"&gt;here&lt;/a&gt;. However if I do that... I lose the information about those 3 clusters from the above image... &lt;/p&gt;\n\n&lt;p&gt;Does this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0}
{"secure_media":null,"thumbnail":"self","id":"147d6u","is_self":true,"edited":false,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"permalink":"/r/MachineLearning/comments/147d6u/why_am_i_normalizing_data_andor_weight_vectors_in/","num_comments":0,"mod_reports":[],"domain":"self.MachineLearning","author":"Ayakalam","user_reports":[],"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. &lt;/p&gt;\n\n&lt;p&gt;As I understand it:&lt;/p&gt;\n\n&lt;p&gt;1) The data used in SOMs must be normalized, to have unit length. &lt;/p&gt;\n\n&lt;p&gt;2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt &amp;#39;blow up&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;This is confusing to me however. If you look &lt;a href=\"http://imgur.com/bN7TA\"&gt;here&lt;/a&gt;, this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. &lt;/p&gt;\n\n&lt;p&gt;If I normalize it however, that means I am projecting it unto the unit circle as shown &lt;a href=\"http://imgur.com/NoFGl\"&gt;here&lt;/a&gt;. However if I do that... I lose the information about those 3 clusters from the above image... &lt;/p&gt;\n\n&lt;p&gt;Does this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Why am I normalizing data and/or weight vectors in SOMs?","subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/147d6u/why_am_i_normalizing_data_andor_weight_vectors_in/","retrieved_on":1413323657,"author_flair_css_class":null,"selftext":"\nSo I have a simple question -  I have written a simple SOM code, using two-dimensional data to play around with. \n\nAs I understand it:\n\n1) The data used in SOMs must be normalized, to have unit length. \n\n2) A weight vector that a SOM uses during the update is normalized to unit length every iteration as well, so that the weight vector doesnt 'blow up'.\n\nThis is confusing to me however. If you look [here](http://imgur.com/bN7TA), this is my 2-dimensional original data that I would like to train a SOM to. (It is un-normalized). I can clearly make out 3 clusters. \n\nIf I normalize it however, that means I am projecting it unto the unit circle as shown [here](http://imgur.com/NoFGl). However if I do that... I lose the information about those 3 clusters from the above image... \n\nDoes this mean that SOMs are only capable of being used on normalized data? To me this would limit their capabilities...","downs":0,"created_utc":1354550148}
{"secure_media":null,"thumbnail":"default","permalink":"/r/MachineLearning/comments/146zlq/my_education_in_machine_learning_via_cousera_a/","media_embed":{},"stickied":false,"over_18":false,"ups":54,"report_reasons":null,"media":null,"secure_media_embed":{},"edited":false,"is_self":false,"id":"146zlq","score":54,"gilded":0,"selftext_html":null,"author":"srkiboy83","user_reports":[],"domain":"richardminerich.com","num_comments":16,"mod_reports":[],"downs":0,"author_flair_css_class":null,"selftext":"","created_utc":1354527300,"url":"http://richardminerich.com/2012/12/my-education-in-machine-learning-via-cousera/","subreddit_id":"t5_2r3gv","retrieved_on":1413324224,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"My Education in Machine Learning via Cousera, A Review So Far","banned_by":null,"link_flair_css_class":null,"author_flair_text":null}
{"id":"146geb","is_self":true,"edited":false,"secure_media_embed":{},"ups":0,"report_reasons":null,"media":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/146geb/weka_experimenter_class_attribute_is_not_nominal/","thumbnail":"default","secure_media":null,"author_flair_text":null,"link_flair_css_class":null,"title":"Weka Experimenter 'Class attribute is not nominal' but data is processed from Explorer","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"retrieved_on":1413325184,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/146geb/weka_experimenter_class_attribute_is_not_nominal/","created_utc":1354500818,"author_flair_css_class":null,"selftext":"","downs":0,"mod_reports":[],"num_comments":1,"domain":"self.MachineLearning","user_reports":[],"author":"ArchangelleSareon","score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0}
{"id":"149ykw","edited":false,"is_self":true,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":0,"media":null,"report_reasons":null,"permalink":"/r/MachineLearning/comments/149ykw/askml_what_schools_should_i_apply_to_if_i_want_to/","secure_media":null,"thumbnail":"self","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"[AskML] What schools should I apply to if I want to do deep learning but my GPA is not good?","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/149ykw/askml_what_schools_should_i_apply_to_if_i_want_to/","retrieved_on":1413319581,"selftext":"I'm looking to apply to either a Ph.D. or MS program in computer science, and I want to do machine learning, deep learning specifically. \n\nThe problem is that my GPA is not good. I graduated with a degree in physics in 2003, but I didn't really care about grades then, so my grades were terrible. A few years later I went back and got a degree in CS (graduated in 2010) and my grades were somewhat better (3.0), but still not what they should have been. Those two different degrees are combined on my transcript, and I had more hours in physics, so my transcript looks really bad.  \n\nI don't have any research experience, but I have been working in datamining/programming for the past couple of years since I got my CS degree, and I've learned a good bit in that time. Don't know that this will count for much, since it's not academic work, unfortunately.\n\nSo I am wondering if some of you can give me a recommendation for a place to apply to that has faculty doing deep learning that might have me.","author_flair_css_class":null,"downs":0,"created_utc":1354649674,"num_comments":11,"mod_reports":[],"domain":"self.MachineLearning","author":"plc123","user_reports":[],"score":0,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to apply to either a Ph.D. or MS program in computer science, and I want to do machine learning, deep learning specifically. &lt;/p&gt;\n\n&lt;p&gt;The problem is that my GPA is not good. I graduated with a degree in physics in 2003, but I didn&amp;#39;t really care about grades then, so my grades were terrible. A few years later I went back and got a degree in CS (graduated in 2010) and my grades were somewhat better (3.0), but still not what they should have been. Those two different degrees are combined on my transcript, and I had more hours in physics, so my transcript looks really bad.  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have any research experience, but I have been working in datamining/programming for the past couple of years since I got my CS degree, and I&amp;#39;ve learned a good bit in that time. Don&amp;#39;t know that this will count for much, since it&amp;#39;s not academic work, unfortunately.&lt;/p&gt;\n\n&lt;p&gt;So I am wondering if some of you can give me a recommendation for a place to apply to that has faculty doing deep learning that might have me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"id":"149ihu","is_self":false,"edited":false,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/149ihu/help_trying_to_understand_the_basics_of_bayesian/","thumbnail":"default","secure_media":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Help: Trying to understand the basics of bayesian inference","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413320255,"url":"http://math.stackexchange.com/questions/245951/trying-to-understand-the-basics-of-bayesian-inference","subreddit_id":"t5_2r3gv","created_utc":1354633708,"downs":0,"author_flair_css_class":null,"selftext":"","mod_reports":[],"num_comments":0,"author":"jokerbrb","user_reports":[],"domain":"math.stackexchange.com","selftext_html":null,"score":1,"gilded":0}
{"num_comments":7,"mod_reports":[],"gilded":0,"score":8,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all:&lt;/p&gt;\n\n&lt;p&gt;I need one point clarified for me regarding gibbs sampling on a dirichlet process--&lt;/p&gt;\n\n&lt;p&gt;I am pretty much up-to-speed on the generative portion of the algorithm:&lt;/p&gt;\n\n&lt;p&gt;for each datapoint to be generated:\n*Draw a partition, c_i, from a Chinese Restaurant Process, ployna urn, or STICK()...&lt;/p&gt;\n\n&lt;p&gt;*look up the parameter vector, phi_i, associated with c_i&lt;/p&gt;\n\n&lt;p&gt;*If there is currently no existing phi(c_i), draw one from the base measure Phi, and associate it with subsequent c_i&lt;/p&gt;\n\n&lt;p&gt;*generate datapoint based on F(phi_i )&lt;/p&gt;\n\n&lt;p&gt;For gibbs sampling, we remove one datapoint at random from his table and sample a new table from the CRP...\nIf he was the only datapoint at his old table, we can forget the index-&amp;gt;parameter mapping previously associated with that table.\nWe calculate the posterior: p(x|y,z) which is a lot like saying, p(x|y, Phi_z).&lt;/p&gt;\n\n&lt;p&gt;What special sauce is my intuition missing here? it is not obvious to me how this will optimize the number of tables/clusters.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"GratefulTony","link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Need clarification on Dirichlet Process Inference","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"Hello all:\n\nI need one point clarified for me regarding gibbs sampling on a dirichlet process--\n\nI am pretty much up-to-speed on the generative portion of the algorithm:\n\nfor each datapoint to be generated:\n*Draw a partition, c_i, from a Chinese Restaurant Process, ployna urn, or STICK()...\n\n*look up the parameter vector, phi_i, associated with c_i\n\n*If there is currently no existing phi(c_i), draw one from the base measure Phi, and associate it with subsequent c_i\n\n *generate datapoint based on F(phi_i )\n\n\nFor gibbs sampling, we remove one datapoint at random from his table and sample a new table from the CRP...\nIf he was the only datapoint at his old table, we can forget the index-&gt;parameter mapping previously associated with that table.\nWe calculate the posterior: p(x|y,z) which is a lot like saying, p(x|y, Phi_z).\n\nWhat special sauce is my intuition missing here? it is not obvious to me how this will optimize the number of tables/clusters.\n","downs":0,"created_utc":1354736331,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14c6qn/need_clarification_on_dirichlet_process_inference/","retrieved_on":1413316323,"secure_media":null,"thumbnail":"self","is_self":true,"edited":1354738609,"id":"14c6qn","permalink":"/r/MachineLearning/comments/14c6qn/need_clarification_on_dirichlet_process_inference/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":8,"report_reasons":null,"media":null}
{"num_comments":7,"mod_reports":[],"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using GIZA++ to align Korean and English sentences. I know how to train a model with GIZA++ with my parallel corpus, but I can&amp;#39;t find a way to test my model and use it for future predictions. I see the -tc option that is supposed to be the test corpus, but not sure how to use the option to test the model (what format the test corpus should be in...) or make predictions. So any NLP gurus out there please help!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","user_reports":[],"author":"yukw777","domain":"self.MachineLearning","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"How do you test a GIZA++ model?","link_flair_css_class":null,"author_flair_text":null,"downs":0,"selftext":"I'm using GIZA++ to align Korean and English sentences. I know how to train a model with GIZA++ with my parallel corpus, but I can't find a way to test my model and use it for future predictions. I see the -tc option that is supposed to be the test corpus, but not sure how to use the option to test the model (what format the test corpus should be in...) or make predictions. So any NLP gurus out there please help!!","author_flair_css_class":null,"created_utc":1354726814,"url":"http://www.reddit.com/r/MachineLearning/comments/14bvhv/how_do_you_test_a_giza_model/","subreddit_id":"t5_2r3gv","retrieved_on":1413316798,"secure_media":null,"thumbnail":"self","edited":false,"is_self":true,"id":"14bvhv","permalink":"/r/MachineLearning/comments/14bvhv/how_do_you_test_a_giza_model/","media_embed":{},"over_18":false,"stickied":false,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{}}
{"is_self":false,"edited":false,"id":"14evef","permalink":"/r/MachineLearning/comments/14evef/nltk_scikitlearn_for_sentiment_classification/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":28,"media":null,"report_reasons":null,"secure_media":null,"thumbnail":"http://a.thumbs.redditmedia.com/hESGpHzGBW9w8kR6.jpg","link_flair_text":null,"distinguished":null,"title":"NLTK + Scikit-Learn for Sentiment Classification","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354837593,"subreddit_id":"t5_2r3gv","url":"http://streamhacker.com/2012/11/22/text-classification-sentiment-analysis-nltk-scikitlearn/","retrieved_on":1413312394,"num_comments":9,"mod_reports":[],"gilded":0,"score":28,"selftext_html":null,"domain":"streamhacker.com","author":"japerk","user_reports":[]}
{"permalink":"/r/MachineLearning/comments/14dwa5/code_school_try_r/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":1,"media":null,"edited":false,"is_self":false,"id":"14dwa5","secure_media":null,"thumbnail":"default","author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354806567,"subreddit_id":"t5_2r3gv","url":"http://tryr.codeschool.com/","retrieved_on":1413313796,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Code School - Try R","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"gilded":0,"score":1,"selftext_html":null,"domain":"tryr.codeschool.com","author":"[deleted]","user_reports":[],"num_comments":0,"mod_reports":[]}
{"is_self":false,"edited":false,"id":"14daen","permalink":"/r/MachineLearning/comments/14daen/successful_test_flight_of_autonomous_black_hawk/","media_embed":{},"over_18":false,"stickied":false,"report_reasons":null,"ups":17,"media":null,"secure_media_embed":{},"secure_media":null,"thumbnail":"http://e.thumbs.redditmedia.com/2AmqNhGhSfeYdmOp.jpg","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Successful Test Flight of Autonomous Black Hawk Helicopter","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"downs":0,"selftext":"","author_flair_css_class":null,"created_utc":1354770800,"url":"http://blog.al.com/breaking/2012/12/black_hawk_flies_lands_and_avo.html","subreddit_id":"t5_2r3gv","retrieved_on":1413314636,"num_comments":1,"mod_reports":[],"score":17,"selftext_html":null,"gilded":0,"author":"stockthrow","user_reports":[],"domain":"blog.al.com"}
{"secure_media":null,"thumbnail":"self","media_embed":{},"stickied":false,"over_18":false,"ups":5,"report_reasons":null,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/14gwr8/anyone_have_a_good_cased_based_reasoning_tutorial/","id":"14gwr8","is_self":true,"edited":false,"author":"daidoji70","user_reports":[],"domain":"self.MachineLearning","score":5,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know anything about Cased Based Reasoning and the Wikipedia&amp;#39;s not really doing justice to this topic (imo).  Anyone got any good resources (from a modeler&amp;#39;s not implementer&amp;#39;s standpoint) on how to utilize such a tool for a n00b?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":3,"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/14gwr8/anyone_have_a_good_cased_based_reasoning_tutorial/","subreddit_id":"t5_2r3gv","retrieved_on":1413309466,"downs":0,"author_flair_css_class":null,"selftext":"I don't know anything about Cased Based Reasoning and the Wikipedia's not really doing justice to this topic (imo).  Anyone got any good resources (from a modeler's not implementer's standpoint) on how to utilize such a tool for a n00b?","created_utc":1354920877,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Anyone have a good Cased Based Reasoning Tutorial?","banned_by":null}
{"num_comments":0,"mod_reports":[],"gilded":0,"score":1,"selftext_html":null,"author":"ektaguptha","user_reports":[],"domain":"ctamachines.in","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"construction heavy equipments","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"downs":0,"author_flair_css_class":null,"selftext":"","created_utc":1354859168,"url":"http://ctamachines.in","subreddit_id":"t5_2r3gv","retrieved_on":1413311505,"secure_media":null,"thumbnail":"default","edited":false,"is_self":false,"id":"14fiji","permalink":"/r/MachineLearning/comments/14fiji/construction_heavy_equipments/","media_embed":{},"stickied":false,"over_18":false,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{}}
{"created_utc":1354839232,"downs":0,"selftext":"http://mmtx.nlm.nih.gov/\n\nI have been looking into it to expand this corpus of medical data I have but I am not quite sure how to use it.  ","author_flair_css_class":null,"retrieved_on":1413312325,"url":"http://www.reddit.com/r/MachineLearning/comments/14ex85/has_anyone_ever_use_metamap_mmtx_to_expand_text/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","title":"Has anyone ever use MetaMap / MMTx to expand text with features?","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"gilded":0,"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"http://mmtx.nlm.nih.gov/\"&gt;http://mmtx.nlm.nih.gov/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I have been looking into it to expand this corpus of medical data I have but I am not quite sure how to use it.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"[deleted]","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":0,"permalink":"/r/MachineLearning/comments/14ex85/has_anyone_ever_use_metamap_mmtx_to_expand_text/","ups":0,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"is_self":true,"edited":false,"id":"14ex85","thumbnail":"default","secure_media":null}
{"thumbnail":"default","secure_media":null,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/14ij00/pleases_stop_using_excellike_formats_to_exchange/","id":"14ij00","edited":false,"is_self":false,"user_reports":[],"author":"talgalili","domain":"r-bloggers.com","gilded":0,"score":1,"selftext_html":null,"mod_reports":[],"num_comments":0,"retrieved_on":1413307188,"url":"http://www.r-bloggers.com/pleases-stop-using-excel-like-formats-to-exchange-data/","subreddit_id":"t5_2r3gv","created_utc":1355001450,"downs":0,"author_flair_css_class":null,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Pleases stop using Excel-like formats to exchange data","distinguished":null,"link_flair_text":null}
{"mod_reports":[],"num_comments":4,"gilded":0,"score":15,"selftext_html":null,"author":"locster","user_reports":[],"domain":"europepmc.org","subreddit":"MachineLearning","banned_by":null,"title":"Derivation of a novel efficient supervised learning algorithm from cortical-subcortical loops, Ashok Chandrashekar, Richard Granger","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1355096196,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413304213,"url":"http://europepmc.org/articles/PMC3254165/pdf/fncom-05-00050.pdf","subreddit_id":"t5_2r3gv","thumbnail":"default","secure_media":null,"edited":false,"is_self":false,"id":"14klwi","permalink":"/r/MachineLearning/comments/14klwi/derivation_of_a_novel_efficient_supervised/","report_reasons":null,"ups":15,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false}
{"mod_reports":[],"num_comments":8,"user_reports":[],"author":"skolor","domain":"self.MachineLearning","gilded":0,"score":3,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been tasked with coming up with a way of ranking the difficulty of elements in a problem set. We have a set of about 30 &amp;quot;problems&amp;quot;, which we will get a binary correct/incorrect outcome from when an individual goes through the problem set. We also have data now for about 300 individuals going through this set.&lt;/p&gt;\n\n&lt;p&gt;The end result is we want a weighting for each of these problems, based on its difficulty, so that we can decide which did best. In earlier versions we have simply manually assigned a point value, and tallied up the points, but we are concerned about mis-valuing problems, since we have in the past. These problems have a large range of difficulty, the easiest of which nearly everyone got correct, while some of the most difficult on had a single solve (by different individuals).&lt;/p&gt;\n\n&lt;p&gt;One of the proposed solutions is just to assign a point value for all problems, then simply decrease the point value by some scheme based on the number of correct solves. While that would certainly work and be fairly accurate, it seems like there should be a more elegant solution than that.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve spent some time looking for sources on how to do this, but I think a lot of the problem is I just don&amp;#39;t know the right Machine Learning terms to find what I&amp;#39;m looking for. I&amp;#39;m hoping that you all will know what I&amp;#39;m talking about enough to point me in the right direction.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Options for Ranking/difficulty","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413305071,"url":"http://www.reddit.com/r/MachineLearning/comments/14jzjw/options_for_rankingdifficulty/","subreddit_id":"t5_2r3gv","created_utc":1355075412,"downs":0,"author_flair_css_class":null,"selftext":"I've been tasked with coming up with a way of ranking the difficulty of elements in a problem set. We have a set of about 30 \"problems\", which we will get a binary correct/incorrect outcome from when an individual goes through the problem set. We also have data now for about 300 individuals going through this set.\n\nThe end result is we want a weighting for each of these problems, based on its difficulty, so that we can decide which did best. In earlier versions we have simply manually assigned a point value, and tallied up the points, but we are concerned about mis-valuing problems, since we have in the past. These problems have a large range of difficulty, the easiest of which nearly everyone got correct, while some of the most difficult on had a single solve (by different individuals).\n\nOne of the proposed solutions is just to assign a point value for all problems, then simply decrease the point value by some scheme based on the number of correct solves. While that would certainly work and be fairly accurate, it seems like there should be a more elegant solution than that.\n\nI've spent some time looking for sources on how to do this, but I think a lot of the problem is I just don't know the right Machine Learning terms to find what I'm looking for. I'm hoping that you all will know what I'm talking about enough to point me in the right direction.","thumbnail":"self","secure_media":null,"id":"14jzjw","is_self":true,"edited":false,"ups":3,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/14jzjw/options_for_rankingdifficulty/"}
{"thumbnail":"default","secure_media":null,"secure_media_embed":{},"media":null,"ups":0,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/14jh2m/what_are_good_masters_programs_for_ml/","id":"14jh2m","is_self":true,"edited":false,"domain":"self.MachineLearning","user_reports":[],"author":"[deleted]","score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m applying right now and there aren&amp;#39;t any definitive lists out there.  It&amp;#39;s proving quite difficult to find programs.  Schools that are constantly thrown around on this subreddit often don&amp;#39;t offer an M.S. in Machine Learning, ie. CMU (you have to be a Ph.D. to get their M.S. in ML), and Googling &amp;#39;Master in Machine Learning&amp;quot; turns up very little.\nAny help would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"mod_reports":[],"num_comments":0,"retrieved_on":1413305818,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14jh2m/what_are_good_masters_programs_for_ml/","created_utc":1355040487,"selftext":"I'm applying right now and there aren't any definitive lists out there.  It's proving quite difficult to find programs.  Schools that are constantly thrown around on this subreddit often don't offer an M.S. in Machine Learning, ie. CMU (you have to be a Ph.D. to get their M.S. in ML), and Googling 'Master in Machine Learning\" turns up very little.\nAny help would be greatly appreciated.","author_flair_css_class":null,"downs":0,"author_flair_text":null,"link_flair_css_class":null,"title":"What are good masters programs for ML?","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null}
{"thumbnail":"self","secure_media":null,"secure_media_embed":{},"ups":7,"report_reasons":null,"media":null,"stickied":false,"over_18":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/14j890/what_happens_when_you_remove_the_support_vectors/","id":"14j890","edited":false,"is_self":true,"domain":"self.MachineLearning","author":"kripaks","user_reports":[],"score":7,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say I trained an SVM model on some training data. I then  remove the support vectors from the training data and re-train the model. The new model will have new support vectors. But, what all has changed in this new model? Can I confidently say that the new model performs worse than the previous one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"num_comments":13,"retrieved_on":1413306182,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14j890/what_happens_when_you_remove_the_support_vectors/","created_utc":1355028195,"author_flair_css_class":null,"selftext":"Lets say I trained an SVM model on some training data. I then  remove the support vectors from the training data and re-train the model. The new model will have new support vectors. But, what all has changed in this new model? Can I confidently say that the new model performs worse than the previous one?","downs":0,"author_flair_text":null,"link_flair_css_class":null,"banned_by":null,"title":"What happens when you remove the support vectors from the training data and re-train?","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null}
{"permalink":"/r/MachineLearning/comments/14j5hz/specialist_knowledge_is_useless_and_unhelpful/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":2,"media":null,"report_reasons":null,"edited":false,"is_self":false,"id":"14j5hz","secure_media":null,"thumbnail":"default","author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1355025114,"subreddit_id":"t5_2r3gv","url":"http://www.slate.com/articles/health_and_science/new_scientist/2012/12/kaggle_president_jeremy_howard_amateurs_beat_specialists_in_data_prediction.html","retrieved_on":1413306292,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Specialist knowledge is useless and unhelpful: When data prediction is a game, the experts lose out","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"score":2,"gilded":0,"selftext_html":null,"domain":"slate.com","user_reports":[],"author":"meyamashi","num_comments":0,"mod_reports":[]}
{"subreddit":"MachineLearning","title":"predicting energy usage question","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1355178534,"downs":0,"author_flair_css_class":null,"selftext":"Hi /r/machinelearning\n\nI was wondering if you folks might have any suggestions on a pet project that I've been thinking about. Also, if I'm barking up the wrong subreddit, please inform me.\n\nAnyways, I have my hands on data for thousands of households' energy usage for discrete fifteen minute time intervals over ~1+ years of time. I also have data like 'how many sq ft is the house' and 'what was the weather on whichever day' and some other interesting things.\n\nI was wondering if there might be some cool methods to implement in order to try to predict future energy usage amounts on a household basis. Something where I could say \"This household's energy usage for the past xxxx 15 minute blocks of time looks like this. I predict that in the next 15 minutes they will use y kWh.\"\n\nI don't know too much about ML, but stuff like Markov Chains pop into my mind because I have discrete events. Unfortunately, my values for the events aren't very discrete and furthermore they are ordinal. I've never seen a MC that deals with this kind of data (then again, I have a really, really naive sense of MCs).\n\nI'm a decent programmer and mathematician, but haven't forayed into ML ever. I'd love to learn more if you all have any suggestions on how to approach this problem.\n\nThanks!","retrieved_on":1413301432,"url":"http://www.reddit.com/r/MachineLearning/comments/14mnks/predicting_energy_usage_question/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":14,"gilded":0,"score":8,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/machinelearning\"&gt;/r/machinelearning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was wondering if you folks might have any suggestions on a pet project that I&amp;#39;ve been thinking about. Also, if I&amp;#39;m barking up the wrong subreddit, please inform me.&lt;/p&gt;\n\n&lt;p&gt;Anyways, I have my hands on data for thousands of households&amp;#39; energy usage for discrete fifteen minute time intervals over ~1+ years of time. I also have data like &amp;#39;how many sq ft is the house&amp;#39; and &amp;#39;what was the weather on whichever day&amp;#39; and some other interesting things.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there might be some cool methods to implement in order to try to predict future energy usage amounts on a household basis. Something where I could say &amp;quot;This household&amp;#39;s energy usage for the past xxxx 15 minute blocks of time looks like this. I predict that in the next 15 minutes they will use y kWh.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know too much about ML, but stuff like Markov Chains pop into my mind because I have discrete events. Unfortunately, my values for the events aren&amp;#39;t very discrete and furthermore they are ordinal. I&amp;#39;ve never seen a MC that deals with this kind of data (then again, I have a really, really naive sense of MCs).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a decent programmer and mathematician, but haven&amp;#39;t forayed into ML ever. I&amp;#39;d love to learn more if you all have any suggestions on how to approach this problem.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"ireallylikedogs","user_reports":[],"domain":"self.MachineLearning","is_self":true,"edited":false,"id":"14mnks","permalink":"/r/MachineLearning/comments/14mnks/predicting_energy_usage_question/","ups":8,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"thumbnail":"self","secure_media":null}
{"domain":"self.MachineLearning","user_reports":[],"author":"Ayakalam","gilded":0,"score":10,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;So I have been studying up on the Expectation-Maximization algorithm, (EM), but I also know of Gaussian Mixture Models, (GMMs), that are used to describe data that is a Mixture of Gaussians. (MoG). &lt;/p&gt;\n\n&lt;p&gt;Some of what I will say here might be inaccurate, so please correct me, but my question is basically, what is the difference, if any, on the way the EM Algorithm works, VS the maximum likelihood algorithm in GMMs?&lt;/p&gt;\n\n&lt;p&gt;I know that in EM, we are first computing the latent variables, (probability weights if we are clustering), and then we compute the new mu&amp;#39;s and the new covariance matrix. We then repeat with the computation of the updated latent variables, etc etc. &lt;/p&gt;\n\n&lt;p&gt;So I get that, but in GMMs, arent we also kinda doing the same thing? How does this differ from how we would compute the parameters of a GMM for specific data?&lt;/p&gt;\n\n&lt;p&gt;If there is a difference between computation of parameters of GMM and how EM-algorithm works, what is it? Are they the same thing? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":27,"mod_reports":[],"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14lx67/is_there_a_difference_between_computing/","retrieved_on":1413302441,"selftext":"\nHey all, \n\nSo I have been studying up on the Expectation-Maximization algorithm, (EM), but I also know of Gaussian Mixture Models, (GMMs), that are used to describe data that is a Mixture of Gaussians. (MoG). \n\nSome of what I will say here might be inaccurate, so please correct me, but my question is basically, what is the difference, if any, on the way the EM Algorithm works, VS the maximum likelihood algorithm in GMMs?\n\nI know that in EM, we are first computing the latent variables, (probability weights if we are clustering), and then we compute the new mu's and the new covariance matrix. We then repeat with the computation of the updated latent variables, etc etc. \n\nSo I get that, but in GMMs, arent we also kinda doing the same thing? How does this differ from how we would compute the parameters of a GMM for specific data?\n\nIf there is a difference between computation of parameters of GMM and how EM-algorithm works, what is it? Are they the same thing? \n\nThanks!","author_flair_css_class":null,"downs":0,"created_utc":1355155753,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Is there a difference between computing parameters of GMMs and how EM-Algorithm works?","subreddit":"MachineLearning","secure_media":null,"thumbnail":"self","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":10,"media":null,"permalink":"/r/MachineLearning/comments/14lx67/is_there_a_difference_between_computing/","id":"14lx67","edited":false,"is_self":true}
{"retrieved_on":1413303241,"url":"http://www.reddit.com/r/MachineLearning/comments/14lc23/looking_for_this_kind_of_data/","subreddit_id":"t5_2r3gv","created_utc":1355120923,"downs":0,"author_flair_css_class":null,"selftext":"\nHey all, \n\nSo I am looking for a particular kind of data: \n\nWhat I would like to have, it higher than 3-d data, but where we know before hand the number of clusters that exist. \n\nThrough google I have already found a number of data sets that I can download and experiment on, but finding this particular combination has proven to be harder, so I am asking here in case anyone has this type of data. \n\nSo basically, any higher than 3-dimensional data, 5, 10, 20, 100-dimensional data, where we know before hand that there are 3, 8, 10, whatever, number of clusters. \n\nAnyone know where I can get my hands on this? \n\nWould really appreciate it, thanks! ","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Looking for this kind of data...","banned_by":null,"distinguished":null,"link_flair_text":null,"user_reports":[],"author":"Ayakalam","domain":"self.MachineLearning","score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;So I am looking for a particular kind of data: &lt;/p&gt;\n\n&lt;p&gt;What I would like to have, it higher than 3-d data, but where we know before hand the number of clusters that exist. &lt;/p&gt;\n\n&lt;p&gt;Through google I have already found a number of data sets that I can download and experiment on, but finding this particular combination has proven to be harder, so I am asking here in case anyone has this type of data. &lt;/p&gt;\n\n&lt;p&gt;So basically, any higher than 3-dimensional data, 5, 10, 20, 100-dimensional data, where we know before hand that there are 3, 8, 10, whatever, number of clusters. &lt;/p&gt;\n\n&lt;p&gt;Anyone know where I can get my hands on this? &lt;/p&gt;\n\n&lt;p&gt;Would really appreciate it, thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"num_comments":19,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/14lc23/looking_for_this_kind_of_data/","id":"14lc23","edited":false,"is_self":true,"thumbnail":"self","secure_media":null}
{"user_reports":[],"author":"amaboura","domain":"self.MachineLearning","score":9,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"mod_reports":[],"num_comments":8,"retrieved_on":1413298447,"url":"http://www.reddit.com/r/MachineLearning/comments/14ouu8/question_machine_learning_resources/","subreddit_id":"t5_2r3gv","created_utc":1355265747,"downs":0,"selftext":"","author_flair_css_class":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"[Question] machine learning resources","distinguished":null,"link_flair_text":null,"thumbnail":"default","secure_media":null,"ups":9,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/14ouu8/question_machine_learning_resources/","id":"14ouu8","is_self":true,"edited":false}
{"url":"https://plus.google.com/u/0/communities/104673320232127474190","subreddit_id":"t5_2r3gv","retrieved_on":1413296788,"downs":0,"selftext":"","author_flair_css_class":null,"created_utc":1355318894,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Data Science Community on G+ ","user_reports":[],"author":"mralexlin","domain":"plus.google.com","score":16,"gilded":0,"selftext_html":null,"num_comments":1,"mod_reports":[],"media_embed":{},"over_18":false,"stickied":false,"ups":16,"media":null,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/14q2n3/data_science_community_on_g/","id":"14q2n3","edited":false,"is_self":false,"secure_media":null,"thumbnail":"http://e.thumbs.redditmedia.com/IA-E_MHq8xWW5TuF.jpg"}
{"secure_media_embed":{},"media":null,"ups":1,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/14pufy/our_bricks_can_float_on_the_water_do_you_believe/","id":"14pufy","is_self":false,"edited":false,"thumbnail":"default","secure_media":null,"retrieved_on":1413297092,"subreddit_id":"t5_2r3gv","url":"http://www.sourceinfratech.com","created_utc":1355301925,"selftext":"","author_flair_css_class":null,"downs":0,"author_flair_text":null,"link_flair_css_class":null,"banned_by":null,"title":"our bricks can float on the water do you believe?\nvisit our site","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"domain":"sourceinfratech.com","user_reports":[],"author":"sourceinfratech","score":1,"gilded":0,"selftext_html":null,"mod_reports":[],"num_comments":0}
{"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Cat!","url":"http://abstrusegoose.com/496","subreddit_id":"t5_2r3gv","retrieved_on":1413292742,"downs":0,"selftext":"","author_flair_css_class":null,"created_utc":1355435161,"num_comments":16,"mod_reports":[],"author":"RichKatz","user_reports":[],"domain":"abstrusegoose.com","gilded":0,"score":58,"selftext_html":null,"id":"14t2aj","edited":false,"is_self":false,"media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":58,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/14t2aj/cat/","secure_media":null,"thumbnail":"http://f.thumbs.redditmedia.com/fvfekSmn7EJP-P6e.jpg"}
{"thumbnail":"http://c.thumbs.redditmedia.com/j4XWLiKDHkDlIayJ.jpg","secure_media":null,"edited":false,"is_self":false,"id":"14si0a","permalink":"/r/MachineLearning/comments/14si0a/the_naive_bayesian_approach_to_machine_learning/","media":null,"ups":1,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"mod_reports":[],"num_comments":0,"gilded":0,"score":1,"selftext_html":null,"author":"broccolilettuce","user_reports":[],"domain":"bayesianthink.blogspot.com","subreddit":"MachineLearning","banned_by":null,"title":"The Naive Bayesian Approach to Machine Learning","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1355416884,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413293509,"url":"http://bayesianthink.blogspot.com/2012/12/the-naive-bayesian-approach-to-machine.html","subreddit_id":"t5_2r3gv"}
{"secure_media":null,"thumbnail":"default","edited":false,"is_self":false,"id":"14sgv9","permalink":"/r/MachineLearning/comments/14sgv9/machine_repairs_adelaide/","media_embed":{},"over_18":false,"stickied":false,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"num_comments":1,"mod_reports":[],"score":1,"gilded":0,"selftext_html":null,"author":"jadewebber78","user_reports":[],"domain":"scribd.com","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Machine Repairs Adelaide","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"downs":0,"author_flair_css_class":null,"selftext":"","created_utc":1355415758,"url":"http://www.scribd.com/doc/116698891","subreddit_id":"t5_2r3gv","retrieved_on":1413293551}
{"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14rmwg/andrew_ngs_class_vs_tom_mitchells_class/","retrieved_on":1413294693,"author_flair_css_class":null,"selftext":"I am a sophomore in CS and have taken Thrun's Intro to AI. I have good basics in linear algebra, probability and some basic stats. I want to take a ML class to learn about Machine Learning techniques to a point where it is intuitive and I can actually apply the techniques if I choose to work in a ML lab. \n\n\nThe most popular class seems to be Andrew Ng's ML class. I also came across CMU's Tom Mitchell's ML class. I'd like your opinion on which class would give me an intuitive understanding of the techniques in ML and prepare me to start off with research in ML. There are many labs doing AI and ML at the school I go to.\n\nHere's the link to Tom Mitchell's class videos and assignments: https://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml\n\n[EIDT] Sorry I should have made it clear that I am looking at Andrew Ng's Stanford class that was taken by Stanford students. They are lecture videos. I prefer this as many people have said the Coursera class is dumbed down.","downs":0,"created_utc":1355371696,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"Andrew Ng's class vs Tom Mitchell's class","banned_by":null,"subreddit":"MachineLearning","domain":"self.MachineLearning","author":"bachiki","user_reports":[],"score":19,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a sophomore in CS and have taken Thrun&amp;#39;s Intro to AI. I have good basics in linear algebra, probability and some basic stats. I want to take a ML class to learn about Machine Learning techniques to a point where it is intuitive and I can actually apply the techniques if I choose to work in a ML lab. &lt;/p&gt;\n\n&lt;p&gt;The most popular class seems to be Andrew Ng&amp;#39;s ML class. I also came across CMU&amp;#39;s Tom Mitchell&amp;#39;s ML class. I&amp;#39;d like your opinion on which class would give me an intuitive understanding of the techniques in ML and prepare me to start off with research in ML. There are many labs doing AI and ML at the school I go to.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to Tom Mitchell&amp;#39;s class videos and assignments: &lt;a href=\"https://www.cs.cmu.edu/%7Etom/10701_sp11/lectures.shtml\"&gt;https://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;[EIDT] Sorry I should have made it clear that I am looking at Andrew Ng&amp;#39;s Stanford class that was taken by Stanford students. They are lecture videos. I prefer this as many people have said the Coursera class is dumbed down.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"num_comments":10,"mod_reports":[],"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"media":null,"ups":19,"report_reasons":null,"permalink":"/r/MachineLearning/comments/14rmwg/andrew_ngs_class_vs_tom_mitchells_class/","id":"14rmwg","is_self":true,"edited":1355378912,"secure_media":null,"thumbnail":"self"}
{"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Machine Repairs Adelaide","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1355497408,"subreddit_id":"t5_2r3gv","url":"http://www.scribd.com/doc/116699738","retrieved_on":1413290914,"num_comments":3,"mod_reports":[],"score":0,"gilded":0,"selftext_html":null,"domain":"scribd.com","author":"jadewebber78","user_reports":[],"edited":false,"is_self":false,"id":"14uey7","permalink":"/r/MachineLearning/comments/14uey7/machine_repairs_adelaide/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"media":null,"ups":0,"report_reasons":null,"secure_media":null,"thumbnail":"default"}
{"mod_reports":[],"num_comments":3,"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I apologize if this is the wrong subreddit to post to, but I was wondering if anyone is familiar with a variational Bayes algorithm to perform inference for either a general Boltzmann machine, or on the restricted Boltzmann machine?  Any help is greatly appreciated!  &lt;/p&gt;\n\n&lt;p&gt;p.s. Also, has anyone seen a penalized Boltzmann machine (e.g. where the weights between layers have some sort of Lq norm on them? I&amp;#39;ve done a literature search, and can&amp;#39;t seem to find anything along these lines.) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"magic_beans47","user_reports":[],"title":"variational Bayes approximation of Boltzmann machine (or restricted Boltzmann machine)?","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1355460699,"author_flair_css_class":null,"selftext":"I apologize if this is the wrong subreddit to post to, but I was wondering if anyone is familiar with a variational Bayes algorithm to perform inference for either a general Boltzmann machine, or on the restricted Boltzmann machine?  Any help is greatly appreciated!  \n\np.s. Also, has anyone seen a penalized Boltzmann machine (e.g. where the weights between layers have some sort of Lq norm on them? I've done a literature search, and can't seem to find anything along these lines.) ","downs":0,"retrieved_on":1413291716,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14ttdv/variational_bayes_approximation_of_boltzmann/","thumbnail":"self","secure_media":null,"is_self":true,"edited":false,"id":"14ttdv","permalink":"/r/MachineLearning/comments/14ttdv/variational_bayes_approximation_of_boltzmann/","secure_media_embed":{},"ups":1,"media":null,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{}}
{"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Moravec's_paradox - Evolution based explanation on why perceptual tasks are hard and cognitive ones are easy","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1355611321,"subreddit_id":"t5_2r3gv","url":"http://en.wikipedia.org/wiki/Moravec's_paradox","retrieved_on":1413287583,"num_comments":2,"mod_reports":[],"gilded":0,"score":35,"selftext_html":null,"domain":"en.wikipedia.org","author":"rrenaud","user_reports":[],"is_self":false,"edited":false,"id":"14wwx1","permalink":"/r/MachineLearning/comments/14wwx1/moravecs_paradox_evolution_based_explanation_on/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":35,"media":null,"secure_media":null,"thumbnail":"default"}
{"stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":5,"media":null,"permalink":"/r/MachineLearning/comments/14vq1i/question_multiple_class_decision_trees/","id":"14vq1i","is_self":true,"edited":false,"secure_media":null,"thumbnail":"self","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/14vq1i/question_multiple_class_decision_trees/","retrieved_on":1413289173,"selftext":"Hey everybody,\nI have a question about using decision trees for multiclass classification. \n\nI'm familiar with the concept of a \"One-vs-all\" approach where for N classes you would learn N models, where the *i*th model represents if an instance is classified as the *i*th class or not. This may seem silly, but my question is what do you do after you've trained those models and want to classify a new instance? What if more than one of your models say that this instance belongs in the *i*th class?\n\nThanks for reading","author_flair_css_class":null,"downs":0,"created_utc":1355545863,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"[Question] Multiple class decision trees","subreddit":"MachineLearning","domain":"self.MachineLearning","user_reports":[],"author":"badgerbro","gilded":0,"score":5,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,\nI have a question about using decision trees for multiclass classification. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m familiar with the concept of a &amp;quot;One-vs-all&amp;quot; approach where for N classes you would learn N models, where the &lt;em&gt;i&lt;/em&gt;th model represents if an instance is classified as the &lt;em&gt;i&lt;/em&gt;th class or not. This may seem silly, but my question is what do you do after you&amp;#39;ve trained those models and want to classify a new instance? What if more than one of your models say that this instance belongs in the &lt;em&gt;i&lt;/em&gt;th class?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":6,"mod_reports":[]}
{"retrieved_on":1413286114,"subreddit_id":"t5_2r3gv","url":"http://geomblog.blogspot.com/2012/11/data-dimensions-and-geometry-oh-my.html","created_utc":1355675387,"author_flair_css_class":null,"selftext":"","downs":0,"author_flair_text":null,"link_flair_css_class":null,"title":"Data Dimensions and Geometry","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"domain":"geomblog.blogspot.com","author":"reidhoch","user_reports":[],"gilded":0,"score":13,"selftext_html":null,"mod_reports":[],"num_comments":1,"secure_media_embed":{},"media":null,"ups":13,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/14y1g6/data_dimensions_and_geometry/","id":"14y1g6","edited":false,"is_self":false,"thumbnail":"default","secure_media":null}
{"created_utc":1355761392,"downs":0,"selftext":"This is the first ever regression algorithm that I'm writing for practical purposes, so I'm sorry if this seems like child's play to you guys. I'm trying to write a program that models a 3D bump function:\n\nz = f(x,y) = 1/(1+x^2 +y^2 )\n\nI've identified my cost function as:\n\nf(P1...P5) = (∑^i=1:n (z_i - (P1/(1+P2(x_i^2 +P3) + P4(y_i^2 +P5))))^2 / 2n\n\nWhere I have a data set of size n, with a z value for each integer x and y location.\nI'm trying to minimize f(P1...P5) by finding the lowest point where the partials of f(P1...P5) with respect to each P is 0. However, f_P1(P1...P5) evaluates as an expression that can never be 0. I've tried putting P1 in the denominator of the bump function, but again, the derivative involves a fraction and cannot ever be 0.\n\nAm I just bad at calculus, or is there really no way of directly evaluating the function's absolute minimum? If the latter is true, how do I minimize the cost function?\nMy math level is high school AP Calc with some linear algebra and the basics of partials.\n\noh god how ddid this get here i am not good with computer thank you for help","author_flair_css_class":null,"retrieved_on":1413283448,"url":"http://www.reddit.com/r/MachineLearning/comments/14zyec/help_new_to_machine_learning_writing_a_bump/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","title":"Help: New to machine learning, writing a Bump Function regression algorithm","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":5,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is the first ever regression algorithm that I&amp;#39;m writing for practical purposes, so I&amp;#39;m sorry if this seems like child&amp;#39;s play to you guys. I&amp;#39;m trying to write a program that models a 3D bump function:&lt;/p&gt;\n\n&lt;p&gt;z = f(x,y) = 1/(1+x&lt;sup&gt;2&lt;/sup&gt; +y&lt;sup&gt;2&lt;/sup&gt; )&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve identified my cost function as:&lt;/p&gt;\n\n&lt;p&gt;f(P1...P5) = (∑&lt;sup&gt;i=1:n&lt;/sup&gt; (z_i - (P1/(1+P2(x_i&lt;sup&gt;2&lt;/sup&gt; +P3) + P4(y_i&lt;sup&gt;2&lt;/sup&gt; +P5))))&lt;sup&gt;2&lt;/sup&gt; / 2n&lt;/p&gt;\n\n&lt;p&gt;Where I have a data set of size n, with a z value for each integer x and y location.\nI&amp;#39;m trying to minimize f(P1...P5) by finding the lowest point where the partials of f(P1...P5) with respect to each P is 0. However, f_P1(P1...P5) evaluates as an expression that can never be 0. I&amp;#39;ve tried putting P1 in the denominator of the bump function, but again, the derivative involves a fraction and cannot ever be 0.&lt;/p&gt;\n\n&lt;p&gt;Am I just bad at calculus, or is there really no way of directly evaluating the function&amp;#39;s absolute minimum? If the latter is true, how do I minimize the cost function?\nMy math level is high school AP Calc with some linear algebra and the basics of partials.&lt;/p&gt;\n\n&lt;p&gt;oh god how ddid this get here i am not good with computer thank you for help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"SlightlyReddishDawn","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":2,"permalink":"/r/MachineLearning/comments/14zyec/help_new_to_machine_learning_writing_a_bump/","ups":5,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"edited":false,"is_self":true,"id":"14zyec","thumbnail":"self","secure_media":null}
{"secure_media":null,"thumbnail":"http://c.thumbs.redditmedia.com/h_OVgCPkpPBirYzJ.jpg","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":52,"report_reasons":null,"media":null,"permalink":"/r/MachineLearning/comments/14ziao/a_list_of_data_science_and_machine_learning/","id":"14ziao","is_self":false,"edited":false,"domain":"conductrics.com","author":"worldsayshi","user_reports":[],"score":52,"gilded":0,"selftext_html":null,"num_comments":3,"mod_reports":[],"subreddit_id":"t5_2r3gv","url":"http://conductrics.com/data-science-resources/","retrieved_on":1413284082,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1355733561,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"A List of Data Science and Machine Learning Resources [x-post Hacker News]","banned_by":null,"subreddit":"MachineLearning"}
{"downs":0,"selftext":"I've been reading Machine Learning - A Probabilistic Perspective by Kevin P. Murphy.\nEach chapter features exercises and one that seems incredibly simple has me completely baffled.\n\nHere's the exercise:\n\n&gt; Probabilities are sensitive to the form of the question that was used to generate the answer.\n&gt;\n&gt; (Source: Minka.) My neighbor has two children. Assuming the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilities—two boys or two girls—have probabilities 1/4 and 1/4.\n&gt;\n&gt; a. Suppose I ask him whether he has any boys and he says yes. What is the possibility that one child is a girl?\n&gt; \n&gt;b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the porbability that the other child is a girl?\n\nAs I understand it a. and b. are identical. But they wouldn't be asking the question if that was the case.\n\nWhat am I missing?\n","author_flair_css_class":null,"created_utc":1355713618,"url":"http://www.reddit.com/r/MachineLearning/comments/14z24n/question_probabilities_sensitive_to_the_form_of/","subreddit_id":"t5_2r3gv","retrieved_on":1413284726,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"[Question] Probabilities sensitive to the form of the question?","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"score":4,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been reading Machine Learning - A Probabilistic Perspective by Kevin P. Murphy.\nEach chapter features exercises and one that seems incredibly simple has me completely baffled.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the exercise:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Probabilities are sensitive to the form of the question that was used to generate the answer.&lt;/p&gt;\n\n&lt;p&gt;(Source: Minka.) My neighbor has two children. Assuming the gender of a child is like a coin flip, it is most likely, a priori, that my neighbor has one boy and one girl, with probability 1/2. The other possibilities—two boys or two girls—have probabilities 1/4 and 1/4.&lt;/p&gt;\n\n&lt;p&gt;a. Suppose I ask him whether he has any boys and he says yes. What is the possibility that one child is a girl?&lt;/p&gt;\n\n&lt;p&gt;b. Suppose instead that I happen to see one of his children run by, and it is a boy. What is the porbability that the other child is a girl?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;As I understand it a. and b. are identical. But they wouldn&amp;#39;t be asking the question if that was the case.&lt;/p&gt;\n\n&lt;p&gt;What am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"user_reports":[],"author":"likesnoosnoo","domain":"self.MachineLearning","num_comments":4,"mod_reports":[],"permalink":"/r/MachineLearning/comments/14z24n/question_probabilities_sensitive_to_the_form_of/","media_embed":{},"stickied":false,"over_18":false,"media":null,"ups":4,"report_reasons":null,"secure_media_embed":{},"is_self":true,"edited":false,"id":"14z24n","secure_media":null,"thumbnail":"self"}
{"mod_reports":[],"num_comments":3,"user_reports":[],"author":"shaggorama","domain":"metaoptimize.com","score":29,"gilded":0,"selftext_html":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"MetaOptimize QA: The \"StackOverflow\" for Machine Learning","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413280372,"url":"http://metaoptimize.com/qa/questions/?sort=mostvoted","subreddit_id":"t5_2r3gv","created_utc":1355853444,"downs":0,"selftext":"","author_flair_css_class":null,"thumbnail":"default","secure_media":null,"id":"1527nn","is_self":false,"edited":false,"ups":29,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/1527nn/metaoptimize_qa_the_stackoverflow_for_machine/"}
{"edited":false,"is_self":true,"id":"151rrp","permalink":"/r/MachineLearning/comments/151rrp/working_on_a_libsvm_gui_how_to_get_classification/","secure_media_embed":{},"ups":7,"report_reasons":null,"media":null,"stickied":false,"over_18":false,"media_embed":{},"thumbnail":"self","secure_media":null,"title":"Working on a libSVM GUI: how to get classification border?","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1355833437,"selftext":"Hi guys,\n\nI was working on a C++ wrapper and a gui for [libsvm](http://www.csie.ntu.edu.tw/~cjlin/libsvm/). When it's ready it will be opensource.\n\nRight now, I can load data, plot it (if 2d), train a model and highlight the support vectors. [Screenshot](http://i.imgur.com/ECLmt.png).\n\nIn their example app svm-toy, the libsvm autors iterate over the whole graphic window and for every pixel they call svm_predict to see which class it belongs to, then colorize it appropriately. That works but looks ugly and i don't want to do it.\n\nSo my question? How can I find the classifying curve so I can plot it nicely? Any help from more experienced libsvm users would be greatly appreciated.\n\nThanks.","author_flair_css_class":null,"downs":0,"retrieved_on":1413280990,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/151rrp/working_on_a_libsvm_gui_how_to_get_classification/","mod_reports":[],"num_comments":9,"gilded":0,"score":7,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;I was working on a C++ wrapper and a gui for &lt;a href=\"http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/\"&gt;libsvm&lt;/a&gt;. When it&amp;#39;s ready it will be opensource.&lt;/p&gt;\n\n&lt;p&gt;Right now, I can load data, plot it (if 2d), train a model and highlight the support vectors. &lt;a href=\"http://i.imgur.com/ECLmt.png\"&gt;Screenshot&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;In their example app svm-toy, the libsvm autors iterate over the whole graphic window and for every pixel they call svm_predict to see which class it belongs to, then colorize it appropriately. That works but looks ugly and i don&amp;#39;t want to do it.&lt;/p&gt;\n\n&lt;p&gt;So my question? How can I find the classifying curve so I can plot it nicely? Any help from more experienced libsvm users would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"foolnotion"}
{"selftext_html":null,"score":1,"gilded":0,"author":"[deleted]","user_reports":[],"domain":"newscientist.com","mod_reports":[],"num_comments":0,"created_utc":1355822876,"downs":0,"selftext":"","author_flair_css_class":null,"retrieved_on":1413281169,"url":"http://www.newscientist.com/article/mg21228354.700-wolf-packs-dont-need-to-cooperate-to-make-a-kill.html","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","title":"Wolf packs","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"thumbnail":"default","secure_media":null,"permalink":"/r/MachineLearning/comments/151n1k/wolf_packs/","ups":1,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"is_self":false,"edited":false,"id":"151n1k"}
{"permalink":"/r/MachineLearning/comments/151k62/probability_of_math_category_in_ml_literature/","ups":1,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"is_self":true,"edited":false,"id":"151k62","thumbnail":"self","secure_media":null,"created_utc":1355817266,"downs":0,"selftext":"I was going to ask you guys what math (aside from linear algebra) would be most worth while, but if the question is formalized a bit better, it could make for a cool project question: could named math concepts/categories be automaticly identified in papers and books, how detailed could reliable classification be, and what are the most common math categories in the ML literature?\n\nIf 'math category' was something you'd see in the title of a powerpoint slide in a college math course, what would your bets be for the most common math categories used in published ML research?\n\n","author_flair_css_class":null,"retrieved_on":1413281283,"url":"http://www.reddit.com/r/MachineLearning/comments/151k62/probability_of_math_category_in_ml_literature/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"Probability of Math category in ML literature","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going to ask you guys what math (aside from linear algebra) would be most worth while, but if the question is formalized a bit better, it could make for a cool project question: could named math concepts/categories be automaticly identified in papers and books, how detailed could reliable classification be, and what are the most common math categories in the ML literature?&lt;/p&gt;\n\n&lt;p&gt;If &amp;#39;math category&amp;#39; was something you&amp;#39;d see in the title of a powerpoint slide in a college math course, what would your bets be for the most common math categories used in published ML research?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"SpaceWizard","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":0}
{"author":"javelinjs","user_reports":[],"domain":"self.MachineLearning","score":5,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I wonder if there exists any MapReduce-based implement for LASSO, instead of MPI-based solutions? Or if I use bagging to combine several LARS results, will it be a good idea for large-scale data?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":5,"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/15346d/question_parallel_implements_for_lasso/","subreddit_id":"t5_2r3gv","retrieved_on":1413279172,"downs":0,"selftext":"Hi guys, \n\nI wonder if there exists any MapReduce-based implement for LASSO, instead of MPI-based solutions? Or if I use bagging to combine several LARS results, will it be a good idea for large-scale data?\n\nThanks.","author_flair_css_class":null,"created_utc":1355883935,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"[Question] Parallel implements for LASSO","secure_media":null,"thumbnail":"self","media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":5,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/15346d/question_parallel_implements_for_lasso/","id":"15346d","edited":false,"is_self":true}
{"mod_reports":[],"num_comments":1,"author":"zabuplunder","user_reports":[],"domain":"imgur.com","score":0,"gilded":0,"selftext_html":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"candy floss machine","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413279286,"url":"http://imgur.com/p519c","subreddit_id":"t5_2r3gv","created_utc":1355880917,"downs":0,"author_flair_css_class":null,"selftext":"","thumbnail":"default","secure_media":null,"id":"15313g","edited":false,"is_self":false,"ups":0,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/15313g/candy_floss_machine/"}
{"score":7,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m putting together a supervised classifier, where some of the features may be absent for a subset of the data. And by absent, those features do not exists, as opposed to missing.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone had worked on building a classifier which had the same issue, what approaches they took, what was/was not successful, and any caveats to watch out for.&lt;/p&gt;\n\n&lt;p&gt;As a side note, I was doing some research today and ran across this paper published 6 years ago from Gal Chechik and others: &lt;a href=\"http://jmlr.csail.mit.edu/papers/volume9/chechik08a/chechik08a.pdf\"&gt;http://jmlr.csail.mit.edu/papers/volume9/chechik08a/chechik08a.pdf&lt;/a&gt;\nI was thinking along the same lines of classification on a subspace, and was glad to see that this was validated by the above paper.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"arcticlobo","user_reports":[],"domain":"self.MachineLearning","num_comments":3,"mod_reports":[],"downs":0,"selftext":"I'm putting together a supervised classifier, where some of the features may be absent for a subset of the data. And by absent, those features do not exists, as opposed to missing.\n\nI was wondering if anyone had worked on building a classifier which had the same issue, what approaches they took, what was/was not successful, and any caveats to watch out for.\n\nAs a side note, I was doing some research today and ran across this paper published 6 years ago from Gal Chechik and others: http://jmlr.csail.mit.edu/papers/volume9/chechik08a/chechik08a.pdf\nI was thinking along the same lines of classification on a subspace, and was glad to see that this was validated by the above paper.\n","author_flair_css_class":null,"created_utc":1356041628,"url":"http://www.reddit.com/r/MachineLearning/comments/156vyb/approaches_to_deal_with_absentnull_features_in/","subreddit_id":"t5_2r3gv","retrieved_on":1413273371,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Approaches to deal with absent/null features in classification?","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"secure_media":null,"thumbnail":"self","permalink":"/r/MachineLearning/comments/156vyb/approaches_to_deal_with_absentnull_features_in/","media_embed":{},"stickied":false,"over_18":false,"ups":7,"report_reasons":null,"media":null,"secure_media_embed":{},"is_self":true,"edited":false,"id":"156vyb"}
{"score":2,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone -- I&amp;#39;m fairly fresh into machine learning and was hoping to get some guidance...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to construct a proof of concept algorithm that would take an MP3 encoded song, and output two files of the same length: one &amp;quot;acapella&amp;quot; and one &amp;quot;instrumental&amp;quot;.  This was the first thing I thought of when reading about the Cocktail Party Algorithm.&lt;/p&gt;\n\n&lt;p&gt;I would think to approach the problem in an ML one could use a neural network, and train it with studio quality song/acapella/instrumental tuples.&lt;/p&gt;\n\n&lt;p&gt;As a beginner, the questions I have are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this idea feasible?&lt;/li&gt;\n&lt;li&gt;How might you preprocess the data (if at all?)&lt;/li&gt;\n&lt;li&gt;What do you think would be the biggest roadblock?&lt;/li&gt;\n&lt;li&gt;How much training data would be necessary?&lt;/li&gt;\n&lt;li&gt;What kind of classifiers (in addition to or in lieu of a backprop NN) would you use to increase accuracy?&lt;/li&gt;\n&lt;li&gt;Other general advice?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks all in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"drubo","num_comments":10,"mod_reports":[],"author_flair_css_class":null,"selftext":"Hi everyone -- I'm fairly fresh into machine learning and was hoping to get some guidance...\n\nI'd like to construct a proof of concept algorithm that would take an MP3 encoded song, and output two files of the same length: one \"acapella\" and one \"instrumental\".  This was the first thing I thought of when reading about the Cocktail Party Algorithm.\n\nI would think to approach the problem in an ML one could use a neural network, and train it with studio quality song/acapella/instrumental tuples.\n\nAs a beginner, the questions I have are:\n\n- Is this idea feasible?\n- How might you preprocess the data (if at all?)\n- What do you think would be the biggest roadblock?\n- How much training data would be necessary?\n- What kind of classifiers (in addition to or in lieu of a backprop NN) would you use to increase accuracy?\n- Other general advice?\n\nThanks all in advance!","downs":0,"created_utc":1356038822,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/156sn8/neural_network_for_singing_voice_separation/","retrieved_on":1413273500,"link_flair_text":null,"distinguished":null,"title":"Neural network for singing voice separation?","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"secure_media":null,"thumbnail":"self","permalink":"/r/MachineLearning/comments/156sn8/neural_network_for_singing_voice_separation/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":2,"media":null,"edited":false,"is_self":true,"id":"156sn8"}
{"banned_by":null,"title":"R+Moodle for generating customized {exams} in machine learning","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1356019964,"selftext":"","author_flair_css_class":null,"downs":0,"retrieved_on":1413274561,"subreddit_id":"t5_2r3gv","url":"http://www.r-bloggers.com/generation-of-e-learning-exams-in-r-for-moodle-olat-etc/","mod_reports":[],"num_comments":0,"score":1,"gilded":0,"selftext_html":null,"domain":"r-bloggers.com","author":"talgalili","user_reports":[],"is_self":false,"edited":false,"id":"1567oj","permalink":"/r/MachineLearning/comments/1567oj/rmoodle_for_generating_customized_exams_in/","secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"stickied":false,"over_18":false,"media_embed":{},"thumbnail":"default","secure_media":null}
{"score":16,"gilded":0,"selftext_html":null,"domain":"machinedlearnings.com","user_reports":[],"author":"rrenaud","mod_reports":[],"num_comments":0,"created_utc":1356017055,"author_flair_css_class":null,"selftext":"","downs":0,"retrieved_on":1413274664,"subreddit_id":"t5_2r3gv","url":"http://www.machinedlearnings.com/2012/12/do-you-really-have-big-data.html","banned_by":null,"title":"Do you really have big data?","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"thumbnail":"http://a.thumbs.redditmedia.com/ULxKMxXBEBvzotXH.jpg","secure_media":null,"permalink":"/r/MachineLearning/comments/15650b/do_you_really_have_big_data/","secure_media_embed":{},"report_reasons":null,"ups":16,"media":null,"stickied":false,"over_18":false,"media_embed":{},"edited":false,"is_self":false,"id":"15650b"}
{"subreddit":"MachineLearning","banned_by":null,"title":"Using satellite imagery to determine pedestrian, cyclist, car, bus modeshare for transportation planning.","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1355973014,"downs":0,"selftext":"For those of you who don't know, the current state of the art in determining the number of cyclists and pedestrians utilizing the streets is to put a volunteer on a street corner, and count them. For cars, we have those nice rubber bands you see placed across the street. Sad, right?\n\nI am hoping to improve on this situation by using the abundance of high quality satellite imagery available to count pedestrians, bicycles, and cars in a given image (just as a jumping off point) in order to determine modeshare.\n\nPutting the various issues that will eventually arise when analyzing this data, I was wondering if any of you might be able to help me find libraries that would help me perform this sort of analysis. Any sort of jumping off point, existing research, similar projects, really anything you can provide would be helpful.\n\nI am simply a web developer interested in alternative transportation and public policy who believes there must be a better way (cell phone location data would be nice, and better, but it is hard to get). The friendlier the language is with the web (python, javascript - Atwood's law anyone?, php, etc...), the better as what I would really like to build is a web service that allows policy makers to perform custom searches, and do their own analysis online.\n\nI am definitely willing to learn whatever I need to to optimize my results, but I'm starting from pretty much nothing when it comes to pattern recognition so please keep in mind I may not understand the vocabulary that goes along with it.\n\nIf it means anything to anyone, I have access to ArcGIS Server if needed. I am hoping to submit this project for my Intro to Geospatial Technologies course, due late February. I plan to use imagery from my university, RIT, as a starting point as our Imaging Science department happens to like to take lots of pictures of campus from space.","author_flair_css_class":null,"retrieved_on":1413276117,"url":"http://www.reddit.com/r/MachineLearning/comments/155arg/using_satellite_imagery_to_determine_pedestrian/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":5,"score":2,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those of you who don&amp;#39;t know, the current state of the art in determining the number of cyclists and pedestrians utilizing the streets is to put a volunteer on a street corner, and count them. For cars, we have those nice rubber bands you see placed across the street. Sad, right?&lt;/p&gt;\n\n&lt;p&gt;I am hoping to improve on this situation by using the abundance of high quality satellite imagery available to count pedestrians, bicycles, and cars in a given image (just as a jumping off point) in order to determine modeshare.&lt;/p&gt;\n\n&lt;p&gt;Putting the various issues that will eventually arise when analyzing this data, I was wondering if any of you might be able to help me find libraries that would help me perform this sort of analysis. Any sort of jumping off point, existing research, similar projects, really anything you can provide would be helpful.&lt;/p&gt;\n\n&lt;p&gt;I am simply a web developer interested in alternative transportation and public policy who believes there must be a better way (cell phone location data would be nice, and better, but it is hard to get). The friendlier the language is with the web (python, javascript - Atwood&amp;#39;s law anyone?, php, etc...), the better as what I would really like to build is a web service that allows policy makers to perform custom searches, and do their own analysis online.&lt;/p&gt;\n\n&lt;p&gt;I am definitely willing to learn whatever I need to to optimize my results, but I&amp;#39;m starting from pretty much nothing when it comes to pattern recognition so please keep in mind I may not understand the vocabulary that goes along with it.&lt;/p&gt;\n\n&lt;p&gt;If it means anything to anyone, I have access to ArcGIS Server if needed. I am hoping to submit this project for my Intro to Geospatial Technologies course, due late February. I plan to use imagery from my university, RIT, as a starting point as our Imaging Science department happens to like to take lots of pictures of campus from space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"youarearobot","user_reports":[],"domain":"self.MachineLearning","edited":false,"is_self":true,"id":"155arg","permalink":"/r/MachineLearning/comments/155arg/using_satellite_imagery_to_determine_pedestrian/","report_reasons":null,"ups":2,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"thumbnail":"self","secure_media":null}
{"author":"szza","user_reports":[],"domain":"self.MachineLearning","score":3,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example, if X and Y are linearly correlated at .8, are there something like confidence intervals I can put on the classification rate obtained when threshholding X to identify Y &amp;gt; k?&lt;/p&gt;\n\n&lt;p&gt;The motivation for the question comes from lots of research in the social sciences (particularly education research), where a high correlation is presented as evidence of a strong relationship. But many of the problems under consideration are best thought of as classification problems (e.g. does SAT predict college freshman grade of B+ or better? or Will Tatiana drop out of school?). So it&amp;#39;s quite possible to have a &amp;#39;strong&amp;#39; research result that&amp;#39;s essentially meaningless unless there&amp;#39;s an implication from correlation to classification.&lt;/p&gt;\n\n&lt;p&gt;A discussion of SAT as classifier can be found in my blog post &lt;a href=\"http://highered.blogspot.com/2011/09/sat-error-rates.html\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"num_comments":4,"retrieved_on":1413271673,"url":"http://www.reddit.com/r/MachineLearning/comments/1582vx/is_there_an_simple_form_for_a_relationship/","subreddit_id":"t5_2r3gv","created_utc":1356092806,"downs":0,"author_flair_css_class":null,"selftext":"For example, if X and Y are linearly correlated at .8, are there something like confidence intervals I can put on the classification rate obtained when threshholding X to identify Y &gt; k?\n\nThe motivation for the question comes from lots of research in the social sciences (particularly education research), where a high correlation is presented as evidence of a strong relationship. But many of the problems under consideration are best thought of as classification problems (e.g. does SAT predict college freshman grade of B+ or better? or Will Tatiana drop out of school?). So it's quite possible to have a 'strong' research result that's essentially meaningless unless there's an implication from correlation to classification.\n\nA discussion of SAT as classifier can be found in my blog post [here](http://highered.blogspot.com/2011/09/sat-error-rates.html).","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Is there an simple form for a relationship between correlation and classification rates?","banned_by":null,"distinguished":null,"link_flair_text":null,"thumbnail":"self","secure_media":null,"ups":3,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/1582vx/is_there_an_simple_form_for_a_relationship/","id":"1582vx","is_self":true,"edited":false}
{"num_comments":4,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any help will be much appreciated. I am already working to see how I could change my data to make it suitable for learning but unfortunately not much luck so far.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":0,"gilded":0,"author":"girl_at_school","user_reports":[],"domain":"self.MachineLearning","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Which machine learning algorithm would be ideal for this problem?","link_flair_css_class":null,"author_flair_text":null,"downs":0,"author_flair_css_class":null,"selftext":"Any help will be much appreciated. I am already working to see how I could change my data to make it suitable for learning but unfortunately not much luck so far.","created_utc":1356062136,"url":"http://www.reddit.com/r/MachineLearning/comments/157i5c/which_machine_learning_algorithm_would_be_ideal/","subreddit_id":"t5_2r3gv","retrieved_on":1413272500,"secure_media":null,"thumbnail":"self","edited":false,"is_self":true,"id":"157i5c","permalink":"/r/MachineLearning/comments/157i5c/which_machine_learning_algorithm_would_be_ideal/","media_embed":{},"stickied":false,"over_18":false,"report_reasons":null,"ups":0,"media":null,"secure_media_embed":{}}
{"edited":false,"is_self":true,"id":"15cgsi","permalink":"/r/MachineLearning/comments/15cgsi/libsvm_on_weka_gui_on_osx_problem_evaluating/","secure_media_embed":{},"report_reasons":null,"ups":1,"media":null,"stickied":false,"over_18":false,"media_embed":{},"thumbnail":"default","secure_media":null,"banned_by":null,"title":"\nlibsvm on Weka (GUI, on OSX): \"Problem Evaluating Classifier: Rand\"","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1356304672,"author_flair_css_class":null,"selftext":"After playing around to get this library to work with Weka I finally figured it out by placing the libsvm.jar in the .app/Contents/Resource/Java folder and then pointing Info.plist to it.\n\nI try to use libsvm on my data and after a few seconds it says:\n\n&gt; Problem Evaluating Classifier:    \nRand\n\nGoogling this gives me no results.  The closest thing I can find is this StackOverflow post: http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka\n\nSo I cannot figure out what is wrong with this and how to fix it.  ","downs":0,"retrieved_on":1413265473,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/15cgsi/libsvm_on_weka_gui_on_osx_problem_evaluating/","mod_reports":[],"num_comments":0,"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After playing around to get this library to work with Weka I finally figured it out by placing the libsvm.jar in the .app/Contents/Resource/Java folder and then pointing Info.plist to it.&lt;/p&gt;\n\n&lt;p&gt;I try to use libsvm on my data and after a few seconds it says:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Problem Evaluating Classifier:&lt;br/&gt;\nRand&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Googling this gives me no results.  The closest thing I can find is this StackOverflow post: &lt;a href=\"http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka\"&gt;http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I cannot figure out what is wrong with this and how to fix it.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"[deleted]"}
{"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/15ceea/libsvm_on_weka_gui_on_osx_problem_evaluating/","retrieved_on":1413265557,"author_flair_css_class":null,"selftext":"After playing around to get this library to work with Weka I finally figured it out by placing the libsvm.jar in the .app/Contents/Resource/Java folder and then pointing Info.plist to it.\n\nI try to use libsvm on my data and after a few seconds it says:\n\n&gt; Problem Evaluating Classifier:    \nRand\n\nGoogling this gives me no results.  The closest thing I can find is this StackOverflow post: http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka\n\nSo I cannot figure out what is wrong with this and how to fix it.  ","downs":0,"created_utc":1356301900,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"libsvm on Weka (GUI, on OSX): \"Problem Evaluating Classifier: Rand\"","subreddit":"MachineLearning","domain":"self.MachineLearning","author":"sculler","user_reports":[],"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After playing around to get this library to work with Weka I finally figured it out by placing the libsvm.jar in the .app/Contents/Resource/Java folder and then pointing Info.plist to it.&lt;/p&gt;\n\n&lt;p&gt;I try to use libsvm on my data and after a few seconds it says:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Problem Evaluating Classifier:&lt;br/&gt;\nRand&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Googling this gives me no results.  The closest thing I can find is this StackOverflow post: &lt;a href=\"http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka\"&gt;http://stackoverflow.com/questions/13847942/prob-evaluating-classifier-rand-in-libsvm-weka&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I cannot figure out what is wrong with this and how to fix it.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":0,"mod_reports":[],"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"permalink":"/r/MachineLearning/comments/15ceea/libsvm_on_weka_gui_on_osx_problem_evaluating/","id":"15ceea","is_self":true,"edited":false,"secure_media":null,"thumbnail":"self"}
{"num_comments":26,"mod_reports":[],"score":13,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the discussions about the &lt;a href=\"http://www.reddit.com/r/TheoryOfReddit/comments/15ijw8/are_subreddits_really_the_solution_to_eternal/\"&gt;quality1&lt;/a&gt; &lt;a href=\"http://www.reddit.com/r/TheoryOfReddit/comments/15goza/is_reddit_experiencing_a_brain_drain_of_sorts_or/\"&gt;loss2&lt;/a&gt; of reddit submissions and comments, I thought about a possible way of changing the voting system.&lt;br/&gt;\nAs a user, you currently have one upvote/downvote. This is obviously problematic, because not everybody views the same thing as good. You would have to somehow categorize what you like in different areas, so multidimensional upvotes? Sounds like a bad idea.&lt;br/&gt;\nBut you don&amp;#39;t have to present this to the user. So, keep the current upvote/downvote arrows, but instead of directly adding them up, give posts an n-dimensional vector as rating. The user will recieve one as well, bound to his account and describing what he likes. The system can then compare the two vectors, a high similarity should indicate content the user wants to see, this will give it a high rating for this user. This could be done by projecting the vector of the post onto that of the user and then determine its length. The higher the value, the better.&lt;br/&gt;\nHow are these vectors generated? When a user votes on a post, his voting vector gets imprinted into the post vector, in the simplest case by just adding them up. At the same time, the post vector gets imprinted onto the user vector, for example by norming the length of the post vector to one and then taking the weighted average. To help set things up, a small random offset should be added to he user vector before adding it into an upvote.&lt;br/&gt;\nThis approach has of course some implementation problems, namely the by far increased computational costs (I don&amp;#39;t know how much they take for the current reddit system).   &lt;/p&gt;\n\n&lt;p&gt;I hope this is understandable, I am pretty much just an interested layman in machinelearning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"Taonyl","link_flair_text":null,"distinguished":null,"title":"A new reddit voting system?","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"With the discussions about the [quality1](http://www.reddit.com/r/TheoryOfReddit/comments/15ijw8/are_subreddits_really_the_solution_to_eternal/) [loss2](http://www.reddit.com/r/TheoryOfReddit/comments/15goza/is_reddit_experiencing_a_brain_drain_of_sorts_or/) of reddit submissions and comments, I thought about a possible way of changing the voting system.    \nAs a user, you currently have one upvote/downvote. This is obviously problematic, because not everybody views the same thing as good. You would have to somehow categorize what you like in different areas, so multidimensional upvotes? Sounds like a bad idea.    \nBut you don't have to present this to the user. So, keep the current upvote/downvote arrows, but instead of directly adding them up, give posts an n-dimensional vector as rating. The user will recieve one as well, bound to his account and describing what he likes. The system can then compare the two vectors, a high similarity should indicate content the user wants to see, this will give it a high rating for this user. This could be done by projecting the vector of the post onto that of the user and then determine its length. The higher the value, the better.    \nHow are these vectors generated? When a user votes on a post, his voting vector gets imprinted into the post vector, in the simplest case by just adding them up. At the same time, the post vector gets imprinted onto the user vector, for example by norming the length of the post vector to one and then taking the weighted average. To help set things up, a small random offset should be added to he user vector before adding it into an upvote.    \nThis approach has of course some implementation problems, namely the by far increased computational costs (I don't know how much they take for the current reddit system).   \n\nI hope this is understandable, I am pretty much just an interested layman in machinelearning.","downs":0,"created_utc":1356633492,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/15j2le/a_new_reddit_voting_system/","retrieved_on":1413255099,"secure_media":null,"thumbnail":"self","is_self":true,"edited":false,"id":"15j2le","permalink":"/r/MachineLearning/comments/15j2le/a_new_reddit_voting_system/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":13,"report_reasons":null,"media":null}
{"thumbnail":"self","secure_media":null,"edited":false,"is_self":true,"id":"15i0o1","permalink":"/r/MachineLearning/comments/15i0o1/can_anyone_provide_an_intuitive_explanation_of/","secure_media_embed":{},"media":null,"ups":18,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"mod_reports":[],"num_comments":7,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out how well RBMs can handle very large numbers of sparse input neurons (over &amp;gt;100k, but where all but 20 or 30 are set to 0).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":18,"gilded":0,"domain":"self.MachineLearning","author":"sanity","user_reports":[],"banned_by":null,"title":"Can anyone provide an intuitive explanation of the Restricted Boltzmann Machine learning algorithm?","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1356578704,"author_flair_css_class":null,"selftext":"I'm trying to figure out how well RBMs can handle very large numbers of sparse input neurons (over &gt;100k, but where all but 20 or 30 are set to 0).","downs":0,"retrieved_on":1413256819,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/15i0o1/can_anyone_provide_an_intuitive_explanation_of/"}
{"num_comments":4,"mod_reports":[],"user_reports":[],"author":"Omega037","domain":"self.MachineLearning","gilded":0,"score":5,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working on an &amp;quot;active learning&amp;quot; approach for jointly reducing the cost and number of labeled samples (instances) in &amp;quot;instance-based&amp;quot; methods and ensembles (specifically General Regression Neural Networks, Probabilistic Neural Networks, k-Nearest Neighbor, and ensembles based on these methods).&lt;/p&gt;\n\n&lt;p&gt;The basic idea is that active learning will be used to first select from a pool of unlabeled samples, and then once labeled, determine whether said sample should be &amp;quot;stored&amp;quot; as an instance. Alternatively, this could be done for a stream of samples, where each new sample is chosen randomly and then requires a decision to label it, and a decision to keep it or not.&lt;/p&gt;\n\n&lt;p&gt;Does this seem like it would make sense as an approach, or would it be better just to perform some kind of direct sample reduction technique (a la &amp;quot;Reduction Techniques for Instance-Based Learning Algorithms&amp;quot; by Wilson and Martizon, 2000)? Has something like this been attempted, or would it not really provide any value anyways?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Active learning to reduce the cost and number of stored instances for \"instance-based\" (memory-based) methods? (x-post with metaoptimize)","banned_by":null,"url":"http://www.reddit.com/r/MachineLearning/comments/15hqqo/active_learning_to_reduce_the_cost_and_number_of/","subreddit_id":"t5_2r3gv","retrieved_on":1413257246,"downs":0,"author_flair_css_class":null,"selftext":"I have been working on an \"active learning\" approach for jointly reducing the cost and number of labeled samples (instances) in \"instance-based\" methods and ensembles (specifically General Regression Neural Networks, Probabilistic Neural Networks, k-Nearest Neighbor, and ensembles based on these methods).\n\nThe basic idea is that active learning will be used to first select from a pool of unlabeled samples, and then once labeled, determine whether said sample should be \"stored\" as an instance. Alternatively, this could be done for a stream of samples, where each new sample is chosen randomly and then requires a decision to label it, and a decision to keep it or not.\n\nDoes this seem like it would make sense as an approach, or would it be better just to perform some kind of direct sample reduction technique (a la \"Reduction Techniques for Instance-Based Learning Algorithms\" by Wilson and Martizon, 2000)? Has something like this been attempted, or would it not really provide any value anyways?","created_utc":1356567888,"secure_media":null,"thumbnail":"self","id":"15hqqo","edited":false,"is_self":true,"media_embed":{},"stickied":false,"over_18":false,"ups":5,"report_reasons":null,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/15hqqo/active_learning_to_reduce_the_cost_and_number_of/"}
{"retrieved_on":1413251391,"url":"http://masi.cscs.lsa.umich.edu/~crshalizi/weblog/988.html","subreddit_id":"t5_2r3gv","created_utc":1356732932,"downs":0,"selftext":"","author_flair_css_class":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Learning spatio-temporal dynamics.","distinguished":null,"link_flair_text":null,"user_reports":[],"author":"qkdhfjdjdhd","domain":"masi.cscs.lsa.umich.edu","score":20,"gilded":0,"selftext_html":null,"mod_reports":[],"num_comments":3,"report_reasons":null,"ups":20,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/15le9i/learning_spatiotemporal_dynamics/","id":"15le9i","is_self":false,"edited":false,"thumbnail":"http://a.thumbs.redditmedia.com/zkJ31dI3kxF-NmsZ.jpg","secure_media":null}
{"permalink":"/r/MachineLearning/comments/15kk4x/manual_number_words_date_printing_machine_for_bag/","media":null,"ups":1,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"is_self":true,"edited":false,"id":"15kk4x","thumbnail":"default","secure_media":null,"created_utc":1356699127,"downs":0,"selftext":"","author_flair_css_class":null,"retrieved_on":1413252792,"url":"http://www.reddit.com/r/MachineLearning/comments/15kk4x/manual_number_words_date_printing_machine_for_bag/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","title":"Manual Number Words Date Printing Machine for Bag &amp; Paper &amp; Film","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"yasonjane","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":0}
{"thumbnail":"default","secure_media":null,"edited":false,"is_self":true,"id":"15kj10","permalink":"/r/MachineLearning/comments/15kj10/manual_number_words_date_printing_machine_for_bag/","report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"mod_reports":[],"num_comments":0,"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"yasonfei","user_reports":[],"domain":"self.MachineLearning","subreddit":"MachineLearning","banned_by":null,"title":"Manual Number Words Date Printing Machine for Bag &amp; Paper &amp; Film","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1356696590,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413252842,"url":"http://www.reddit.com/r/MachineLearning/comments/15kj10/manual_number_words_date_printing_machine_for_bag/","subreddit_id":"t5_2r3gv"}
{"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"[Question] Regression with many irrelevant variables?","link_flair_css_class":null,"author_flair_text":null,"downs":0,"author_flair_css_class":null,"selftext":"Hello everybody. I've got a biological datasets of ~20K vectors (samples from tissues) with ~300 features (genes), most of them binary or categorical (0..n), except a few numeric (phenotypes from the organism where the tissue are extracted). I wanted to try the hypothesis that the binary/categorical features are enough to predict the numeric phenotypes (for instance, lifespan of the organism), but after some exploratory data analysis with Weka I could not get anything clear.\n\nMost of the binary or categorical features are mutations in some genomic regions, but in many cases (&gt;90%), the value is equal to 0 (no mutation). Besides, most of this mutations are known not to produce a particular effect (i.e. they are irrelevant wrt the phenotype).\n\nI have tried linear regression and lasso (the one implemented in Matlab) with no much success. I have tried to discretize the attribute into 3/4 classes, and tried several classifiers which finally gave really bad values.\n\nShould I try an specific technique apart from the classical ones? Do you recommend me certain preprocessing techniques? Could I assume that the phenotypes have nothing to do with the genomic mutations (and therefore I am just wasting my time [and yours])? Is there any preprocessing technique I am probably missing? Do you recommend any specific bibliography dealing with similar problems? \n\nThanks for your time, redditors!","created_utc":1356789271,"url":"http://www.reddit.com/r/MachineLearning/comments/15mh7q/question_regression_with_many_irrelevant_variables/","subreddit_id":"t5_2r3gv","retrieved_on":1413249649,"num_comments":15,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody. I&amp;#39;ve got a biological datasets of ~20K vectors (samples from tissues) with ~300 features (genes), most of them binary or categorical (0..n), except a few numeric (phenotypes from the organism where the tissue are extracted). I wanted to try the hypothesis that the binary/categorical features are enough to predict the numeric phenotypes (for instance, lifespan of the organism), but after some exploratory data analysis with Weka I could not get anything clear.&lt;/p&gt;\n\n&lt;p&gt;Most of the binary or categorical features are mutations in some genomic regions, but in many cases (&amp;gt;90%), the value is equal to 0 (no mutation). Besides, most of this mutations are known not to produce a particular effect (i.e. they are irrelevant wrt the phenotype).&lt;/p&gt;\n\n&lt;p&gt;I have tried linear regression and lasso (the one implemented in Matlab) with no much success. I have tried to discretize the attribute into 3/4 classes, and tried several classifiers which finally gave really bad values.&lt;/p&gt;\n\n&lt;p&gt;Should I try an specific technique apart from the classical ones? Do you recommend me certain preprocessing techniques? Could I assume that the phenotypes have nothing to do with the genomic mutations (and therefore I am just wasting my time [and yours])? Is there any preprocessing technique I am probably missing? Do you recommend any specific bibliography dealing with similar problems? &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time, redditors!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":9,"gilded":0,"author":"alfonsoeromero","user_reports":[],"domain":"self.MachineLearning","is_self":true,"edited":false,"id":"15mh7q","permalink":"/r/MachineLearning/comments/15mh7q/question_regression_with_many_irrelevant_variables/","media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":9,"report_reasons":null,"secure_media_embed":{},"secure_media":null,"thumbnail":"self"}
{"mod_reports":[],"num_comments":0,"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","user_reports":[],"author":"yasonfei","domain":"self.MachineLearning","subreddit":"MachineLearning","banned_by":null,"title":"HK-88B Multi-function Pill Making Machine Pill Machine","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1356769245,"downs":0,"selftext":"","author_flair_css_class":null,"retrieved_on":1413250056,"url":"http://www.reddit.com/r/MachineLearning/comments/15m8w1/hk88b_multifunction_pill_making_machine_pill/","subreddit_id":"t5_2r3gv","thumbnail":"default","secure_media":null,"is_self":true,"edited":false,"id":"15m8w1","permalink":"/r/MachineLearning/comments/15m8w1/hk88b_multifunction_pill_making_machine_pill/","media":null,"ups":1,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false}
{"thumbnail":"default","secure_media":null,"id":"15m57j","is_self":true,"edited":false,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/15m57j/hk88b_multifunction_pill_making_machine_pill/","mod_reports":[],"num_comments":0,"author":"yasonjane","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"HK-88B Multi-function Pill Making Machine Pill Machine","distinguished":null,"link_flair_text":null,"retrieved_on":1413250222,"url":"http://www.reddit.com/r/MachineLearning/comments/15m57j/hk88b_multifunction_pill_making_machine_pill/","subreddit_id":"t5_2r3gv","created_utc":1356763236,"downs":0,"author_flair_css_class":null,"selftext":""}
{"subreddit":"MachineLearning","banned_by":null,"title":"Kernel Approximations for Efficient SVMs (from r/compsci)","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1356904453,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413245812,"url":"http://peekaboo-vision.blogspot.co.uk/2012/12/kernel-approximations-for-efficient.html","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":0,"score":29,"selftext_html":null,"gilded":0,"user_reports":[],"author":"yggdrasilly","domain":"peekaboo-vision.blogspot.co.uk","is_self":false,"edited":false,"id":"15ov5d","permalink":"/r/MachineLearning/comments/15ov5d/kernel_approximations_for_efficient_svms_from/","ups":29,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"thumbnail":"http://a.thumbs.redditmedia.com/Sq1H6Z9ybJNee-vx.jpg","secure_media":null}
{"id":"15qw1k","edited":false,"is_self":true,"secure_media_embed":{},"report_reasons":null,"ups":0,"media":null,"stickied":false,"over_18":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/15qw1k/question_about_continuous_bayesian_inference/","thumbnail":"self","secure_media":null,"author_flair_text":null,"link_flair_css_class":null,"title":"Question about continuous bayesian inference: checking whether a coin is fair with a liar's help","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"retrieved_on":1413242655,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/15qw1k/question_about_continuous_bayesian_inference/","created_utc":1356994813,"author_flair_css_class":null,"selftext":"I came across this while working on a persona project. Though the actual problem has to do with estimating article quality on a reddit-like site I think it's best way to explain it like this: \n\nI have a biased coin that comes up heads X% of the time. However I don't know the value X. I know that if I flip it myself I can estimate the probability of any given bias X by using [posterior probability density function](http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair).\n\nHowever, let's say that I can't look at the coin when it lands. I have to have an untrustworthy friend check for me. I know my friend will lie to me Y% of the time. Now obviously if he lies exactly 50% of the time I'll never be able to determine anything about the coin. But if Y !=50% then I can. \n\nGiven a prior probably density for the coin Pr(X) and a probability density for the trustworthiness of my friend Pr(Y) how can I determine the new probability density for the coin after my friend has reported that he sees it land heads?","downs":0,"mod_reports":[],"num_comments":6,"domain":"self.MachineLearning","author":"dominosci","user_reports":[],"gilded":0,"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across this while working on a persona project. Though the actual problem has to do with estimating article quality on a reddit-like site I think it&amp;#39;s best way to explain it like this: &lt;/p&gt;\n\n&lt;p&gt;I have a biased coin that comes up heads X% of the time. However I don&amp;#39;t know the value X. I know that if I flip it myself I can estimate the probability of any given bias X by using &lt;a href=\"http://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair\"&gt;posterior probability density function&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;However, let&amp;#39;s say that I can&amp;#39;t look at the coin when it lands. I have to have an untrustworthy friend check for me. I know my friend will lie to me Y% of the time. Now obviously if he lies exactly 50% of the time I&amp;#39;ll never be able to determine anything about the coin. But if Y !=50% then I can. &lt;/p&gt;\n\n&lt;p&gt;Given a prior probably density for the coin Pr(X) and a probability density for the trustworthiness of my friend Pr(Y) how can I determine the new probability density for the coin after my friend has reported that he sees it land heads?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"author":"qkdhfjdjdhd","user_reports":[],"domain":"arxiv.org","gilded":0,"score":39,"selftext_html":null,"mod_reports":[],"num_comments":4,"retrieved_on":1413244354,"url":"http://arxiv.org/pdf/1006.3868v4.pdf","subreddit_id":"t5_2r3gv","created_utc":1356940666,"downs":0,"author_flair_css_class":null,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Interesting article by Gelman &amp; Shalizi about foundations of Bayesian inference.","banned_by":null,"distinguished":null,"link_flair_text":null,"thumbnail":"default","secure_media":null,"report_reasons":null,"ups":39,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/15prv8/interesting_article_by_gelman_shalizi_about/","id":"15prv8","is_self":false,"edited":false}
{"thumbnail":"default","secure_media":null,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/15p968/gaussian_processes_in_machine_learning/","id":"15p968","is_self":false,"edited":false,"author":"qkdhfjdjdhd","user_reports":[],"domain":"cs.ubc.ca","gilded":0,"score":1,"selftext_html":null,"mod_reports":[],"num_comments":0,"retrieved_on":1413245200,"url":"http://www.cs.ubc.ca/~hutter/EARG.shtml/earg/papers05/rasmussen_gps_in_ml.pdf","subreddit_id":"t5_2r3gv","created_utc":1356918910,"downs":0,"author_flair_css_class":null,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Gaussian Processes in Machine Learning","banned_by":null,"distinguished":null,"link_flair_text":null}
{"permalink":"/r/MachineLearning/comments/15p80b/why_visualization_cannot_afford_ignoring_data/","media":null,"ups":0,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"edited":false,"is_self":false,"id":"15p80b","thumbnail":"http://b.thumbs.redditmedia.com/RlBMLuEucqZkXy87.jpg","secure_media":null,"created_utc":1356917720,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413245256,"url":"http://fellinlovewithdata.com/reflections/why-visualization-cannot-afford-ignoring-data-mining-and-vice-versa","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"Why Visualization Cannot Afford Ignoring Data Mining and Vice Versa","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"selftext_html":null,"score":0,"gilded":0,"author":"rrenaud","user_reports":[],"domain":"fellinlovewithdata.com","mod_reports":[],"num_comments":0}
