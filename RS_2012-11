{"num_comments":10,"retrieved_on":1413443785,"permalink":"/r/MachineLearning/comments/12gpgd/correlation_between_numerical_and_categorical_data/","gilded":0,"author":"WayUpLow","media":null,"report_reasons":null,"created_utc":1351787248,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"Is there any way to find the correlation between two fields when one of them is categorical and the other is numerical? \n\nAny advice and/or resources appreciated.","author_flair_text":null,"score":2,"secure_media_embed":{},"ups":2,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any way to find the correlation between two fields when one of them is categorical and the other is numerical? &lt;/p&gt;\n\n&lt;p&gt;Any advice and/or resources appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","title":"Correlation between numerical and categorical data","is_self":true,"edited":false,"link_flair_text":null,"id":"12gpgd","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/12gpgd/correlation_between_numerical_and_categorical_data/","mod_reports":[],"media_embed":{},"over_18":false}
{"media_embed":{},"mod_reports":[],"url":"https://github.com/johnmyleswhite/BanditsBook","id":"12gnps","user_reports":[],"over_18":false,"is_self":false,"title":"Python code for the book Bandit Algorithms for Website Optimization","edited":false,"link_flair_text":null,"author_flair_css_class":null,"secure_media":null,"domain":"github.com","secure_media_embed":{},"ups":12,"selftext_html":null,"stickied":false,"thumbnail":"http://c.thumbs.redditmedia.com/qXf13PkX_FTS-heD.jpg","distinguished":null,"downs":0,"gilded":0,"permalink":"/r/MachineLearning/comments/12gnps/python_code_for_the_book_bandit_algorithms_for/","author":"cavedave","retrieved_on":1413443847,"num_comments":3,"author_flair_text":"naive","score":12,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","media":null,"created_utc":1351785557,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv"}
{"ups":1,"secure_media_embed":{},"author_flair_css_class":null,"domain":"waltherpragerandphilosophy.blogspot.com","secure_media":null,"distinguished":null,"downs":0,"selftext_html":null,"thumbnail":"default","stickied":false,"retrieved_on":1413444277,"num_comments":0,"permalink":"/r/MachineLearning/comments/12gbqb/the_how/","gilded":0,"author":"[deleted]","subreddit":"MachineLearning","banned_by":null,"created_utc":1351770613,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"score":1,"author_flair_text":null,"link_flair_css_class":null,"selftext":"","url":"http://waltherpragerandphilosophy.blogspot.com/2012/04/note-on-learning.html","user_reports":[],"id":"12gbqb","media_embed":{},"mod_reports":[],"over_18":false,"title":"the 'how'","is_self":false,"link_flair_text":null,"edited":false}
{"over_18":false,"user_reports":[],"id":"12jfcl","url":"http://www.reddit.com/r/MachineLearning/comments/12jfcl/do_deep_networks_require_homogeneous_data/","mod_reports":[],"media_embed":{},"edited":false,"link_flair_text":null,"title":"Do deep networks require homogeneous data?","is_self":true,"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very interested in the emerging field of deep learning. One thing I&amp;#39;ve noticed, though, is that most of the example applications involved homogeneous input data. &lt;/p&gt;\n\n&lt;p&gt;What I mean is that each feature if the same &amp;quot;kind&amp;quot; of value. For example, many applications involve images or speech signals. And in an image each feature is a pixel. In an audio segment each feature is an amplitude sample. They&amp;#39;re all the same kind of value. &lt;/p&gt;\n\n&lt;p&gt;Even in the winners of Merck competition were faced with features that were homogeneous (as far as I could tell).&lt;/p&gt;\n\n&lt;p&gt;So my question is: is homogeneousness of the features a requirement for deep learning? Are there examples of people successfully using completely heterogenous features for deep learning? Is deep learning particularly good for large feature sets?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":5,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"report_reasons":null,"created_utc":1351897832,"subreddit_id":"t5_2r3gv","banned_by":null,"media":null,"subreddit":"MachineLearning","selftext":"I'm very interested in the emerging field of deep learning. One thing I've noticed, though, is that most of the example applications involved homogeneous input data. \n\nWhat I mean is that each feature if the same \"kind\" of value. For example, many applications involve images or speech signals. And in an image each feature is a pixel. In an audio segment each feature is an amplitude sample. They're all the same kind of value. \n\nEven in the winners of Merck competition were faced with features that were homogeneous (as far as I could tell).\n\nSo my question is: is homogeneousness of the features a requirement for deep learning? Are there examples of people successfully using completely heterogenous features for deep learning? Is deep learning particularly good for large feature sets?","link_flair_css_class":null,"score":5,"author_flair_text":null,"num_comments":8,"retrieved_on":1413440120,"gilded":0,"permalink":"/r/MachineLearning/comments/12jfcl/do_deep_networks_require_homogeneous_data/","author":"rudyl313"}
{"author_flair_text":null,"score":0,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","media":null,"created_utc":1351882494,"report_reasons":null,"subreddit_id":"t5_2r3gv","banned_by":null,"gilded":0,"permalink":"/r/MachineLearning/comments/12izoy/facebook_is_hiring_earn_an_interview_by/","author":"[deleted]","retrieved_on":1413440719,"num_comments":0,"selftext_html":null,"stickied":false,"thumbnail":"default","distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"kaggle.com","secure_media_embed":{},"ups":0,"edited":false,"link_flair_text":null,"is_self":false,"title":"Facebook is hiring - earn an interview by performing well in a Kaggle competition","over_18":false,"media_embed":{},"mod_reports":[],"url":"https://www.kaggle.com/c/facebook-ii","user_reports":[],"id":"12izoy"}
{"id":"12iuri","user_reports":[],"url":"http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/","mod_reports":[],"media_embed":{},"over_18":false,"title":"How deep learning on GPUs wins datamining contest without feature engineering","is_self":false,"edited":false,"link_flair_text":null,"secure_media_embed":{},"ups":51,"secure_media":null,"domain":"blog.kaggle.com","author_flair_css_class":null,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"default","selftext_html":null,"num_comments":11,"retrieved_on":1413440906,"gilded":0,"author":"allegro_con_fuoco","permalink":"/r/MachineLearning/comments/12iuri/how_deep_learning_on_gpus_wins_datamining_contest/","media":null,"created_utc":1351877771,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":51}
{"secure_media":null,"domain":"reddit.com","author_flair_css_class":null,"secure_media_embed":{},"ups":7,"thumbnail":"default","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"gilded":0,"permalink":"/r/MachineLearning/comments/12iuk0/is_the_logsumexp_trick_a_valid_alternative_to/","author":"roger_","num_comments":4,"retrieved_on":1413440915,"link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":7,"media":null,"created_utc":1351877589,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","mod_reports":[],"media_embed":{},"id":"12iuk0","user_reports":[],"url":"http://www.reddit.com/r/compsci/comments/12hfrn/is_the_logsumexp_trick_a_valid_substitute_over/","over_18":false,"is_self":false,"title":"Is the log-sum-exp \"trick\" a valid alternative to scaling when trying to avoid numerical underflow in HMM calculations? [x-post from /r/compsci]","edited":false,"link_flair_text":null}
{"over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/12kt19/question_about_the_likelihood_function_for_a/","id":"12kt19","user_reports":[],"media_embed":{},"mod_reports":[],"edited":1351975423,"link_flair_text":null,"title":"Question about the likelihood function for a semi-Markov model","is_self":true,"distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I have an observed state sequence and (continuous) transition times, and I&amp;#39;d like to obtain a likelihood function for this data. &lt;/p&gt;\n\n&lt;p&gt;The semi-Markov model is similar to the normal Markov chain, except the time durations spent in each state are chosen from a specific distribution. The  parameters are: the state transition probabilities &lt;code&gt;a_ij&lt;/code&gt;, the initial state distribution &lt;code&gt;pi&lt;/code&gt; and  the holding time distribution density for state i &lt;code&gt;f_i(t)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;I feel the likelihood should be something like &lt;a href=\"http://i.imgur.com/brsmb.png\"&gt;this&lt;/a&gt;, but I&amp;#39;ve seen references where they use the cumulative distribution function instead of the probability density &lt;code&gt;f_i(t)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Using the cumulative density makes sense when the observation times are arbitrary, but  I&amp;#39;m not sure it&amp;#39;s appropriate when you &lt;em&gt;know&lt;/em&gt; the transition times and want to do MLE or something.&lt;/p&gt;\n\n&lt;p&gt;Which should it be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"secure_media_embed":{},"ups":8,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","subreddit":"MachineLearning","media":null,"created_utc":1351972789,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":8,"link_flair_css_class":null,"selftext":"Basically I have an observed state sequence and (continuous) transition times, and I'd like to obtain a likelihood function for this data. \n\n\nThe semi-Markov model is similar to the normal Markov chain, except the time durations spent in each state are chosen from a specific distribution. The  parameters are: the state transition probabilities ```a_ij```, the initial state distribution ```pi``` and  the holding time distribution density for state i ```f_i(t)```.\n\nI feel the likelihood should be something like [this](http://i.imgur.com/brsmb.png), but I've seen references where they use the cumulative distribution function instead of the probability density ```f_i(t)```.\n\nUsing the cumulative density makes sense when the observation times are arbitrary, but  I'm not sure it's appropriate when you *know* the transition times and want to do MLE or something.\n\nWhich should it be?","retrieved_on":1413438083,"num_comments":4,"author":"roger_","gilded":0,"permalink":"/r/MachineLearning/comments/12kt19/question_about_the_likelihood_function_for_a/"}
{"score":1,"author_flair_text":null,"selftext":"I've been trying to train a classifier on some text data to predict class A.The features I'm using are ngrams\nB. I'm using a linear SVM. \nC. I'm limiting the number of features to 500k ranked my MI, which seems reasonable considering the feature vectors are so sparse. \nD. I have 4x as many negative instances as instances for class A. \n\nHowever, when I train the model (10 fold cv), I get about 80% precision but 10-11% recall on class A. The -ve precision and recall are also about 80%. I'm wondering why the recall for class A is so low.  Possible options I'm considering is to downsample the -ve set but I don't think 4:1 is super unbalanced. Has anybody run into this before or has any good suggestions?","link_flair_css_class":null,"subreddit":"MachineLearning","created_utc":1351950222,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"permalink":"/r/MachineLearning/comments/12kaka/text_classifier_yields_reasonable_precision/","gilded":0,"author":"ra84","retrieved_on":1413438776,"num_comments":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been trying to train a classifier on some text data to predict class A.The features I&amp;#39;m using are ngrams\nB. I&amp;#39;m using a linear SVM. \nC. I&amp;#39;m limiting the number of features to 500k ranked my MI, which seems reasonable considering the feature vectors are so sparse. \nD. I have 4x as many negative instances as instances for class A. &lt;/p&gt;\n\n&lt;p&gt;However, when I train the model (10 fold cv), I get about 80% precision but 10-11% recall on class A. The -ve precision and recall are also about 80%. I&amp;#39;m wondering why the recall for class A is so low.  Possible options I&amp;#39;m considering is to downsample the -ve set but I don&amp;#39;t think 4:1 is super unbalanced. Has anybody run into this before or has any good suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":1,"secure_media_embed":{},"edited":false,"link_flair_text":null,"is_self":true,"title":"Text classifier yields reasonable precision, terrible recall","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/12kaka/text_classifier_yields_reasonable_precision/","user_reports":[],"id":"12kaka"}
{"edited":false,"link_flair_text":null,"title":"Top 10 algorithms in data mining: outdated, but a good starting point for newcomers to the field","is_self":false,"over_18":false,"user_reports":[],"id":"12mxy7","url":"http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf","mod_reports":[],"media_embed":{},"media":null,"report_reasons":null,"created_utc":1352072624,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":50,"num_comments":4,"retrieved_on":1413435065,"author":"dtelad11","permalink":"/r/MachineLearning/comments/12mxy7/top_10_algorithms_in_data_mining_outdated_but_a/","gilded":0,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"default","selftext_html":null,"secure_media_embed":{},"ups":50,"secure_media":null,"domain":"cs.uvm.edu","author_flair_css_class":null}
{"link_flair_text":null,"edited":false,"is_self":false,"title":"(Cross post) Gifting Humble Bundles in Exchange for Help on Computational Statistics Questions","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/statistics/comments/12lwov/gifting_humble_bundles_in_exchange_for_help_on/","user_reports":[],"id":"12lxgq","score":0,"author_flair_text":null,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","report_reasons":null,"created_utc":1352030213,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"permalink":"/r/MachineLearning/comments/12lxgq/cross_post_gifting_humble_bundles_in_exchange_for/","gilded":0,"author":"[deleted]","retrieved_on":1413436531,"num_comments":0,"selftext_html":null,"stickied":false,"thumbnail":"default","distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"reddit.com","secure_media":null,"ups":0,"secure_media_embed":{}}
{"edited":false,"link_flair_text":null,"is_self":false,"title":"Gifting Humble Bundles in Exchange for Help on Computational Statistics Questions","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/statistics/comments/12lwov/gifting_humble_bundles_in_exchange_for_help_on/","user_reports":[],"id":"12lwra","score":1,"author_flair_text":null,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","banned_by":null,"created_utc":1352028087,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"gilded":0,"permalink":"/r/MachineLearning/comments/12lwra/gifting_humble_bundles_in_exchange_for_help_on/","author":"[deleted]","retrieved_on":1413436558,"num_comments":0,"selftext_html":null,"stickied":false,"thumbnail":"default","distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"reddit.com","secure_media":null,"ups":1,"secure_media_embed":{}}
{"domain":"wired.com","secure_media":null,"author_flair_css_class":null,"ups":22,"secure_media_embed":{},"thumbnail":"http://f.thumbs.redditmedia.com/bfPLxsK3SFf2-eu5.jpg","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"permalink":"/r/MachineLearning/comments/12q0ss/the_grapes_of_math/","author":"cavedave","gilded":0,"num_comments":2,"retrieved_on":1413430135,"link_flair_css_class":null,"selftext":"","score":22,"author_flair_text":"naive","subreddit_id":"t5_2r3gv","created_utc":1352195778,"report_reasons":null,"banned_by":null,"media":null,"subreddit":"MachineLearning","mod_reports":[],"media_embed":{},"user_reports":[],"id":"12q0ss","url":"http://www.wired.com/wiredscience/2012/10/mf-fruition-sciences-winemakers/","over_18":false,"is_self":false,"title":"The Grapes of Math","edited":false,"link_flair_text":null}
{"over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"12ppv9","url":"http://www.reddit.com/r/MachineLearning/comments/12ppv9/is_there_anything_like_eureqa_available_in_open/","edited":false,"link_flair_text":null,"is_self":true,"title":"Is there anything like eureqa available in open source?","stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen many programs that do this sort of thing, but nothing as well as eureqa does it.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t know:  It takes a table of numbers and uses logistic regression to guess the equation(s) that best fit your table.&lt;/p&gt;\n\n&lt;p&gt;It has my vote for best ML program ever for end users.  Yet it is closed source so you are stuck if you really want to use it without shelling out money...&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://creativemachines.cornell.edu/eureqa\"&gt;http://creativemachines.cornell.edu/eureqa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":7,"secure_media_embed":{},"link_flair_css_class":null,"selftext":"I have seen many programs that do this sort of thing, but nothing as well as eureqa does it.\n\nIf you don't know:  It takes a table of numbers and uses logistic regression to guess the equation(s) that best fit your table.\n\nIt has my vote for best ML program ever for end users.  Yet it is closed source so you are stuck if you really want to use it without shelling out money...\n\nhttp://creativemachines.cornell.edu/eureqa","score":7,"author_flair_text":null,"created_utc":1352178858,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","author":"progcat","gilded":0,"permalink":"/r/MachineLearning/comments/12ppv9/is_there_anything_like_eureqa_available_in_open/","num_comments":1,"retrieved_on":1413430685}
{"num_comments":28,"retrieved_on":1413424977,"permalink":"/r/MachineLearning/comments/12t0k6/inside_the_secret_world_of_the_data_crunchers_who/","gilded":0,"author":"margin_hound","media":null,"created_utc":1352308731,"subreddit_id":"t5_2r3gv","banned_by":null,"report_reasons":null,"subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":69,"secure_media_embed":{},"ups":69,"secure_media":null,"domain":"swampland.time.com","author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"http://d.thumbs.redditmedia.com/m5946FX2vTwhYzU-.jpg","stickied":false,"selftext_html":null,"title":"Inside the Secret World of the Data Crunchers Who Helped Obama Win","is_self":false,"link_flair_text":null,"edited":false,"user_reports":[],"id":"12t0k6","url":"http://swampland.time.com/2012/11/07/inside-the-secret-world-of-quants-and-data-crunchers-who-helped-obama-win/","mod_reports":[],"media_embed":{},"over_18":false}
{"edited":false,"link_flair_text":null,"title":"http://swampland.time.com/2012/11/07/inside-the-secret-world-of-quants-and-data-crunchers-who-helped-obama-win/","is_self":true,"over_18":false,"user_reports":[],"id":"12ss99","url":"http://www.reddit.com/r/MachineLearning/comments/12ss99/httpswamplandtimecom20121107insidethesecretworldof/","mod_reports":[],"media_embed":{},"subreddit_id":"t5_2r3gv","created_utc":1352301011,"banned_by":null,"report_reasons":null,"media":null,"subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"score":1,"author_flair_text":null,"num_comments":0,"retrieved_on":1413425514,"author":"[deleted]","gilded":0,"permalink":"/r/MachineLearning/comments/12ss99/httpswamplandtimecom20121107insidethesecretworldof/","downs":0,"distinguished":null,"stickied":false,"thumbnail":"default","selftext_html":null,"ups":1,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null}
{"is_self":false,"title":"Creating statistical web applications using the new \"Shiny\" R package","link_flair_text":null,"edited":false,"mod_reports":[],"media_embed":{},"id":"12vb9x","user_reports":[],"url":"http://www.r-bloggers.com/introducing-shiny-easy-web-applications-in-r/","over_18":false,"permalink":"/r/MachineLearning/comments/12vb9x/creating_statistical_web_applications_using_the/","gilded":0,"author":"talgalili","num_comments":0,"retrieved_on":1413421480,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":2,"media":null,"created_utc":1352400658,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","secure_media":null,"domain":"r-bloggers.com","author_flair_css_class":null,"secure_media_embed":{},"ups":2,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/ZzY71Mnn1y5VPnno.jpg","selftext_html":null,"downs":0,"distinguished":null}
{"title":"Basic Sentiment Analysis with Python | fjavieralba.com [xpost /r/programming]","is_self":false,"edited":false,"link_flair_text":null,"user_reports":[],"id":"12xjny","url":"http://fjavieralba.com/basic-sentiment-analysis-with-python.html","mod_reports":[],"media_embed":{},"over_18":false,"num_comments":0,"retrieved_on":1413417854,"gilded":0,"author":"urmyheartBeatStopR","permalink":"/r/MachineLearning/comments/12xjny/basic_sentiment_analysis_with_python/","media":null,"created_utc":1352494163,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":1,"secure_media_embed":{},"ups":1,"secure_media":null,"domain":"fjavieralba.com","author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"default","stickied":false,"selftext_html":null}
{"permalink":"/r/MachineLearning/comments/12x96h/state_of_machine_learning_in_the_united_states/","gilded":0,"author":"coopster","num_comments":19,"retrieved_on":1413418293,"selftext":"I'm about to graduate with a PhD in machine learning in the US, and something's been bothering me and raising my curiosity.  I am subscribed to a bunch of the ML mailing lists, and these lists are where a large number of openings in the ML-related fields (both academic and commercial) are advertised.\n\nAlmost 95% of all the openings advertised are based in Europe, Canada, or China.  Other than the classic tech giants (Google, etc.) and Silicon Valley startups, what is the state of non-academic ML in the US?  I know that many firms, contractors, etc., are looking into applied ML to further their projects, but I don't see nearly the same amount of \"generalized\" applied research activity in the US as I do in other countries.\n\nIs there the same level of activity here, just not as widely discussed?  Are any subscribers here actual commercial practitioners of ML techniques?","link_flair_css_class":null,"author_flair_text":null,"score":24,"media":null,"created_utc":1352484509,"report_reasons":null,"subreddit_id":"t5_2r3gv","banned_by":null,"subreddit":"MachineLearning","secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"secure_media_embed":{},"ups":24,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m about to graduate with a PhD in machine learning in the US, and something&amp;#39;s been bothering me and raising my curiosity.  I am subscribed to a bunch of the ML mailing lists, and these lists are where a large number of openings in the ML-related fields (both academic and commercial) are advertised.&lt;/p&gt;\n\n&lt;p&gt;Almost 95% of all the openings advertised are based in Europe, Canada, or China.  Other than the classic tech giants (Google, etc.) and Silicon Valley startups, what is the state of non-academic ML in the US?  I know that many firms, contractors, etc., are looking into applied ML to further their projects, but I don&amp;#39;t see nearly the same amount of &amp;quot;generalized&amp;quot; applied research activity in the US as I do in other countries.&lt;/p&gt;\n\n&lt;p&gt;Is there the same level of activity here, just not as widely discussed?  Are any subscribers here actual commercial practitioners of ML techniques?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"is_self":true,"title":"State of Machine Learning in the United States?  Are there industry employed ML practitioners in here or just enthusiasts and students? ","edited":false,"link_flair_text":null,"mod_reports":[],"media_embed":{},"id":"12x96h","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/12x96h/state_of_machine_learning_in_the_united_states/","over_18":false}
{"id":"12y7og","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/12y7og/im_currently_in_linear_algebra_what_should_i_look/","mod_reports":[],"media_embed":{},"over_18":false,"title":"I'm currently in Linear Algebra, what should I look up that isn't generally covered in an elementary class?","is_self":true,"link_flair_text":null,"edited":false,"secure_media_embed":{},"ups":13,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re going to get right up to the edge of vector space isomorphisms before the syllabus runs out.  I&amp;#39;m finding it very interesting, but we&amp;#39;ve had to skip a lot of application illustrations for the sake of time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":5,"retrieved_on":1413416776,"gilded":0,"permalink":"/r/MachineLearning/comments/12y7og/im_currently_in_linear_algebra_what_should_i_look/","author":"cephelotron","media":null,"created_utc":1352520886,"subreddit_id":"t5_2r3gv","report_reasons":null,"banned_by":null,"subreddit":"MachineLearning","selftext":"We're going to get right up to the edge of vector space isomorphisms before the syllabus runs out.  I'm finding it very interesting, but we've had to skip a lot of application illustrations for the sake of time.","link_flair_css_class":null,"author_flair_text":null,"score":13}
{"secure_media":null,"thumbnail":"self","id":"130ovv","edited":false,"is_self":true,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"media":null,"ups":1,"report_reasons":null,"permalink":"/r/MachineLearning/comments/130ovv/image_recovery_via_ml/","num_comments":0,"mod_reports":[],"domain":"self.MachineLearning","user_reports":[],"author":"MuffinShit","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to use machine learning techniques in image recovery. If the image is truncated (say half is completely cut off - no information) is it possible to recover the missing portion? I&amp;#39;ve been looking at Collborative Filtering so far, and if the image is noisy rather than incomplete using a Wiener Filter. &lt;/p&gt;\n\n&lt;p&gt;Any advice is greatly appreciated. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"Image Recovery via ML","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/130ovv/image_recovery_via_ml/","retrieved_on":1413385448,"selftext":"I'd like to use machine learning techniques in image recovery. If the image is truncated (say half is completely cut off - no information) is it possible to recover the missing portion? I've been looking at Collborative Filtering so far, and if the image is noisy rather than incomplete using a Wiener Filter. \n\nAny advice is greatly appreciated. Thanks!","author_flair_css_class":null,"downs":0,"created_utc":1352658838}
{"title":"Anything better than pybrain for someone new machine learning?","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1352760659,"selftext":"","author_flair_css_class":null,"downs":0,"retrieved_on":1413381855,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1336yd/anything_better_than_pybrain_for_someone_new/","mod_reports":[],"num_comments":4,"gilded":0,"score":3,"selftext_html":null,"domain":"self.MachineLearning","author":"chewygumbarsnack","user_reports":[],"edited":false,"is_self":true,"id":"1336yd","permalink":"/r/MachineLearning/comments/1336yd/anything_better_than_pybrain_for_someone_new/","secure_media_embed":{},"report_reasons":null,"ups":3,"media":null,"over_18":false,"stickied":false,"media_embed":{},"thumbnail":"self","secure_media":null}
{"created_utc":1352756680,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413382054,"url":"http://www.reddit.com/r/aiHub/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"Announcing /r/aiHub ~ aiHub gathers quality and informative reddit submissions and discussions from the field of Artificial Intelligence.","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"gilded":0,"score":0,"selftext_html":null,"author":"locster","user_reports":[],"domain":"reddit.com","mod_reports":[],"num_comments":0,"permalink":"/r/MachineLearning/comments/13322f/announcing_raihub_aihub_gathers_quality_and/","media":null,"ups":0,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"is_self":false,"edited":false,"id":"13322f","thumbnail":"default","secure_media":null}
{"secure_media":null,"thumbnail":"http://e.thumbs.redditmedia.com/2vhA51jXHDlXDdgX.jpg","id":"132t7y","edited":false,"is_self":false,"stickied":false,"over_18":false,"media_embed":{"width":600,"scrolling":false,"height":338,"content":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/giXBP-wwYpw?fs=1&amp;feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;"},"secure_media_embed":{},"ups":23,"report_reasons":null,"media":{"type":"youtube.com","oembed":{"width":600,"author_url":"http://www.youtube.com/user/sfdatamining","thumbnail_width":480,"version":"1.0","author_name":"sfdatamining","thumbnail_url":"http://i4.ytimg.com/vi/giXBP-wwYpw/hqdefault.jpg","title":"Building Analytical Applications on Hadoop - Josh Wills","provider_name":"YouTube","url":"http://www.youtube.com/watch?v=giXBP-wwYpw","height":338,"provider_url":"http://www.youtube.com/","type":"video","thumbnail_height":360,"html":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/giXBP-wwYpw?fs=1&amp;feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","description":"Data scientists-- the analytical professionals who straddle the line between statistician and software engineer-- are in demand like never before. Due to the scarcity of data science talent, it has become increasingly important for data scientists to spend less time answering one-off questions and more time building analytical applications that enable a broad class of users to interact with large data sets, ask detailed questions, and make valid inferences."}},"permalink":"/r/MachineLearning/comments/132t7y/building_analytical_applications_on_hadoop/","num_comments":1,"mod_reports":[],"domain":"youtube.com","author":"agconway","user_reports":[],"gilded":0,"score":23,"selftext_html":null,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Building Analytical Applications on Hadoop","subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.youtube.com/watch?v=giXBP-wwYpw","retrieved_on":1413382404,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1352749326}
{"thumbnail":"self","secure_media":null,"id":"132kmg","edited":false,"is_self":true,"ups":2,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/132kmg/hi_rmachinelearning_any_chance_with_a_hand_with_a/","mod_reports":[],"num_comments":8,"user_reports":[],"author":"AryanHonesty","domain":"self.MachineLearning","score":2,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, i&amp;#39;m doing a university course and the backpropogation algorithm has come up. I understand it and how it works, except for the following;&lt;/p&gt;\n\n&lt;p&gt;In this video; &lt;a href=\"http://www.youtube.com/watch?v=p1-FiWjThs8&amp;amp;feature=relmfu\"&gt;http://www.youtube.com/watch?v=p1-FiWjThs8&amp;amp;feature=relmfu&lt;/a&gt; , it is stated that I need the gradient of error (and therefore the inverse function of the node) for the output node before continuing. The bit I mean is at 6:30. &lt;/p&gt;\n\n&lt;p&gt;However, in the example I have, the output node is a summation node, and I was wondering how I would go about finding the error gradient in this case? I can&amp;#39;t get the derivative of the activation function (A sum), can I?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Hi r/machinelearning , any chance with a hand with a Neural Networks problem?","distinguished":null,"link_flair_text":null,"retrieved_on":1413382732,"url":"http://www.reddit.com/r/MachineLearning/comments/132kmg/hi_rmachinelearning_any_chance_with_a_hand_with_a/","subreddit_id":"t5_2r3gv","created_utc":1352741733,"downs":0,"selftext":"Basically, i'm doing a university course and the backpropogation algorithm has come up. I understand it and how it works, except for the following;\n\nIn this video; http://www.youtube.com/watch?v=p1-FiWjThs8&amp;feature=relmfu , it is stated that I need the gradient of error (and therefore the inverse function of the node) for the output node before continuing. The bit I mean is at 6:30. \n\nHowever, in the example I have, the output node is a summation node, and I was wondering how I would go about finding the error gradient in this case? I can't get the derivative of the activation function (A sum), can I?\n\nThanks in advance!","author_flair_css_class":null}
{"secure_media":null,"thumbnail":"self","permalink":"/r/MachineLearning/comments/1356ii/discussion_googles_scalable_deep_learning_1000/","media_embed":{},"stickied":false,"over_18":false,"ups":18,"media":null,"report_reasons":null,"secure_media_embed":{},"is_self":true,"edited":false,"id":"1356ii","score":18,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Should google be applying their recently devised scalable deep learning approach to freely available human genome data?&lt;/p&gt;\n\n&lt;p&gt;Some thoughts/links/info...&lt;/p&gt;\n\n&lt;p&gt;The &lt;a href=\"http://aws.amazon.com/1000genomes/\"&gt;1000 genome project&lt;/a&gt; has already released data for 1700 genomes, weighing in at 200TB. We know the 3 billion base pairs of a human can be represented directly (uncompressed) in 750MB. 750MB * 1700 sequences = 1.275 TB. One suggestion I&amp;#39;ve encountered is that this is raw sequencing output that needs splicing together, if you do that and are happy with that one interpretation for each genome we&amp;#39;re perhaps looking at 1.275TB instead of 200 - a good start at making this data more manageable.&lt;/p&gt;\n\n&lt;p&gt;1.275TB will fit on a single HD. Furthermore it&amp;#39;s not unreasonable to conceive of a single PC box with say 2TB of RAM. However...&lt;/p&gt;\n\n&lt;p&gt;Google already perform deep learning on large data sets, see &lt;a href=\"http://techtalks.tv/talks/57639/\"&gt;Scaling Deep Learning, Jeff Dean, Google&lt;/a&gt; and &lt;a href=\"http://fora.tv/2012/10/14/Peter_Norvig_Channeling_the_Flood_of_Data\"&gt;Peter Norvig: Channeling the Flood of Data&lt;/a&gt;. tl;dr - they partition the data between machines within a data centre and communicate connection weight deltas between machines.&lt;/p&gt;\n\n&lt;p&gt;It seems to me that deep learning may be able to discover deeper structures in the genetic sequences than are currently known, and that we might be able to find correlations between these deep structures and phenotype level features. Would this work? Is anyone aware of such a project? Thanks for reading.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"locster","user_reports":[],"domain":"self.MachineLearning","num_comments":24,"mod_reports":[],"downs":0,"selftext":"Should google be applying their recently devised scalable deep learning approach to freely available human genome data?\n\nSome thoughts/links/info...\n\nThe [1000 genome project](http://aws.amazon.com/1000genomes/) has already released data for 1700 genomes, weighing in at 200TB. We know the 3 billion base pairs of a human can be represented directly (uncompressed) in 750MB. 750MB * 1700 sequences = 1.275 TB. One suggestion I've encountered is that this is raw sequencing output that needs splicing together, if you do that and are happy with that one interpretation for each genome we're perhaps looking at 1.275TB instead of 200 - a good start at making this data more manageable.\n\n1.275TB will fit on a single HD. Furthermore it's not unreasonable to conceive of a single PC box with say 2TB of RAM. However...\n\nGoogle already perform deep learning on large data sets, see [Scaling Deep Learning, Jeff Dean, Google](http://techtalks.tv/talks/57639/) and [Peter Norvig: Channeling the Flood of Data](http://fora.tv/2012/10/14/Peter_Norvig_Channeling_the_Flood_of_Data). tl;dr - they partition the data between machines within a data centre and communicate connection weight deltas between machines.\n\nIt seems to me that deep learning may be able to discover deeper structures in the genetic sequences than are currently known, and that we might be able to find correlations between these deep structures and phenotype level features. Would this work? Is anyone aware of such a project? Thanks for reading.","author_flair_css_class":null,"created_utc":1352842029,"url":"http://www.reddit.com/r/MachineLearning/comments/1356ii/discussion_googles_scalable_deep_learning_1000/","subreddit_id":"t5_2r3gv","retrieved_on":1413378975,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Discussion: Google's Scalable Deep Learning + 1000 Genome Project?","link_flair_css_class":null,"author_flair_text":null}
{"thumbnail":"default","secure_media":null,"id":"134so1","is_self":false,"edited":false,"secure_media_embed":{},"ups":0,"media":null,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/134so1/google_fellow_jeff_dean_presents_scaling_deep/","mod_reports":[],"num_comments":0,"domain":"techtalks.tv","author":"Jmartin024","user_reports":[],"score":0,"gilded":0,"selftext_html":null,"author_flair_text":null,"link_flair_css_class":null,"title":"Google Fellow, Jeff Dean, presents 'Scaling Deep Learning' | TechTalks.tv","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"retrieved_on":1413379516,"subreddit_id":"t5_2r3gv","url":"http://techtalks.tv/talks/scaling-deep-learning/57639/","created_utc":1352830371,"author_flair_css_class":null,"selftext":"","downs":0}
{"domain":"r-bloggers.com","user_reports":[],"author":"talgalili","gilded":0,"score":1,"selftext_html":null,"mod_reports":[],"num_comments":0,"retrieved_on":1413379567,"subreddit_id":"t5_2r3gv","url":"http://www.r-bloggers.com/benchmarking-bigglm/","created_utc":1352829199,"selftext":"","author_flair_css_class":null,"downs":0,"author_flair_text":null,"link_flair_css_class":null,"title":"Benchmarking bigglm (R's linear model function for \"big data\")","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"thumbnail":"http://a.thumbs.redditmedia.com/4gzcFpWHg-eBcR10.jpg","secure_media":null,"secure_media_embed":{},"ups":1,"media":null,"report_reasons":null,"stickied":false,"over_18":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/134r9a/benchmarking_bigglm_rs_linear_model_function_for/","id":"134r9a","is_self":false,"edited":false}
{"thumbnail":"default","secure_media":null,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/134j9c/how_i_cracked_troyis_the_online_flash_chess_game/","id":"134j9c","is_self":false,"edited":false,"author":"talgalili","user_reports":[],"domain":"r-bloggers.com","gilded":0,"score":1,"selftext_html":null,"mod_reports":[],"num_comments":0,"retrieved_on":1413379898,"url":"http://www.r-bloggers.com/how-i-cracked-troyis-the-online-flash-game/","subreddit_id":"t5_2r3gv","created_utc":1352821924,"downs":0,"selftext":"","author_flair_css_class":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"\"How I cracked Troyis (the online flash chess game) using R\" (watch the video from minute 1:05)","distinguished":null,"link_flair_text":null}
{"author":"julian255","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":34,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought it would be cool to collect a list of some really awesome (maybe even a few &amp;quot;lay person&amp;quot; accesable) research papers on the topic of ML. I wanted to open it up to the &lt;a href=\"/r/MachineLearning\"&gt;/r/MachineLearning&lt;/a&gt; community and see what papers you found really interesting. Here are two I thought were pretty interesting:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.sciencemag.org/content/331/6014/176.full.pdf\"&gt;Quantitative Analysis of Culture Using Millions of Digitized Books&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;and &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/38115.pdf\"&gt;Building High-level Features Using Large Scale Unsupervised Learning&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":10,"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/1345b3/compiling_a_list_of_awesome_ml_papers/","subreddit_id":"t5_2r3gv","retrieved_on":1413380468,"downs":0,"author_flair_css_class":null,"selftext":"I thought it would be cool to collect a list of some really awesome (maybe even a few \"lay person\" accesable) research papers on the topic of ML. I wanted to open it up to the /r/MachineLearning community and see what papers you found really interesting. Here are two I thought were pretty interesting:\n\n[Quantitative Analysis of Culture Using Millions of Digitized Books](http://www.sciencemag.org/content/331/6014/176.full.pdf)\n\nand \n\n[Building High-level Features Using Large Scale Unsupervised Learning](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/38115.pdf)","created_utc":1352797343,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Compiling a list of awesome ML papers","secure_media":null,"thumbnail":"self","media_embed":{},"stickied":false,"over_18":false,"media":null,"ups":34,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/1345b3/compiling_a_list_of_awesome_ml_papers/","id":"1345b3","edited":false,"is_self":true}
{"thumbnail":"default","secure_media":null,"id":"133jg3","edited":false,"is_self":false,"media":null,"ups":1,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/133jg3/large_scale_distributed_deep_networks/","mod_reports":[],"num_comments":0,"user_reports":[],"author":"marshallp","domain":"research.google.com","gilded":0,"score":1,"selftext_html":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Large Scale Distributed Deep Networks","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413381357,"url":"http://research.google.com/archive/large_deep_networks_nips2012.html","subreddit_id":"t5_2r3gv","created_utc":1352771559,"downs":0,"author_flair_css_class":null,"selftext":""}
{"thumbnail":"self","secure_media":null,"edited":false,"is_self":true,"id":"137a5d","permalink":"/r/MachineLearning/comments/137a5d/cant_figure_out_how_to_properly_implement_simple/","secure_media_embed":{},"ups":7,"media":null,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"mod_reports":[],"num_comments":16,"score":7,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi r/machinelearning, I&amp;#39;m in the process of learning Machine Learning for my own personal curiosity.  I&amp;#39;m loosely using Andrew Ng&amp;#39;s lectures and notes from CS229, but am not taking the course.&lt;/p&gt;\n\n&lt;p&gt;I found a data set online with various car properties including miles per gallon.  I&amp;#39;m trying to use the car properties to predict miles per gallon.  This is the equation I&amp;#39;m using (or at least attempting to use):&lt;/p&gt;\n\n&lt;p&gt;equation: &lt;a href=\"http://i.imgur.com/qVRfN.png\"&gt;http://i.imgur.com/qVRfN.png&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;source: &lt;a href=\"http://cs229.stanford.edu/notes/cs229-notes1.pdf\"&gt;http://cs229.stanford.edu/notes/cs229-notes1.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m finding is that the error does go down when this equation is used.  Also, I end up with negative weights for &amp;quot;Acceleration&amp;quot; and &amp;quot;Weight&amp;quot; which makes sense (higher acceleration and weight leads to lower MPG, that checks out).  Once I actually apply the weights to create predictions, they are entirely off.  &lt;/p&gt;\n\n&lt;p&gt;Is this a common problem for beginners?  Any clear indication what I might be doing incorrectly?  Here is my code, any tips or anything would be greatly appreciated.  Also, if you have any comments on my code please chime in.  I am aware C# is not the best language for this and that I should be using matrices, other than that, I am interested in learning more from other people. &lt;/p&gt;\n\n&lt;p&gt;--START CODE--&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;using System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.IO;\n\nnamespace CarMpg {\n    class Program {\n        static void Main(string[] args) {\n            var cars = Importer.GetCars();\n            var theta = Model.GetTheta(cars, .00000000001);\n            var predictions = Model.GetPredictions(cars, theta);\n\n            var outputPath = @&amp;quot;H:\\_MachineLearning\\CarMpg\\Result.csv&amp;quot;;\n\n            if (File.Exists(outputPath)) {\n                File.Delete(outputPath);\n            }\n\n            var streamWriter = new StreamWriter(outputPath);\n            foreach (var prediction in predictions) {\n                streamWriter.WriteLine(prediction.Car.Mpg + &amp;quot;,&amp;quot; + prediction.MpgPrediction);\n            }\n            streamWriter.Close();\n        }\n    }\n}\n\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\n\nnamespace CarMpg {\n    public static class Model {\n        public static double GetH(Car car, Theta theta) {\n            return\n                theta.BaselineWeight +\n                theta.AccelerationWeight * car.Acceleration +\n                theta.CylindersWeight * car.Cylinders +\n                theta.DisplacementWeight * car.Displacement +\n                theta.HorsePowerWeight * car.HorsePower +\n                theta.ModelYearWeight * car.ModelYear +\n                theta.WeightWeight * car.Weight;\n        }\n\n        public static Theta GetTheta(List&amp;lt;Car&amp;gt; cars, double alpha) {\n            var theta = new Theta();\n            var costs = new List&amp;lt;double&amp;gt;();\n            for (int i = 0; i &amp;lt; 1000; i++) {\n\n                var errors = new List&amp;lt;Error&amp;gt;();\n                foreach (var car in cars) {\n                    var errorFactor = car.Mpg - GetH(car, theta);\n                    var error = new Error(car, errorFactor);\n                    errors.Add(error);\n                }\n\n                costs.Add(cars.Select(a =&amp;gt; Math.Pow(a.Mpg - GetH(a, theta),2)).Sum());\n\n                var errorSum = new Error();\n                errorSum.BaselineWeight = errors.Sum(a =&amp;gt; a.BaselineWeight);\n                errorSum.AccelerationWeight = errors.Sum(a =&amp;gt; a.AccelerationWeight);\n                errorSum.CylindersWeight = errors.Sum(a =&amp;gt; a.CylindersWeight);\n                errorSum.DisplacementWeight = errors.Sum(a =&amp;gt; a.DisplacementWeight);\n                errorSum.HorsePowerWeight = errors.Sum(a =&amp;gt; a.HorsePowerWeight);\n                errorSum.ModelYearWeight = errors.Sum(a =&amp;gt; a.ModelYearWeight);\n                errorSum.WeightWeight = errors.Sum(a =&amp;gt; a.WeightWeight);\n\n                theta.BaselineWeight += errorSum.BaselineWeight * alpha;\n                theta.AccelerationWeight += errorSum.AccelerationWeight * alpha;\n                theta.CylindersWeight += errorSum.CylindersWeight * alpha;\n                theta.DisplacementWeight += errorSum.DisplacementWeight * alpha;\n                theta.HorsePowerWeight += errorSum.HorsePowerWeight * alpha;\n                theta.ModelYearWeight += errorSum.ModelYearWeight * alpha;\n                theta.WeightWeight += errorSum.WeightWeight * alpha;\n            }\n\n            return theta;\n        }\n\n        public static List&amp;lt;Prediction&amp;gt; GetPredictions(List&amp;lt;Car&amp;gt; cars, Theta theta) {\n            return cars.Select(a =&amp;gt; new Prediction(a, theta)).ToList();\n        }\n    }\n}\n\n\n\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.IO;\n\nnamespace CarMpg {\n    public static class Importer {\n        public static List&amp;lt;Car&amp;gt; GetCars() {\n            var path = @&amp;quot;H:\\_MachineLearning\\CarMpg\\Data.csv&amp;quot;;\n            var streamReader = new StreamReader(path);\n            var line = streamReader.ReadLine();\n            line = streamReader.ReadLine(); //Skip header\n\n            var cars = new List&amp;lt;Car&amp;gt;();\n\n            while (line != null) {\n                if (!line.Contains(&amp;quot;?&amp;quot;)) {\n                    var car = new Car(line);\n                    cars.Add(car);\n                }\n\n                line = streamReader.ReadLine(); \n            }\n\n            return cars;\n        }\n    }\n\n    public class Car {\n        public double Mpg;\n        public double Cylinders;\n        public double Displacement;\n        public double HorsePower;\n        public double Weight;\n        public double Acceleration;\n        public double ModelYear;\n        public double Origin;\n        public string Name;\n\n        public Car(string line) {\n            var splits = line.Split(&amp;#39;,&amp;#39;);\n\n            Mpg = Convert.ToDouble(splits[0]);\n            Cylinders = Convert.ToDouble(splits[1]);\n            Displacement= Convert.ToDouble(splits[2]);\n            HorsePower = Convert.ToDouble(splits[3]);\n            Weight = Convert.ToDouble(splits[4]);\n            Acceleration = Convert.ToDouble(splits[5]);\n            ModelYear = Convert.ToDouble(splits[6]);\n            Origin = Convert.ToDouble(splits[7]);\n            Name = splits[8].Replace(&amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;);\n        }\n    }\n\n    public class Theta : Row {\n    }\n\n    public class Error : Row {\n        public Error() { }\n\n        public Error(Car car, double errorFactor) {\n            BaselineWeight = errorFactor;\n            CylindersWeight = car.Cylinders * errorFactor;\n            DisplacementWeight = car.Displacement * errorFactor;\n            HorsePowerWeight = car.HorsePower * errorFactor;\n            WeightWeight = car.Weight * errorFactor;\n            AccelerationWeight = car.Weight * errorFactor;\n            ModelYearWeight = car.ModelYear * errorFactor;\n        }\n    }\n\n    public class Row {\n        public double BaselineWeight;\n        public double CylindersWeight;\n        public double DisplacementWeight;\n        public double HorsePowerWeight;\n        public double WeightWeight;\n        public double AccelerationWeight;\n        public double ModelYearWeight;\n\n        public Row() {\n            BaselineWeight = .5;\n            CylindersWeight = .5;\n            DisplacementWeight = .5;\n            HorsePowerWeight = .5;\n            WeightWeight = .5;\n            AccelerationWeight = .5;\n            ModelYearWeight = .5;\n        }\n    }\n\n    public class Prediction {\n        public Car Car;\n        public double MpgPrediction;\n\n        public Prediction(Car car, Theta theta) {\n            Car = car;\n            MpgPrediction = Model.GetH(car, theta);\n        }\n    }\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"leex1867","banned_by":null,"title":"Can't figure out how to properly implement simple gradient descent ","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1352928312,"author_flair_css_class":null,"selftext":"Hi r/machinelearning, I'm in the process of learning Machine Learning for my own personal curiosity.  I'm loosely using Andrew Ng's lectures and notes from CS229, but am not taking the course.\n\nI found a data set online with various car properties including miles per gallon.  I'm trying to use the car properties to predict miles per gallon.  This is the equation I'm using (or at least attempting to use):\n\nequation: http://i.imgur.com/qVRfN.png \n\nsource: http://cs229.stanford.edu/notes/cs229-notes1.pdf\n\nWhat I'm finding is that the error does go down when this equation is used.  Also, I end up with negative weights for \"Acceleration\" and \"Weight\" which makes sense (higher acceleration and weight leads to lower MPG, that checks out).  Once I actually apply the weights to create predictions, they are entirely off.  \n\nIs this a common problem for beginners?  Any clear indication what I might be doing incorrectly?  Here is my code, any tips or anything would be greatly appreciated.  Also, if you have any comments on my code please chime in.  I am aware C# is not the best language for this and that I should be using matrices, other than that, I am interested in learning more from other people. \n\n\n--START CODE--\n\n\n\n\tusing System;\n\tusing System.Collections.Generic;\n\tusing System.Text;\n\tusing System.IO;\n\n\tnamespace CarMpg {\n\t\tclass Program {\n\t\t\tstatic void Main(string[] args) {\n\t\t\t\tvar cars = Importer.GetCars();\n\t\t\t\tvar theta = Model.GetTheta(cars, .00000000001);\n\t\t\t\tvar predictions = Model.GetPredictions(cars, theta);\n\n\t\t\t\tvar outputPath = @\"H:\\_MachineLearning\\CarMpg\\Result.csv\";\n\n\t\t\t\tif (File.Exists(outputPath)) {\n\t\t\t\t\tFile.Delete(outputPath);\n\t\t\t\t}\n\n\t\t\t\tvar streamWriter = new StreamWriter(outputPath);\n\t\t\t\tforeach (var prediction in predictions) {\n\t\t\t\t\tstreamWriter.WriteLine(prediction.Car.Mpg + \",\" + prediction.MpgPrediction);\n\t\t\t\t}\n\t\t\t\tstreamWriter.Close();\n\t\t\t}\n\t\t}\n\t}\n\n\n\n\tusing System;\n\tusing System.Collections.Generic;\n\tusing System.Linq;\n\tusing System.Text;\n\n\tnamespace CarMpg {\n\t\tpublic static class Model {\n\t\t\tpublic static double GetH(Car car, Theta theta) {\n\t\t\t\treturn\n\t\t\t\t\ttheta.BaselineWeight +\n\t\t\t\t\ttheta.AccelerationWeight * car.Acceleration +\n\t\t\t\t\ttheta.CylindersWeight * car.Cylinders +\n\t\t\t\t\ttheta.DisplacementWeight * car.Displacement +\n\t\t\t\t\ttheta.HorsePowerWeight * car.HorsePower +\n\t\t\t\t\ttheta.ModelYearWeight * car.ModelYear +\n\t\t\t\t\ttheta.WeightWeight * car.Weight;\n\t\t\t}\n\n\t\t\tpublic static Theta GetTheta(List&lt;Car&gt; cars, double alpha) {\n\t\t\t\tvar theta = new Theta();\n\t\t\t\tvar costs = new List&lt;double&gt;();\n\t\t\t\tfor (int i = 0; i &lt; 1000; i++) {\n\n\t\t\t\t\tvar errors = new List&lt;Error&gt;();\n\t\t\t\t\tforeach (var car in cars) {\n\t\t\t\t\t\tvar errorFactor = car.Mpg - GetH(car, theta);\n\t\t\t\t\t\tvar error = new Error(car, errorFactor);\n\t\t\t\t\t\terrors.Add(error);\n\t\t\t\t\t}\n\n\t\t\t\t\tcosts.Add(cars.Select(a =&gt; Math.Pow(a.Mpg - GetH(a, theta),2)).Sum());\n\n\t\t\t\t\tvar errorSum = new Error();\n\t\t\t\t\terrorSum.BaselineWeight = errors.Sum(a =&gt; a.BaselineWeight);\n\t\t\t\t\terrorSum.AccelerationWeight = errors.Sum(a =&gt; a.AccelerationWeight);\n\t\t\t\t\terrorSum.CylindersWeight = errors.Sum(a =&gt; a.CylindersWeight);\n\t\t\t\t\terrorSum.DisplacementWeight = errors.Sum(a =&gt; a.DisplacementWeight);\n\t\t\t\t\terrorSum.HorsePowerWeight = errors.Sum(a =&gt; a.HorsePowerWeight);\n\t\t\t\t\terrorSum.ModelYearWeight = errors.Sum(a =&gt; a.ModelYearWeight);\n\t\t\t\t\terrorSum.WeightWeight = errors.Sum(a =&gt; a.WeightWeight);\n\n\t\t\t\t\ttheta.BaselineWeight += errorSum.BaselineWeight * alpha;\n\t\t\t\t\ttheta.AccelerationWeight += errorSum.AccelerationWeight * alpha;\n\t\t\t\t\ttheta.CylindersWeight += errorSum.CylindersWeight * alpha;\n\t\t\t\t\ttheta.DisplacementWeight += errorSum.DisplacementWeight * alpha;\n\t\t\t\t\ttheta.HorsePowerWeight += errorSum.HorsePowerWeight * alpha;\n\t\t\t\t\ttheta.ModelYearWeight += errorSum.ModelYearWeight * alpha;\n\t\t\t\t\ttheta.WeightWeight += errorSum.WeightWeight * alpha;\n\t\t\t\t}\n\n\t\t\t\treturn theta;\n\t\t\t}\n\n\t\t\tpublic static List&lt;Prediction&gt; GetPredictions(List&lt;Car&gt; cars, Theta theta) {\n\t\t\t\treturn cars.Select(a =&gt; new Prediction(a, theta)).ToList();\n\t\t\t}\n\t\t}\n\t}\n\n\n\n\tusing System;\n\tusing System.Collections.Generic;\n\tusing System.Text;\n\tusing System.IO;\n\n\tnamespace CarMpg {\n\t\tpublic static class Importer {\n\t\t\tpublic static List&lt;Car&gt; GetCars() {\n\t\t\t\tvar path = @\"H:\\_MachineLearning\\CarMpg\\Data.csv\";\n\t\t\t\tvar streamReader = new StreamReader(path);\n\t\t\t\tvar line = streamReader.ReadLine();\n\t\t\t\tline = streamReader.ReadLine(); //Skip header\n\n\t\t\t\tvar cars = new List&lt;Car&gt;();\n\n\t\t\t\twhile (line != null) {\n\t\t\t\t\tif (!line.Contains(\"?\")) {\n\t\t\t\t\t\tvar car = new Car(line);\n\t\t\t\t\t\tcars.Add(car);\n\t\t\t\t\t}\n\n\t\t\t\t\tline = streamReader.ReadLine(); \n\t\t\t\t}\n\n\t\t\t\treturn cars;\n\t\t\t}\n\t\t}\n\n\t\tpublic class Car {\n\t\t\tpublic double Mpg;\n\t\t\tpublic double Cylinders;\n\t\t\tpublic double Displacement;\n\t\t\tpublic double HorsePower;\n\t\t\tpublic double Weight;\n\t\t\tpublic double Acceleration;\n\t\t\tpublic double ModelYear;\n\t\t\tpublic double Origin;\n\t\t\tpublic string Name;\n\t\t\t\n\t\t\tpublic Car(string line) {\n\t\t\t\tvar splits = line.Split(',');\n\n\t\t\t\tMpg = Convert.ToDouble(splits[0]);\n\t\t\t\tCylinders = Convert.ToDouble(splits[1]);\n\t\t\t\tDisplacement= Convert.ToDouble(splits[2]);\n\t\t\t\tHorsePower = Convert.ToDouble(splits[3]);\n\t\t\t\tWeight = Convert.ToDouble(splits[4]);\n\t\t\t\tAcceleration = Convert.ToDouble(splits[5]);\n\t\t\t\tModelYear = Convert.ToDouble(splits[6]);\n\t\t\t\tOrigin = Convert.ToDouble(splits[7]);\n\t\t\t\tName = splits[8].Replace(\"\\\"\", \"\");\n\t\t\t}\n\t\t}\n\n\t\tpublic class Theta : Row {\n\t\t}\n\n\t\tpublic class Error : Row {\n\t\t\tpublic Error() { }\n\n\t\t\tpublic Error(Car car, double errorFactor) {\n\t\t\t\tBaselineWeight = errorFactor;\n\t\t\t\tCylindersWeight = car.Cylinders * errorFactor;\n\t\t\t\tDisplacementWeight = car.Displacement * errorFactor;\n\t\t\t\tHorsePowerWeight = car.HorsePower * errorFactor;\n\t\t\t\tWeightWeight = car.Weight * errorFactor;\n\t\t\t\tAccelerationWeight = car.Weight * errorFactor;\n\t\t\t\tModelYearWeight = car.ModelYear * errorFactor;\n\t\t\t}\n\t\t}\n\n\t\tpublic class Row {\n\t\t\tpublic double BaselineWeight;\n\t\t\tpublic double CylindersWeight;\n\t\t\tpublic double DisplacementWeight;\n\t\t\tpublic double HorsePowerWeight;\n\t\t\tpublic double WeightWeight;\n\t\t\tpublic double AccelerationWeight;\n\t\t\tpublic double ModelYearWeight;\n\n\t\t\tpublic Row() {\n\t\t\t\tBaselineWeight = .5;\n\t\t\t\tCylindersWeight = .5;\n\t\t\t\tDisplacementWeight = .5;\n\t\t\t\tHorsePowerWeight = .5;\n\t\t\t\tWeightWeight = .5;\n\t\t\t\tAccelerationWeight = .5;\n\t\t\t\tModelYearWeight = .5;\n\t\t\t}\n\t\t}\n\n\t\tpublic class Prediction {\n\t\t\tpublic Car Car;\n\t\t\tpublic double MpgPrediction;\n\n\t\t\tpublic Prediction(Car car, Theta theta) {\n\t\t\t\tCar = car;\n\t\t\t\tMpgPrediction = Model.GetH(car, theta);\n\t\t\t}\n\t\t}\n\t}\n","downs":0,"retrieved_on":1413376027,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/137a5d/cant_figure_out_how_to_properly_implement_simple/"}
{"id":"137743","edited":1352926075,"is_self":true,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":6,"media":null,"report_reasons":null,"permalink":"/r/MachineLearning/comments/137743/are_there_any_mlnlp_workspapers_on_parsingsolving/","secure_media":null,"thumbnail":"self","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"Are there any ML/NLP works/papers on parsing/solving math word problems?","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/137743/are_there_any_mlnlp_workspapers_on_parsingsolving/","retrieved_on":1413376145,"author_flair_css_class":null,"selftext":"As the title says, any pointers are much appreciated.\n\nI am exploring, where do we stand in terms of ML/NLP efforts, in context of solving (to begin with - parsing) Math Word Problems.\n\nWe have decent enough softwares in likes of Mathematica which can solve well formulated math equations.\n\nBut when it comes to solving math problems expressed in natural languages, I could not find anything substantial.\nWhen I think about how to approach this, I see it as a sort of Machine Translation problem (translating from English to Math-equations), but there is hardly any 'labeled' data for that. Other approach can be semi (or un) sypervised Relation Extraction.\n\nSince these are just random thoughts, I want to start with some existing work/papers in this direction. My otherwise decent googling skills, didn't help much.","downs":0,"created_utc":1352925658,"num_comments":5,"mod_reports":[],"domain":"self.MachineLearning","author":"akshayxyz","user_reports":[],"gilded":0,"score":6,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, any pointers are much appreciated.&lt;/p&gt;\n\n&lt;p&gt;I am exploring, where do we stand in terms of ML/NLP efforts, in context of solving (to begin with - parsing) Math Word Problems.&lt;/p&gt;\n\n&lt;p&gt;We have decent enough softwares in likes of Mathematica which can solve well formulated math equations.&lt;/p&gt;\n\n&lt;p&gt;But when it comes to solving math problems expressed in natural languages, I could not find anything substantial.\nWhen I think about how to approach this, I see it as a sort of Machine Translation problem (translating from English to Math-equations), but there is hardly any &amp;#39;labeled&amp;#39; data for that. Other approach can be semi (or un) sypervised Relation Extraction.&lt;/p&gt;\n\n&lt;p&gt;Since these are just random thoughts, I want to start with some existing work/papers in this direction. My otherwise decent googling skills, didn&amp;#39;t help much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"is_self":true,"edited":false,"id":"136rne","permalink":"/r/MachineLearning/comments/136rne/best_r_tutorialbook_for_machine_learning/","ups":31,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"thumbnail":"self","secure_media":null,"subreddit":"MachineLearning","title":"Best R tutorial/book for Machine Learning","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1352911804,"downs":0,"selftext":"I am in a ML and Data mining class right now and we are encouraged to use R for our assignments and projects. I've been using Weka with my supervisor.\n\nI have a decent coding background but I wasn't able to understand the syntax of R so I need to find a good tutorial or book to help me learn, probably from the near begining. \n\nIn our first assignment we were supposed to use RWeka and DMwR libraries but even after seeing the solution I still don't understand the code and syntax.\n\nFor our project we are to be pre-processing lots of text (which I could easily do in python) and then do ML on it to improve the classification rate. Since it is imbalanced data (10-90) I know I need to deal with stuff such as the area under the curve (which I can't do in Weka).\n\nSo please recomend me a good tutorial or book I can get to help me learn the syntax for R that touches upon ML related tasks.\n\nThanks\n\n","author_flair_css_class":null,"retrieved_on":1413376722,"url":"http://www.reddit.com/r/MachineLearning/comments/136rne/best_r_tutorialbook_for_machine_learning/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":18,"gilded":0,"score":31,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in a ML and Data mining class right now and we are encouraged to use R for our assignments and projects. I&amp;#39;ve been using Weka with my supervisor.&lt;/p&gt;\n\n&lt;p&gt;I have a decent coding background but I wasn&amp;#39;t able to understand the syntax of R so I need to find a good tutorial or book to help me learn, probably from the near begining. &lt;/p&gt;\n\n&lt;p&gt;In our first assignment we were supposed to use RWeka and DMwR libraries but even after seeing the solution I still don&amp;#39;t understand the code and syntax.&lt;/p&gt;\n\n&lt;p&gt;For our project we are to be pre-processing lots of text (which I could easily do in python) and then do ML on it to improve the classification rate. Since it is imbalanced data (10-90) I know I need to deal with stuff such as the area under the curve (which I can&amp;#39;t do in Weka).&lt;/p&gt;\n\n&lt;p&gt;So please recomend me a good tutorial or book I can get to help me learn the syntax for R that touches upon ML related tasks.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"tekesavvy","user_reports":[],"domain":"self.MachineLearning"}
{"url":"http://www.reddit.com/r/MachineLearning/comments/136rj8/best_r_tutorial_book_for_machine_learning/","subreddit_id":"t5_2r3gv","retrieved_on":1413376725,"downs":0,"selftext":"I am in a ML and Data mining class right now and we are encouraged to use R for our assignments and projects.  I've been using Weka with my supervisor.\n\nI have a decent coding background but I wasn't able to understand the syntax of R so I need to find a good tutorial or book to help me learn, probably from the near begining.  \n\nIn our first assignment we were supposed to use RWeka and DMwR libraries but even after seeing the solution I still don't understand the code and syntax.\n\nFor our project we are to be pre-processing lots of text (which I could easily do in python) and then do ML on it to improve the classification rate.  Since it is imbalanced data (10-90) I know I need to deal with stuff such as the area under the curve (which I can't do in Weka).\n\nSo please recomend me a good tutorial or book I can get to help me learn the syntax for R that touches upon ML related tasks.\n\nThanks","author_flair_css_class":null,"created_utc":1352911698,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Best R tutorial / book for Machine Learning ","banned_by":null,"author":"[deleted]","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in a ML and Data mining class right now and we are encouraged to use R for our assignments and projects.  I&amp;#39;ve been using Weka with my supervisor.&lt;/p&gt;\n\n&lt;p&gt;I have a decent coding background but I wasn&amp;#39;t able to understand the syntax of R so I need to find a good tutorial or book to help me learn, probably from the near begining.  &lt;/p&gt;\n\n&lt;p&gt;In our first assignment we were supposed to use RWeka and DMwR libraries but even after seeing the solution I still don&amp;#39;t understand the code and syntax.&lt;/p&gt;\n\n&lt;p&gt;For our project we are to be pre-processing lots of text (which I could easily do in python) and then do ML on it to improve the classification rate.  Since it is imbalanced data (10-90) I know I need to deal with stuff such as the area under the curve (which I can&amp;#39;t do in Weka).&lt;/p&gt;\n\n&lt;p&gt;So please recomend me a good tutorial or book I can get to help me learn the syntax for R that touches upon ML related tasks.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":0,"mod_reports":[],"media_embed":{},"stickied":false,"over_18":false,"ups":1,"media":null,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/136rj8/best_r_tutorial_book_for_machine_learning/","id":"136rj8","is_self":true,"edited":false,"secure_media":null,"thumbnail":"default"}
{"thumbnail":"self","secure_media":null,"edited":false,"is_self":true,"id":"136jfz","permalink":"/r/MachineLearning/comments/136jfz/how_to_deal_with_data_that_changes_over_time/","secure_media_embed":{},"report_reasons":null,"ups":1,"media":null,"stickied":false,"over_18":false,"media_embed":{},"mod_reports":[],"num_comments":0,"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to machine learning. I have read a little about the different types of classifying and regression algorithms that exist. I was hoping people could give me some suggestions (or suggest things to read) regarding how to deal with data that is changing over time, or that has a time element to it. ie. This might be sports statistics and would concern how to weigh data that is different at each round or event and how to weigh the other data fields across a time period.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"shogun333","user_reports":[],"title":"How to deal with data that changes over time","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1352903021,"author_flair_css_class":null,"selftext":"Hi,\n\nI'm new to machine learning. I have read a little about the different types of classifying and regression algorithms that exist. I was hoping people could give me some suggestions (or suggest things to read) regarding how to deal with data that is changing over time, or that has a time element to it. ie. This might be sports statistics and would concern how to weigh data that is different at each round or event and how to weigh the other data fields across a time period.","downs":0,"retrieved_on":1413377046,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/136jfz/how_to_deal_with_data_that_changes_over_time/"}
{"selftext":"","author_flair_css_class":null,"downs":0,"created_utc":1353020763,"subreddit_id":"t5_2r3gv","url":"http://littlestat.com","retrieved_on":1413372975,"link_flair_text":null,"distinguished":null,"title":"Calculate Statistics Online","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"score":17,"selftext_html":null,"gilded":0,"domain":"littlestat.com","author":"turnersr","user_reports":[],"num_comments":0,"mod_reports":[],"permalink":"/r/MachineLearning/comments/139lhv/calculate_statistics_online/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":17,"media":null,"is_self":false,"edited":false,"id":"139lhv","secure_media":null,"thumbnail":"default"}
{"secure_media":null,"thumbnail":"http://a.thumbs.redditmedia.com/4gzcFpWHg-eBcR10.jpg","permalink":"/r/MachineLearning/comments/139dgg/innovation_in_statistical_computing/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"edited":false,"is_self":false,"id":"139dgg","gilded":0,"score":1,"selftext_html":null,"domain":"r-bloggers.com","author":"talgalili","user_reports":[],"num_comments":0,"mod_reports":[],"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1353013945,"subreddit_id":"t5_2r3gv","url":"http://www.r-bloggers.com/innovation-in-statistical-computing/","retrieved_on":1413373264,"link_flair_text":null,"distinguished":null,"title":"Innovation in Statistical Computing","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null}
{"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/1398qz/topic_modeling_how_would_you_cluster_documents_by/","retrieved_on":1413373433,"author_flair_css_class":null,"selftext":"I'm trying to implement a method that takes a long list of search terms and groups them together by topic where the number of topics is not known in advance.\n\n**Example list: ** shoes for women, women shoes, cheap shoes, wholesale women shoes, cheap running shoes, shoes for cheap\n\n**Example Output Group 1: (Womens Shoes) ** shoes for women, women shoes, wholesale women shoes\n\n**Example Output Group 2: (Cheap Shoes) ** cheap shoes, cheap running shoes, shoes for cheap\n\nI'm trying to recreate the functionality of the **Ad Group Ideas (Beta)** found on (you must login for it to show) https://adwords.google.com/o/KeywordTool\n\nIn this use case and in Google's method a keyword must only belong to one topic.\n\nI've tried using LSA in R but it produces many topics with keywords belonging to multiple topics.\n\nAny ideas on how to approach this?\n-or-\nIs there some approach I can take on the output of LSA to hard cluster without knowing in advance the number of topics?","downs":0,"created_utc":1353009906,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"[Topic Modeling] How would you cluster documents by topics when the documents should belong to exclusively the most relevant topic?","subreddit":"MachineLearning","domain":"self.MachineLearning","user_reports":[],"author":"[deleted]","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to implement a method that takes a long list of search terms and groups them together by topic where the number of topics is not known in advance.&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example list: *&lt;/em&gt; shoes for women, women shoes, cheap shoes, wholesale women shoes, cheap running shoes, shoes for cheap&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example Output Group 1: (Womens Shoes) *&lt;/em&gt; shoes for women, women shoes, wholesale women shoes&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example Output Group 2: (Cheap Shoes) *&lt;/em&gt; cheap shoes, cheap running shoes, shoes for cheap&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to recreate the functionality of the &lt;strong&gt;Ad Group Ideas (Beta)&lt;/strong&gt; found on (you must login for it to show) &lt;a href=\"https://adwords.google.com/o/KeywordTool\"&gt;https://adwords.google.com/o/KeywordTool&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this use case and in Google&amp;#39;s method a keyword must only belong to one topic.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using LSA in R but it produces many topics with keywords belonging to multiple topics.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on how to approach this?\n-or-\nIs there some approach I can take on the output of LSA to hard cluster without knowing in advance the number of topics?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":0,"mod_reports":[],"stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":1,"media":null,"permalink":"/r/MachineLearning/comments/1398qz/topic_modeling_how_would_you_cluster_documents_by/","id":"1398qz","is_self":true,"edited":false,"secure_media":null,"thumbnail":"default"}
{"score":1,"selftext_html":null,"gilded":0,"domain":"waltherpragerandphilosophy.blogspot.ro","author":"[deleted]","user_reports":[],"num_comments":0,"mod_reports":[],"selftext":"","author_flair_css_class":null,"downs":0,"created_utc":1353003830,"subreddit_id":"t5_2r3gv","url":"http://waltherpragerandphilosophy.blogspot.ro/2012/03/note-on-oblivion.html","retrieved_on":1413373675,"link_flair_text":null,"distinguished":null,"title":"Oblivion","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"secure_media":null,"thumbnail":"default","permalink":"/r/MachineLearning/comments/139222/oblivion/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"is_self":false,"edited":false,"id":"139222"}
{"thumbnail":"self","secure_media":null,"permalink":"/r/MachineLearning/comments/1390uq/perspectivepose_estimation_of_marker_object/","media":null,"ups":2,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"edited":false,"is_self":true,"id":"1390uq","score":2,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello /MachineLearning, &lt;/p&gt;\n\n&lt;p&gt;I am interested in perspective/pose estimation of an object\nusing a Neural Network. &lt;/p&gt;\n\n&lt;p&gt;First of all this object, is a 2D marker (5x5cm flat sticker) pasted onto\na 1x1mtr flat surface. This marker is known as a fiducial marker, like the markers from this image;  &lt;a href=\"http://www.uco.es/investiga/grupos/ava/sites/default/files/images/imagenogre.jpg\"&gt;link!&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Detection of this object is no problem, but I have no idea on how to do a perspective estimation of this object using a ANN. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of other &amp;#39;ways&amp;#39; to estimate the perspective of my object. For instance; using OpenCV. However my current OpenCV implementation lacks precision, which I am hoping to get using a trained ANN (I could be entirely wrong tho&amp;#39;).&lt;/p&gt;\n\n&lt;p&gt;This lack of precision within my OpenCV implementation is partially due to a (slightly) blurry image of my object (blurry due to hyperfocal distance and camera distance).&lt;/p&gt;\n\n&lt;p&gt;My thoughts on how to do this; &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;Generate the marker, apply a (small) perspective transform, save the marker image and perspective transform to a training set. Repeat this step X number of times.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;*Develop  FF ANN (feed forward artificial neural network)  and train using the (above) training set. *&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;em&gt;Grab object from camera image, and identify the perspective transform using the FF ANN.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So /MachineLearning, what do you guys think ? Moving in the right\ndirection, or am I way of ? :o&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","user_reports":[],"author":"nphinity","domain":"self.MachineLearning","mod_reports":[],"num_comments":6,"created_utc":1353002711,"downs":0,"selftext":"Hello /MachineLearning, \n\nI am interested in perspective/pose estimation of an object\nusing a Neural Network. \n\nFirst of all this object, is a 2D marker (5x5cm flat sticker) pasted onto\na 1x1mtr flat surface. This marker is known as a fiducial marker, like the markers from this image;  [link!](http://www.uco.es/investiga/grupos/ava/sites/default/files/images/imagenogre.jpg)\n\nDetection of this object is no problem, but I have no idea on how to do a perspective estimation of this object using a ANN. \n\nI'm aware of other 'ways' to estimate the perspective of my object. For instance; using OpenCV. However my current OpenCV implementation lacks precision, which I am hoping to get using a trained ANN (I could be entirely wrong tho').\n\nThis lack of precision within my OpenCV implementation is partially due to a (slightly) blurry image of my object (blurry due to hyperfocal distance and camera distance).\n\n\nMy thoughts on how to do this; \n\n\n1. *Generate the marker, apply a (small) perspective transform, save the marker image and perspective transform to a training set. Repeat this step X number of times.*\n\n2. *Develop  FF ANN (feed forward artificial neural network)  and train using the (above) training set. *\n\n3. *Grab object from camera image, and identify the perspective transform using the FF ANN.*\n\n\nSo /MachineLearning, what do you guys think ? Moving in the right\ndirection, or am I way of ? :o\n\nThanks!\n","author_flair_css_class":null,"retrieved_on":1413373720,"url":"http://www.reddit.com/r/MachineLearning/comments/1390uq/perspectivepose_estimation_of_marker_object/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"Perspective/Pose estimation of marker object","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null}
{"secure_media":null,"thumbnail":"self","permalink":"/r/MachineLearning/comments/138xep/eli5_latent_dirichlet_allocation_can_you_explain/","media_embed":{},"over_18":false,"stickied":false,"ups":14,"report_reasons":null,"media":null,"secure_media_embed":{},"edited":false,"is_self":true,"id":"138xep","gilded":0,"score":14,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to ML would appreciate the help. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","user_reports":[],"author":"umdebaba","domain":"self.MachineLearning","num_comments":4,"mod_reports":[],"downs":0,"selftext":"New to ML would appreciate the help. Thanks in advance!","author_flair_css_class":null,"created_utc":1352999671,"url":"http://www.reddit.com/r/MachineLearning/comments/138xep/eli5_latent_dirichlet_allocation_can_you_explain/","subreddit_id":"t5_2r3gv","retrieved_on":1413373840,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"[ELI5] -Latent Dirichlet Allocation. Can you explain it like I'm five?","banned_by":null,"link_flair_css_class":null,"author_flair_text":null}
{"permalink":"/r/MachineLearning/comments/138x38/how_to_visualise_this_data/","ups":1,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"edited":false,"is_self":true,"id":"138x38","thumbnail":"self","secure_media":null,"created_utc":1352999365,"downs":0,"author_flair_css_class":null,"selftext":"Ok so I have 2 categorical variables and two percentages and I am really struggling with how to visualise these appropriately. The data is of the form: \n\nfields_per_rule, rules_per_class, test accuracy,training accuracy\n\n1\t1\t0\t0   \n\n1\t13\t45.293\t92\n\n1\t38\t42.7539\t86\n\n1\t50\t43.3398\t86\n\n16\t1\t8.71094\t30\n\n16\t13\t20.8789\t93\n\n16\t38\t34.6484\t99\n\n16\t50\t38.125\t100\n\n48\t1\t0.195312\t11\n\n48\t13\t0.644531\t77\n\n48\t38\t1.42578\t99\n\n48\t50\t2.10938\t99\n\n64\t1\t0.0195312\t11\n\n64\t13\t0.234375\t82\n\n64\t38\t0.351562\t99\n\n64\t50\t0.664062\t99\n\nI have results for varying sizes of training and test sets. So far I have created scatter plots of rules_per_class and fields_per_rule against percentage, but these turned out to look pretty horrible. Any help appreciated. ","retrieved_on":1413373851,"url":"http://www.reddit.com/r/MachineLearning/comments/138x38/how_to_visualise_this_data/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"How to visualise this data ? ","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok so I have 2 categorical variables and two percentages and I am really struggling with how to visualise these appropriately. The data is of the form: &lt;/p&gt;\n\n&lt;p&gt;fields_per_rule, rules_per_class, test accuracy,training accuracy&lt;/p&gt;\n\n&lt;p&gt;1   1   0   0   &lt;/p&gt;\n\n&lt;p&gt;1   13  45.293  92&lt;/p&gt;\n\n&lt;p&gt;1   38  42.7539 86&lt;/p&gt;\n\n&lt;p&gt;1   50  43.3398 86&lt;/p&gt;\n\n&lt;p&gt;16  1   8.71094 30&lt;/p&gt;\n\n&lt;p&gt;16  13  20.8789 93&lt;/p&gt;\n\n&lt;p&gt;16  38  34.6484 99&lt;/p&gt;\n\n&lt;p&gt;16  50  38.125  100&lt;/p&gt;\n\n&lt;p&gt;48  1   0.195312    11&lt;/p&gt;\n\n&lt;p&gt;48  13  0.644531    77&lt;/p&gt;\n\n&lt;p&gt;48  38  1.42578 99&lt;/p&gt;\n\n&lt;p&gt;48  50  2.10938 99&lt;/p&gt;\n\n&lt;p&gt;64  1   0.0195312   11&lt;/p&gt;\n\n&lt;p&gt;64  13  0.234375    82&lt;/p&gt;\n\n&lt;p&gt;64  38  0.351562    99&lt;/p&gt;\n\n&lt;p&gt;64  50  0.664062    99&lt;/p&gt;\n\n&lt;p&gt;I have results for varying sizes of training and test sets. So far I have created scatter plots of rules_per_class and fields_per_rule against percentage, but these turned out to look pretty horrible. Any help appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"author":"WayUpLow","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":0}
{"author":"leex1867","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all, thanks a lot to everyone for your help yesterday. &lt;/p&gt;\n\n&lt;p&gt;I rewrote my code so that it uses arrays of arrays instead of data objects (I chose arrays of arrays instead of rectangular arrays to more easily take entire rows from the matrix, any comments on which is better?) and I only abstracted away the initial data read and matrix multiplication.  The code should be much easier to read now :)&lt;/p&gt;\n\n&lt;p&gt;To summarize the problems I was seeing yesterday, my program appeared to be reducing errors, but unless I use a very small value for alpha, my weights explode to infinity.  Also, the resulting predictions were way off.  This is the equation I am trying to implement:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://i.imgur.com/qVRfN.png\"&gt;http://i.imgur.com/qVRfN.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My understanding of the equation:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://i.imgur.com/CH9qw.jpg\"&gt;http://i.imgur.com/CH9qw.jpg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Here is my new code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    static void Main(string[] args) {\n        // 0 Baseline\n        // 1 Cylinders\n        // 2 Displacement\n        // 3 HorsePower\n        // 4 Weight\n        // 5 Acceleration\n        // 6 ModelYear\n\n        var theta = new double[] { 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, };\n        var x = Importer.GetX(); // double[][] representing features\n        var y = Importer.GetY(); // double[] representing mpg\n        var alpha = 0.00000000001;\n\n        var iterations = 1000;\n        var rows = x.Length;\n        var featureCount = theta.Length;\n\n        for (int i = 0; i &amp;lt; iterations; i++) { // i loops through each theta\n            var errorSum = new double[featureCount];\n            int k;\n            for (int j = 0; j &amp;lt; rows; j++) { // j loops through each row in our data\n                var errorFactor = y[j] - GetH(x[j], theta);\n\n                for (k = 0; k &amp;lt; featureCount; k++) { // k loops through each column (feature) of our data\n                    errorSum[k] += errorFactor * x[j][k];\n                }\n            }\n\n            for (k = 0; k &amp;lt; featureCount; k++) { // k loops through each column (feature) of our data\n                theta[k] += errorSum[k] * alpha;\n            }\n        }\n    }\n\n    static double GetH(double[] row, double[] theta) {\n        return Matrix.Matrix.Multiply(row, theta);\n    }\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thank you all for your help, I feel so close to the answer!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":6,"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/138ux4/gradient_descent_take_2_new_code/","subreddit_id":"t5_2r3gv","retrieved_on":1413373939,"downs":0,"author_flair_css_class":null,"selftext":"First of all, thanks a lot to everyone for your help yesterday. \n\nI rewrote my code so that it uses arrays of arrays instead of data objects (I chose arrays of arrays instead of rectangular arrays to more easily take entire rows from the matrix, any comments on which is better?) and I only abstracted away the initial data read and matrix multiplication.  The code should be much easier to read now :)\n\nTo summarize the problems I was seeing yesterday, my program appeared to be reducing errors, but unless I use a very small value for alpha, my weights explode to infinity.  Also, the resulting predictions were way off.  This is the equation I am trying to implement:\n\nhttp://i.imgur.com/qVRfN.png\n\nMy understanding of the equation:\n\nhttp://i.imgur.com/CH9qw.jpg\n\nHere is my new code:\n\n        static void Main(string[] args) {\n            // 0 Baseline\n            // 1 Cylinders\n            // 2 Displacement\n            // 3 HorsePower\n            // 4 Weight\n            // 5 Acceleration\n            // 6 ModelYear\n\n            var theta = new double[] { 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, };\n            var x = Importer.GetX(); // double[][] representing features\n            var y = Importer.GetY(); // double[] representing mpg\n            var alpha = 0.00000000001;\n\n            var iterations = 1000;\n            var rows = x.Length;\n            var featureCount = theta.Length;\n            \n            for (int i = 0; i &lt; iterations; i++) { // i loops through each theta\n                var errorSum = new double[featureCount];\n                int k;\n                for (int j = 0; j &lt; rows; j++) { // j loops through each row in our data\n                    var errorFactor = y[j] - GetH(x[j], theta);\n\n                    for (k = 0; k &lt; featureCount; k++) { // k loops through each column (feature) of our data\n                        errorSum[k] += errorFactor * x[j][k];\n                    }\n                }\n\n                for (k = 0; k &lt; featureCount; k++) { // k loops through each column (feature) of our data\n                    theta[k] += errorSum[k] * alpha;\n                }\n            }\n        }\n\n        static double GetH(double[] row, double[] theta) {\n            return Matrix.Matrix.Multiply(row, theta);\n        }\n\nThank you all for your help, I feel so close to the answer!","created_utc":1352997472,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Gradient Descent Take 2 (new code)","secure_media":null,"thumbnail":"self","media_embed":{},"stickied":false,"over_18":false,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/138ux4/gradient_descent_take_2_new_code/","id":"138ux4","is_self":true,"edited":1352998893}
{"permalink":"/r/MachineLearning/comments/138ubw/standard_data_sets_for_predictive_neural_networks/","media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":9,"report_reasons":null,"secure_media_embed":{},"edited":false,"is_self":true,"id":"138ubw","secure_media":null,"thumbnail":"self","downs":0,"author_flair_css_class":null,"selftext":"Hi everyone. I've build a NN for a class research project and I'm wondering what data sets are the \"gold standard\" to use to test it against. I'm primarily interested in prediction and not classification.\n\nThanks for any help. ","created_utc":1352996956,"url":"http://www.reddit.com/r/MachineLearning/comments/138ubw/standard_data_sets_for_predictive_neural_networks/","subreddit_id":"t5_2r3gv","retrieved_on":1413373960,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"standard data sets for predictive neural networks","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"score":9,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I&amp;#39;ve build a NN for a class research project and I&amp;#39;m wondering what data sets are the &amp;quot;gold standard&amp;quot; to use to test it against. I&amp;#39;m primarily interested in prediction and not classification.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any help. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"parisjackson2","user_reports":[],"domain":"self.MachineLearning","num_comments":25,"mod_reports":[]}
{"edited":false,"is_self":false,"id":"137nlk","permalink":"/r/MachineLearning/comments/137nlk/why_do_scientists_continue_supporting_the_kaggle/","media_embed":{"height":338,"width":600,"scrolling":false,"content":"&lt;iframe src=\"http://player.vimeo.com/video/53095017\" width=\"600\" height=\"338\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;"},"stickied":false,"over_18":false,"ups":49,"report_reasons":null,"media":{"type":"vimeo.com","oembed":{"title":"Lightning Talk - Mike Selik - End of Kaggle","thumbnail_url":"http://b.vimeocdn.com/ts/366/584/366584169_1280.jpg","thumbnail_width":1280,"author_url":"http://vimeo.com/continuumanalytics","version":"1.0","author_name":"Continuum Analytics","width":600,"description":"Vimeo is the home for high-quality videos and the people who love them.","html":"&lt;iframe src=\"http://player.vimeo.com/video/53095017\" width=\"600\" height=\"338\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;","type":"video","thumbnail_height":720,"height":338,"provider_url":"http://vimeo.com/","provider_name":"Vimeo"}},"secure_media_embed":{},"secure_media":null,"thumbnail":"http://a.thumbs.redditmedia.com/bXOo4yEIkoJvQrLN.jpg","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Why do scientists continue supporting the Kaggle model? Kaggle = scientists exploited.","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"downs":0,"author_flair_css_class":null,"selftext":"","created_utc":1352940511,"url":"http://vimeo.com/53095017","subreddit_id":"t5_2r3gv","retrieved_on":1413375539,"num_comments":68,"mod_reports":[],"score":49,"selftext_html":null,"gilded":0,"user_reports":[],"author":"rhiever","domain":"vimeo.com"}
{"edited":false,"is_self":false,"id":"13blz2","permalink":"/r/MachineLearning/comments/13blz2/early_detection_of_twitter_trends_explained/","report_reasons":null,"ups":52,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"thumbnail":"http://f.thumbs.redditmedia.com/qvgneJt3kibzRYu5.jpg","secure_media":null,"subreddit":"MachineLearning","banned_by":null,"title":"Early detection of Twitter trends explained","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1353104745,"downs":0,"selftext":"","author_flair_css_class":null,"retrieved_on":1413370318,"url":"http://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":27,"selftext_html":null,"score":52,"gilded":0,"author":"qvadis","user_reports":[],"domain":"snikolov.wordpress.com"}
{"permalink":"/r/MachineLearning/comments/13bl5k/tutorials_for_weka_imbalanced_data_roc_curves_and/","media_embed":{},"over_18":false,"stickied":false,"report_reasons":null,"ups":4,"media":null,"secure_media_embed":{},"is_self":true,"edited":false,"id":"13bl5k","secure_media":null,"thumbnail":"self","downs":0,"selftext":"I am working on a classification project that has imbalanced data, (90-10%) and I am working on the classification rate.  I understand the basics of how to use Weka explorer.\n\nSince this data is imbalanced I need to look at the ROC curves and the AUC to determine if I am actually improving my classification rate (and not just labeling everything with the 90% label).\n\nDo you know of any good tutorials that deal with this in Weka?  If not, what about in R (though I have little experience with this).\n\nThanks","author_flair_css_class":null,"created_utc":1353103942,"url":"http://www.reddit.com/r/MachineLearning/comments/13bl5k/tutorials_for_weka_imbalanced_data_roc_curves_and/","subreddit_id":"t5_2r3gv","retrieved_on":1413370346,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Tutorials for Weka, imbalanced data, ROC Curves and AUC?","link_flair_css_class":null,"author_flair_text":null,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a classification project that has imbalanced data, (90-10%) and I am working on the classification rate.  I understand the basics of how to use Weka explorer.&lt;/p&gt;\n\n&lt;p&gt;Since this data is imbalanced I need to look at the ROC curves and the AUC to determine if I am actually improving my classification rate (and not just labeling everything with the 90% label).&lt;/p&gt;\n\n&lt;p&gt;Do you know of any good tutorials that deal with this in Weka?  If not, what about in R (though I have little experience with this).&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":4,"gilded":0,"user_reports":[],"author":"tekesavvy","domain":"self.MachineLearning","num_comments":2,"mod_reports":[]}
{"retrieved_on":1413371902,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13aex5/book_for_learning_statistics_and_probability/","created_utc":1353050215,"author_flair_css_class":null,"selftext":"I am new to Machine learning and I find it hard to understand many of the probability and statistical approaches. I don't have a proper probability and statistical background and I want to gain that understanding. Especially topics like markov chains, markov random fields [used a lot in computer vision] go over the top of my head. Can someone please recommend me some good books [assuming I have very little mathematical background] to cover these topics?","downs":0,"author_flair_text":null,"link_flair_css_class":null,"banned_by":null,"title":"Book for learning statistics and probability theory to help developer deeper understanding of machine learning algorithms","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"domain":"self.MachineLearning","user_reports":[],"author":"S1r1usBl4ck","gilded":0,"score":12,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am new to Machine learning and I find it hard to understand many of the probability and statistical approaches. I don&amp;#39;t have a proper probability and statistical background and I want to gain that understanding. Especially topics like markov chains, markov random fields [used a lot in computer vision] go over the top of my head. Can someone please recommend me some good books [assuming I have very little mathematical background] to cover these topics?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"num_comments":12,"secure_media_embed":{},"ups":12,"report_reasons":null,"media":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/13aex5/book_for_learning_statistics_and_probability/","id":"13aex5","edited":false,"is_self":true,"thumbnail":"self","secure_media":null}
{"num_comments":25,"mod_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi r/machineLearning!&lt;/p&gt;\n\n&lt;p&gt;So, after having dabbled here and there in machine learning for some time now, I think I now know what I am truly interested in. It took me some time to figure it out, but I needed to survey the landscape first. &lt;/p&gt;\n\n&lt;p&gt;I want to really dive into _un_supervised learning. This has a two fold advantage for me, one, I am very interested in the subject, and two, I believe I will be able to use it in my line of work which is very signal-processing intensive. &lt;/p&gt;\n\n&lt;p&gt;So, quite simply, I wanted to ask any of you, and those more experienced than me, for an &amp;quot;executive summary&amp;quot; or list, of unsupervised learning algorithms. &lt;/p&gt;\n\n&lt;p&gt;(If you like, feel free to add a brief run down of your thoughts for each one on the list. What are their strengths? What might be some disadvantages? Pitfalls? Can you sprinkle some intuition on each one?)&lt;/p&gt;\n\n&lt;p&gt;From the list, I am fairly certain that I will be able to do enough focused research and learn them all very well eventually. I just need a starting point list. &lt;/p&gt;\n\n&lt;p&gt;(I have already taken Andrew Ngs class on coursera). &lt;/p&gt;\n\n&lt;p&gt;So far my list includes: &lt;/p&gt;\n\n&lt;p&gt;1) K-Means&lt;/p&gt;\n\n&lt;p&gt;2) ICA&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":16,"gilded":0,"user_reports":[],"author":"Ayakalam","domain":"self.MachineLearning","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"List of unsupervised learning algorithms. ","link_flair_css_class":null,"author_flair_text":null,"downs":0,"selftext":"\nHi r/machineLearning!\n\n\nSo, after having dabbled here and there in machine learning for some time now, I think I now know what I am truly interested in. It took me some time to figure it out, but I needed to survey the landscape first. \n\n\nI want to really dive into _un_supervised learning. This has a two fold advantage for me, one, I am very interested in the subject, and two, I believe I will be able to use it in my line of work which is very signal-processing intensive. \n\n\nSo, quite simply, I wanted to ask any of you, and those more experienced than me, for an \"executive summary\" or list, of unsupervised learning algorithms. \n\n\n(If you like, feel free to add a brief run down of your thoughts for each one on the list. What are their strengths? What might be some disadvantages? Pitfalls? Can you sprinkle some intuition on each one?)\n\n\nFrom the list, I am fairly certain that I will be able to do enough focused research and learn them all very well eventually. I just need a starting point list. \n\n\n(I have already taken Andrew Ngs class on coursera). \n\n\nSo far my list includes: \n\n1) K-Means\n\n2) ICA\n\n\nThanks in advance!! \n\n\n","author_flair_css_class":null,"created_utc":1353039429,"url":"http://www.reddit.com/r/MachineLearning/comments/13a5xi/list_of_unsupervised_learning_algorithms/","subreddit_id":"t5_2r3gv","retrieved_on":1413372226,"secure_media":null,"thumbnail":"self","is_self":true,"edited":false,"id":"13a5xi","permalink":"/r/MachineLearning/comments/13a5xi/list_of_unsupervised_learning_algorithms/","media_embed":{},"stickied":false,"over_18":false,"report_reasons":null,"ups":16,"media":null,"secure_media_embed":{}}
{"num_comments":0,"mod_reports":[],"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to implement a method that takes a long list of search terms and groups them together by topic where the number of topics is not known in advance.&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example list: *&lt;/em&gt; shoes for women, women shoes, cheap shoes, wholesale women shoes, cheap running shoes, shoes for cheap&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example Output Group 1: (Womens Shoes) *&lt;/em&gt; shoes for women, women shoes, wholesale women shoes&lt;/p&gt;\n\n&lt;p&gt;*&lt;em&gt;Example Output Group 2: (Cheap Shoes) *&lt;/em&gt; cheap shoes, cheap running shoes, shoes for cheap&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to recreate the functionality of the &lt;strong&gt;Ad Group Ideas (Beta)&lt;/strong&gt; found on (you must login for it to show) &lt;a href=\"https://adwords.google.com/o/KeywordTool\"&gt;https://adwords.google.com/o/KeywordTool&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this use case and in Google&amp;#39;s method a keyword must only belong to one topic.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried using LSA in R but it produces many topics with keywords belonging to multiple topics.&lt;/p&gt;\n\n&lt;p&gt;Any ideas on how to approach this?\n-or-\nIs there some approach I can take on the output of LSA to hard cluster without knowing in advance the number of topics?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"umdebaba","user_reports":[],"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"[Topic Modeling] How to implement Google Ad Group Ideas (beta) functionality?","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"selftext":"I'm trying to implement a method that takes a long list of search terms and groups them together by topic where the number of topics is not known in advance.\n\n**Example list: ** shoes for women, women shoes, cheap shoes, wholesale women shoes, cheap running shoes, shoes for cheap\n\n**Example Output Group 1: (Womens Shoes) ** shoes for women, women shoes, wholesale women shoes\n\n**Example Output Group 2: (Cheap Shoes) ** cheap shoes, cheap running shoes, shoes for cheap\n\nI'm trying to recreate the functionality of the **Ad Group Ideas (Beta)** found on (you must login for it to show) https://adwords.google.com/o/KeywordTool\n\nIn this use case and in Google's method a keyword must only belong to one topic.\n\nI've tried using LSA in R but it produces many topics with keywords belonging to multiple topics.\n\nAny ideas on how to approach this?\n-or-\nIs there some approach I can take on the output of LSA to hard cluster without knowing in advance the number of topics?","author_flair_css_class":null,"downs":0,"created_utc":1353029028,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/139ueo/topic_modeling_how_to_implement_google_ad_group/","retrieved_on":1413372649,"secure_media":null,"thumbnail":"self","edited":false,"is_self":true,"id":"139ueo","permalink":"/r/MachineLearning/comments/139ueo/topic_modeling_how_to_implement_google_ad_group/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":1,"media":null,"report_reasons":null}
{"id":"13cjeb","edited":false,"is_self":true,"media_embed":{},"stickied":false,"over_18":false,"report_reasons":null,"ups":1,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/13cjeb/generating_a_multiple_choice_quiz_from_content_to/","secure_media":null,"thumbnail":"self","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Generating a multiple choice quiz from content to determine a users level of knowledge","banned_by":null,"url":"http://www.reddit.com/r/MachineLearning/comments/13cjeb/generating_a_multiple_choice_quiz_from_content_to/","subreddit_id":"t5_2r3gv","retrieved_on":1413369107,"downs":0,"selftext":"Wondering if any of you guys have seen any solutions that address this problem.\n\nWe need a solution that will generate multiple choice quiz answers based on a set of questions and answers\n\nEg\n\nQuestion: What colors are tshirts available in?\n\nAnswer:        Green, blue and Red\n\n\nI'm looking to build some software that would to generate a number of answers which include 1 correct answer and 3 incorrect answers\n\n\n* A; Purple and Black\n* B: Green, blue and Red\n* C: Yellow, red and small\n* D: Happy, Sad and Grey\n\n\nHas anyone seen anything like this?\n\n\nHow should i go about solving this?","author_flair_css_class":null,"created_utc":1353151612,"num_comments":0,"mod_reports":[],"author":"nickhac","user_reports":[],"domain":"self.MachineLearning","score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering if any of you guys have seen any solutions that address this problem.&lt;/p&gt;\n\n&lt;p&gt;We need a solution that will generate multiple choice quiz answers based on a set of questions and answers&lt;/p&gt;\n\n&lt;p&gt;Eg&lt;/p&gt;\n\n&lt;p&gt;Question: What colors are tshirts available in?&lt;/p&gt;\n\n&lt;p&gt;Answer:        Green, blue and Red&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to build some software that would to generate a number of answers which include 1 correct answer and 3 incorrect answers&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A; Purple and Black&lt;/li&gt;\n&lt;li&gt;B: Green, blue and Red&lt;/li&gt;\n&lt;li&gt;C: Yellow, red and small&lt;/li&gt;\n&lt;li&gt;D: Happy, Sad and Grey&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Has anyone seen anything like this?&lt;/p&gt;\n\n&lt;p&gt;How should i go about solving this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0}
{"is_self":true,"edited":false,"id":"13gjk5","permalink":"/r/MachineLearning/comments/13gjk5/machine_learning_grad_schools_in_europe/","ups":8,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"thumbnail":"self","secure_media":null,"subreddit":"MachineLearning","banned_by":null,"title":"Machine Learning grad schools in Europe","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1353347090,"downs":0,"author_flair_css_class":null,"selftext":"Hello everyone,\nI'm a senior student in Computer Science. And I'm thinking to apply for a master's degree. I'm having trouble choosing the university. If any of you is a student at one of those universities and if you share your experience, that would be great.","retrieved_on":1413363870,"url":"http://www.reddit.com/r/MachineLearning/comments/13gjk5/machine_learning_grad_schools_in_europe/","subreddit_id":"t5_2r3gv","mod_reports":[],"num_comments":23,"score":8,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,\nI&amp;#39;m a senior student in Computer Science. And I&amp;#39;m thinking to apply for a master&amp;#39;s degree. I&amp;#39;m having trouble choosing the university. If any of you is a student at one of those universities and if you share your experience, that would be great.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"grozar","user_reports":[],"domain":"self.MachineLearning"}
{"permalink":"/r/MachineLearning/comments/13j0y7/optimal_descriptive_nfl_rankings/","media":null,"ups":18,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"edited":false,"is_self":false,"id":"13j0y7","thumbnail":"default","secure_media":null,"created_utc":1353445104,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413360438,"url":"http://seanjtaylor.com/post/36149816687/optimal-descriptive-nfl-rankings","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","title":"Optimal Descriptive NFL Rankings","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":18,"gilded":0,"selftext_html":null,"user_reports":[],"author":"agconway","domain":"seanjtaylor.com","mod_reports":[],"num_comments":2}
{"permalink":"/r/MachineLearning/comments/13iczq/very_successful_model_builder_on_why_machines/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":0,"media":null,"is_self":false,"edited":false,"id":"13iczq","secure_media":null,"thumbnail":"http://c.thumbs.redditmedia.com/ulvLVcNzKvCEoepD.jpg","selftext":"","author_flair_css_class":null,"downs":0,"created_utc":1353421962,"subreddit_id":"t5_2r3gv","url":"http://blog.ad-tech.com/understanding-understanding-why-machine-learning-cant-work-without-humans","retrieved_on":1413361343,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Very successful model builder on why machines can't work without humans","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"gilded":0,"score":0,"selftext_html":null,"domain":"blog.ad-tech.com","author":"rrenaud","user_reports":[],"num_comments":17,"mod_reports":[]}
{"mod_reports":[],"num_comments":13,"author":"capt_compile2","user_reports":[],"domain":"self.MachineLearning","score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a Computer Science and Electrical Engineering undergrad and I&amp;#39;ve started becoming really interested in BCI. How can I get involved/get started with BCI? I&amp;#39;m guessing the hard part here is the EEG classification? How can I get started with that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"How can I get started with non-invasive brain computer interfaces?","banned_by":null,"distinguished":null,"link_flair_text":null,"retrieved_on":1413361528,"url":"http://www.reddit.com/r/MachineLearning/comments/13i8h7/how_can_i_get_started_with_noninvasive_brain/","subreddit_id":"t5_2r3gv","created_utc":1353414764,"downs":0,"selftext":"Hi\n\nI'm a Computer Science and Electrical Engineering undergrad and I've started becoming really interested in BCI. How can I get involved/get started with BCI? I'm guessing the hard part here is the EEG classification? How can I get started with that?","author_flair_css_class":null,"thumbnail":"self","secure_media":null,"id":"13i8h7","is_self":true,"edited":false,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/13i8h7/how_can_i_get_started_with_noninvasive_brain/"}
{"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"permalink":"/r/MachineLearning/comments/13i4pg/hire_buy_sell_dingo_hire/","id":"13i4pg","is_self":false,"edited":false,"thumbnail":"default","secure_media":null,"retrieved_on":1413361669,"url":"http://hirebuysell.com.au/listing/112/dingo_hire.html","subreddit_id":"t5_2r3gv","created_utc":1353406507,"downs":0,"author_flair_css_class":null,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Hire Buy Sell: DINGO HIRE","distinguished":null,"link_flair_text":null,"author":"hirebuysell","user_reports":[],"domain":"hirebuysell.com.au","gilded":0,"score":1,"selftext_html":null,"mod_reports":[],"num_comments":0}
{"author_flair_text":null,"link_flair_css_class":null,"title":"AUC of imbalanced data","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"retrieved_on":1413352308,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13orn2/auc_of_imbalanced_data/","created_utc":1353711125,"selftext":"I understand if you look at the AUC of a ROC curve of a balanced data set it should be over 0.5 otherwise your classifier is horrible.\n\nNow what happens if you have imbalanced data?  Say 90%-10%?  Does the AUC have to be over 90%?  Over 50%?  How does it work, how does it change and how do you calculate the new ratio?","author_flair_css_class":null,"downs":0,"mod_reports":[],"num_comments":2,"domain":"self.MachineLearning","author":"PurpleHydra","user_reports":[],"gilded":0,"score":2,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand if you look at the AUC of a ROC curve of a balanced data set it should be over 0.5 otherwise your classifier is horrible.&lt;/p&gt;\n\n&lt;p&gt;Now what happens if you have imbalanced data?  Say 90%-10%?  Does the AUC have to be over 90%?  Over 50%?  How does it work, how does it change and how do you calculate the new ratio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","id":"13orn2","is_self":true,"edited":false,"secure_media_embed":{},"ups":2,"report_reasons":null,"media":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/13orn2/auc_of_imbalanced_data/","thumbnail":"self","secure_media":null}
{"domain":"self.MachineLearning","user_reports":[],"author":"parisjackson2","score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve built a NN and sometimes while I&amp;#39;m training it the average of the errors between the computed outputs and the real outputs will go down to a point and then start to rise. &lt;/p&gt;\n\n&lt;p&gt;For example -\nSay there are 50 training rows and the average difference between my results and the real results is:\nIteration 1)0.019\nIteration 2)0.018\nIteration 3)0.017\nIteration 4)0.016\nIteration 5)0.015\nIteration 6)0.016\nIteration 7)0.017\nIteration 8)0.018\nIteration 9)0.019\nIteration 10)0.02&lt;/p&gt;\n\n&lt;p&gt;Does anyone know why this would occur? I would think the errors would quickly go back down but they don&amp;#39;t seem to.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"mod_reports":[],"num_comments":1,"retrieved_on":1413352593,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13oks3/what_would_cause_neural_network_errors_to/","created_utc":1353703582,"author_flair_css_class":null,"selftext":"I've built a NN and sometimes while I'm training it the average of the errors between the computed outputs and the real outputs will go down to a point and then start to rise. \n\nFor example -\nSay there are 50 training rows and the average difference between my results and the real results is:\nIteration 1)0.019\nIteration 2)0.018\nIteration 3)0.017\nIteration 4)0.016\nIteration 5)0.015\nIteration 6)0.016\nIteration 7)0.017\nIteration 8)0.018\nIteration 9)0.019\nIteration 10)0.02\n\nDoes anyone know why this would occur? I would think the errors would quickly go back down but they don't seem to.","downs":0,"author_flair_text":null,"link_flair_css_class":null,"title":"What would cause Neural Network errors to decrease then increase while training?","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"thumbnail":"self","secure_media":null,"secure_media_embed":{},"ups":0,"media":null,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/13oks3/what_would_cause_neural_network_errors_to/","id":"13oks3","is_self":true,"edited":false}
{"permalink":"/r/MachineLearning/comments/13o8t4/i_want_to_build_a_crime_index_and_political/","secure_media_embed":{},"report_reasons":null,"ups":17,"media":null,"stickied":false,"over_18":false,"media_embed":{},"edited":false,"is_self":true,"id":"13o8t4","thumbnail":"self","secure_media":null,"created_utc":1353690673,"author_flair_css_class":null,"selftext":"Hello, I have this side project where I crawl the local news websites in my country and want to build a crime index and political instability index.\n\nI have already covered the information retrieval part of the project. My plan is:\n\n1. Unsupervised topic extraction.\n2. Near duplicates detection.\n3. Supervised classification and incident level (crime/political - high/medium/low).\n\nI will use python and sklearn and have already research the algorithms that I can use for those tasks. I think 1 - 2 could give me a relevancy factor of a story: the more news papers publish about an story or topic the more relevant.\n\nMy next step is to build the monthly, weekly and daily index (nation-wide and per cities) based on the features that I have, and I'm a little lost here as the \"instability sensitivity\" might increase to the time. I mean, the index from the major instability incident of the last year could be less than the index for this year. Also if to use fixed scale 0-100 or not.\n\nI would appreciate any pointer to a paper, relevant readings or thoughts.\n\nThanks.","downs":0,"retrieved_on":1413353065,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13o8t4/i_want_to_build_a_crime_index_and_political/","banned_by":null,"title":"I want to build a crime index and political instability index based in news stories.","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"gilded":0,"score":17,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have this side project where I crawl the local news websites in my country and want to build a crime index and political instability index.&lt;/p&gt;\n\n&lt;p&gt;I have already covered the information retrieval part of the project. My plan is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Unsupervised topic extraction.&lt;/li&gt;\n&lt;li&gt;Near duplicates detection.&lt;/li&gt;\n&lt;li&gt;Supervised classification and incident level (crime/political - high/medium/low).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I will use python and sklearn and have already research the algorithms that I can use for those tasks. I think 1 - 2 could give me a relevancy factor of a story: the more news papers publish about an story or topic the more relevant.&lt;/p&gt;\n\n&lt;p&gt;My next step is to build the monthly, weekly and daily index (nation-wide and per cities) based on the features that I have, and I&amp;#39;m a little lost here as the &amp;quot;instability sensitivity&amp;quot; might increase to the time. I mean, the index from the major instability incident of the last year could be less than the index for this year. Also if to use fixed scale 0-100 or not.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any pointer to a paper, relevant readings or thoughts.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","user_reports":[],"author":"alzwke","mod_reports":[],"num_comments":11}
{"id":"13qdef","edited":false,"is_self":true,"media":null,"ups":5,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/13qdef/methods_to_choose_kernel_points_during_learning/","thumbnail":"self","secure_media":null,"link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Methods to choose kernel points during learning","distinguished":null,"link_flair_text":null,"retrieved_on":1413349920,"url":"http://www.reddit.com/r/MachineLearning/comments/13qdef/methods_to_choose_kernel_points_during_learning/","subreddit_id":"t5_2r3gv","created_utc":1353796099,"downs":0,"selftext":"Hi,\n\nWhen using kernels in a machine learning problem, each data point is generally chosen as a center for a kernel basis function. For a huge training set, this can lead to a huge number of parameters which need to be trained. What are some commonly used methods to select some subset of the data which can be used for prediction? I know that some sparse kernel methods (like SVM) can be used for some problems, but are there generic ways to deal with this (which can work with classification, regression, and any choice of loss function)?\n\nThanks!","author_flair_css_class":null,"mod_reports":[],"num_comments":6,"author":"chip_0","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":5,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;When using kernels in a machine learning problem, each data point is generally chosen as a center for a kernel basis function. For a huge training set, this can lead to a huge number of parameters which need to be trained. What are some commonly used methods to select some subset of the data which can be used for prediction? I know that some sparse kernel methods (like SVM) can be used for some problems, but are there generic ways to deal with this (which can work with classification, regression, and any choice of loss function)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"id":"13qc8o","is_self":true,"edited":false,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":1,"media":null,"permalink":"/r/MachineLearning/comments/13qc8o/methods_to_choosing_kernel_points_during_learning/","secure_media":null,"thumbnail":"default","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Methods to choosing kernel points during learning","subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13qc8o/methods_to_choosing_kernel_points_during_learning/","retrieved_on":1413349968,"author_flair_css_class":null,"selftext":"Hi,\n\nWhen using kernels in a machine learning problem, each data point is generally chosen as a center for a kernel basis function. For a huge training set, this can lead to a huge number of parameters which need to be trained. What are some commonly used methods to select some subset of the data which can be used for prediction? I know that some sparse kernel methods (like SVM) can be used for some problems, but are there generic ways to deal with this (which can work with classification, regression, and any choice of loss function)?\n\nThanks!","downs":0,"created_utc":1353794868,"num_comments":0,"mod_reports":[],"domain":"self.MachineLearning","user_reports":[],"author":"[deleted]","score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;When using kernels in a machine learning problem, each data point is generally chosen as a center for a kernel basis function. For a huge training set, this can lead to a huge number of parameters which need to be trained. What are some commonly used methods to select some subset of the data which can be used for prediction? I know that some sparse kernel methods (like SVM) can be used for some problems, but are there generic ways to deal with this (which can work with classification, regression, and any choice of loss function)?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Scientists See Promise in Deep-Learning Programs","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1353770768,"subreddit_id":"t5_2r3gv","url":"http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html","retrieved_on":1413350810,"num_comments":8,"mod_reports":[],"score":47,"gilded":0,"selftext_html":null,"domain":"nytimes.com","author":"[deleted]","user_reports":[],"edited":false,"is_self":false,"id":"13prya","permalink":"/r/MachineLearning/comments/13prya/scientists_see_promise_in_deeplearning_programs/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":47,"media":null,"report_reasons":null,"secure_media":null,"thumbnail":"default"}
{"mod_reports":[],"num_comments":2,"score":0,"selftext_html":null,"gilded":0,"domain":"reddit.com","user_reports":[],"author":"languist","title":"[This will take years.] Build me a online curriculum to go from basic arithmetic to understanding the math of 1) Deep Learning networks and 2) the type of (vector?) math that GPUs do so quickly [xpost from /r/math]","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"created_utc":1353887264,"selftext":"","author_flair_css_class":null,"downs":0,"retrieved_on":1413347195,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/math/comments/13s6da/this_will_take_years_build_me_a_online_curriculum/","thumbnail":"default","secure_media":null,"edited":false,"is_self":false,"id":"13s72u","permalink":"/r/MachineLearning/comments/13s72u/this_will_take_years_build_me_a_online_curriculum/","secure_media_embed":{},"ups":0,"media":null,"report_reasons":null,"over_18":false,"stickied":false,"media_embed":{}}
{"permalink":"/r/MachineLearning/comments/13rm7y/edmit_a_new_subreddit_for_educational_data_mining/","media":null,"ups":0,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"is_self":true,"edited":false,"id":"13rm7y","thumbnail":"self","secure_media":null,"created_utc":1353866507,"downs":0,"selftext":"Hello everyone, I just created [EDMit](http://www.reddit.com/r/EDMit/), a subreddit dedicated to the application of Machine Learning and other computational sciences to the education domain.\n\nPlease subscribe and post if you are interested!\n\nIf you are particularly passionate about this idea, send me a PM and I will add you as a mod :) ","author_flair_css_class":null,"retrieved_on":1413348075,"url":"http://www.reddit.com/r/MachineLearning/comments/13rm7y/edmit_a_new_subreddit_for_educational_data_mining/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"EDMit - a new subreddit for Educational Data Mining","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":0,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I just created &lt;a href=\"http://www.reddit.com/r/EDMit/\"&gt;EDMit&lt;/a&gt;, a subreddit dedicated to the application of Machine Learning and other computational sciences to the education domain.&lt;/p&gt;\n\n&lt;p&gt;Please subscribe and post if you are interested!&lt;/p&gt;\n\n&lt;p&gt;If you are particularly passionate about this idea, send me a PM and I will add you as a mod :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"virtuous_d","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":0}
{"id":"13rjpg","is_self":true,"edited":false,"stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":18,"media":null,"permalink":"/r/MachineLearning/comments/13rjpg/im_interested_in_building_a_robust_monopoly_board/","secure_media":null,"thumbnail":"self","author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"I'm interested in building a robust Monopoly board game simulator, anyone familiar with previous work done in this area?","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13rjpg/im_interested_in_building_a_robust_monopoly_board/","retrieved_on":1413348178,"selftext":"As a hobby I'm considering building a program to simulate Monopoly games to test strategies. I know that there are many models out there that simply calculate the likelihood of landing on a particular space. My goal would be to build something a bit more detailed to gather some more interesting data about the game. I'd like to incorporate agent based learning into the model to test and develop strategies for playing. I'm wondering if anyone has seen a similar project done in the past. I spent about an hour searching on google and I found a couple of things:\n\n\n*Monopoly Nerd's blog: \n\nhttp://monopolynerd.wordpress.com/\n\nThis guy built a web-based simulator to produce winning percentages based on starting conditions.\n\n\n*ESTIMATING THE PROBABILITY THAT THE GAME OF MONOPOLY NEVER ENDS\n\nhttp://www.informs-sim.org/wsc09papers/036.pdf\n\nThis is a paper that a couple of guys from Cornell wrote about their analysis on the game and determining the likelihood of having a game that \"never ends\". The model they used had some severe limitations (only two players, limited to no trading, etc).\n\n\n*Agent Based Simulation, Negotiation, and Strategy Optimization of Monopoly\n\nhttp://www.tjhsst.edu/~rlatimer/techlab08/LoffredoPaperQ2-08.pdf\n\nThis is the only thing that comes close to what I want to do, but the paper is extremely vague as it doesn't include any results so I don't know if the project was even completed. There is no institution or organization listed but the author's name is there.\n\nLet me know if any of you have stumbled across something related to Monopoly simulations on the internet. I know it's probably not likely but it's worth a shot. Thanks.","author_flair_css_class":null,"downs":0,"created_utc":1353863603,"num_comments":4,"mod_reports":[],"domain":"self.MachineLearning","author":"juicedealer","user_reports":[],"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a hobby I&amp;#39;m considering building a program to simulate Monopoly games to test strategies. I know that there are many models out there that simply calculate the likelihood of landing on a particular space. My goal would be to build something a bit more detailed to gather some more interesting data about the game. I&amp;#39;d like to incorporate agent based learning into the model to test and develop strategies for playing. I&amp;#39;m wondering if anyone has seen a similar project done in the past. I spent about an hour searching on google and I found a couple of things:&lt;/p&gt;\n\n&lt;p&gt;*Monopoly Nerd&amp;#39;s blog: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://monopolynerd.wordpress.com/\"&gt;http://monopolynerd.wordpress.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This guy built a web-based simulator to produce winning percentages based on starting conditions.&lt;/p&gt;\n\n&lt;p&gt;*ESTIMATING THE PROBABILITY THAT THE GAME OF MONOPOLY NEVER ENDS&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.informs-sim.org/wsc09papers/036.pdf\"&gt;http://www.informs-sim.org/wsc09papers/036.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a paper that a couple of guys from Cornell wrote about their analysis on the game and determining the likelihood of having a game that &amp;quot;never ends&amp;quot;. The model they used had some severe limitations (only two players, limited to no trading, etc).&lt;/p&gt;\n\n&lt;p&gt;*Agent Based Simulation, Negotiation, and Strategy Optimization of Monopoly&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.tjhsst.edu/%7Erlatimer/techlab08/LoffredoPaperQ2-08.pdf\"&gt;http://www.tjhsst.edu/~rlatimer/techlab08/LoffredoPaperQ2-08.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the only thing that comes close to what I want to do, but the paper is extremely vague as it doesn&amp;#39;t include any results so I don&amp;#39;t know if the project was even completed. There is no institution or organization listed but the author&amp;#39;s name is there.&lt;/p&gt;\n\n&lt;p&gt;Let me know if any of you have stumbled across something related to Monopoly simulations on the internet. I know it&amp;#39;s probably not likely but it&amp;#39;s worth a shot. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","score":18,"gilded":0}
{"secure_media":null,"thumbnail":"http://a.thumbs.redditmedia.com/pjOv3Ca36hAw_46Z.jpg","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":6,"report_reasons":null,"media":null,"permalink":"/r/MachineLearning/comments/13r3ec/gaussian_distributions_form_a_monoid_and_why/","id":"13r3ec","is_self":false,"edited":false,"domain":"izbicki.me","author":"PokerPirate","user_reports":[],"selftext_html":null,"score":6,"gilded":0,"num_comments":9,"mod_reports":[],"subreddit_id":"t5_2r3gv","url":"http://izbicki.me/blog/gausian-distributions-are-monoids","retrieved_on":1413348839,"selftext":"","author_flair_css_class":null,"downs":0,"created_utc":1353828655,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Gaussian distributions form a monoid, and why machine learning experts should care","subreddit":"MachineLearning"}
{"domain":"extremetech.com","author":"[deleted]","user_reports":[],"gilded":0,"score":0,"selftext_html":null,"mod_reports":[],"num_comments":0,"retrieved_on":1413344854,"subreddit_id":"t5_2r3gv","url":"http://www.extremetech.com/extreme/140681-computer-ai-successfully-identifies-why-abstract-art-evokes-human-emotion","created_utc":1353956146,"author_flair_css_class":null,"selftext":"","downs":0,"author_flair_text":null,"link_flair_css_class":null,"banned_by":null,"title":"Computer AI successfully identifies why abstract art evokes human emotion","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"thumbnail":"default","secure_media":null,"secure_media_embed":{},"ups":0,"media":null,"report_reasons":null,"stickied":false,"over_18":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/13tpq6/computer_ai_successfully_identifies_why_abstract/","id":"13tpq6","edited":false,"is_self":false}
{"author_flair_css_class":null,"selftext":"I've been marveling at the success Viola &amp; Jones had with their face detector. As I understand, until then, face detection techniques had poor scores for real benchmarking sets (not the easy databases of perfectly framed faces and obvious non-faces). Then along, came these 2 great scientists at MS Research and combined the idea of an integral image, with AdaBoost and weak cascaded classifiers to achieve great fidelity for real-world sets.\n\nThe winning algorithm is a mix of very different techniques, as it seems many good ML &amp; CV algorithms are these days. So I was wondering how one goes about choosing which techniques to aggregate in order to get the best results. Is there a process or framework to these choices or is it just intuition these days?\n","downs":0,"created_utc":1353927567,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13t38t/viola_jones_the_process_of_combining_multiple/","retrieved_on":1413345818,"link_flair_text":null,"distinguished":null,"title":"Viola &amp; Jones, the process of combining multiple techniques to achieve a great algorithm?","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been marveling at the success Viola &amp;amp; Jones had with their face detector. As I understand, until then, face detection techniques had poor scores for real benchmarking sets (not the easy databases of perfectly framed faces and obvious non-faces). Then along, came these 2 great scientists at MS Research and combined the idea of an integral image, with AdaBoost and weak cascaded classifiers to achieve great fidelity for real-world sets.&lt;/p&gt;\n\n&lt;p&gt;The winning algorithm is a mix of very different techniques, as it seems many good ML &amp;amp; CV algorithms are these days. So I was wondering how one goes about choosing which techniques to aggregate in order to get the best results. Is there a process or framework to these choices or is it just intuition these days?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"[deleted]","user_reports":[],"num_comments":0,"mod_reports":[],"permalink":"/r/MachineLearning/comments/13t38t/viola_jones_the_process_of_combining_multiple/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":1,"report_reasons":null,"media":null,"edited":false,"is_self":true,"id":"13t38t","secure_media":null,"thumbnail":"default"}
{"score":1,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to do Multiclass classification with SVM, I have 7 classes. Now I was wondering if this was possible. I&amp;#39;m thinking of creating 7 SVMs for 1 vs all approach. Am i allowed to create 1 kind of feature vector per class? So e.g.&lt;/p&gt;\n\n&lt;p&gt;Class 1 vs rest ==&amp;gt; Use type feature vector 1 (designed for class 1)\nClass 2 vs rest ==&amp;gt; Use type feature vector 2 (designed for class 1)\nClass 3 vs rest ==&amp;gt; Use type feature vector 3 (designed for class 3)&lt;/p&gt;\n\n&lt;p&gt;And then assign the class-label with the highest confidence (probability), to the datapoint.&lt;/p&gt;\n\n&lt;p&gt;Is this cheating ? Or is this allowed ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","domain":"self.MachineLearning","author":"[deleted]","user_reports":[],"num_comments":1,"mod_reports":[],"selftext":"Hi\n\nI'm trying to do Multiclass classification with SVM, I have 7 classes. Now I was wondering if this was possible. I'm thinking of creating 7 SVMs for 1 vs all approach. Am i allowed to create 1 kind of feature vector per class? So e.g.\n\nClass 1 vs rest ==&gt; Use type feature vector 1 (designed for class 1)\nClass 2 vs rest ==&gt; Use type feature vector 2 (designed for class 1)\nClass 3 vs rest ==&gt; Use type feature vector 3 (designed for class 3)\n\nAnd then assign the class-label with the highest confidence (probability), to the datapoint.\n\nIs this cheating ? Or is this allowed ?\n\n","author_flair_css_class":null,"downs":0,"created_utc":1353927006,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13t303/multiclass_classification_with_svm_1_vs_all/","retrieved_on":1413345839,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Multiclass classification with SVM (1 vs all) question","subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"secure_media":null,"thumbnail":"self","permalink":"/r/MachineLearning/comments/13t303/multiclass_classification_with_svm_1_vs_all/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"media":null,"ups":1,"report_reasons":null,"edited":false,"is_self":true,"id":"13t303"}
{"secure_media":null,"thumbnail":"http://c.thumbs.redditmedia.com/b6fYl8H0t90Cd0Jz.jpg","edited":false,"is_self":false,"id":"13sgvq","permalink":"/r/MachineLearning/comments/13sgvq/is_deep_learning_a_revolution_in_artificial/","media_embed":{},"stickied":false,"over_18":false,"ups":31,"report_reasons":null,"media":null,"secure_media_embed":{},"num_comments":8,"mod_reports":[],"gilded":0,"score":31,"selftext_html":null,"author":"Barbas","user_reports":[],"domain":"newyorker.com","distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Is “Deep Learning” a Revolution in Artificial Intelligence? : The New Yorker","link_flair_css_class":null,"author_flair_text":null,"downs":0,"selftext":"","author_flair_css_class":null,"created_utc":1353896815,"url":"http://www.newyorker.com/online/blogs/newsdesk/2012/11/is-deep-learning-a-revolution-in-artificial-intelligence.html","subreddit_id":"t5_2r3gv","retrieved_on":1413346776}
{"gilded":0,"score":10,"selftext_html":null,"domain":"stumbleupon.com","author":"cavedave","user_reports":[],"num_comments":1,"mod_reports":[],"selftext":"","author_flair_css_class":null,"downs":0,"created_utc":1354055961,"subreddit_id":"t5_2r3gv","url":"http://www.stumbleupon.com/su/1HP65o/www.kenvanharen.com/2012/11/getting-started-with-ramp-detecting.html","retrieved_on":1413340778,"link_flair_text":null,"distinguished":null,"title":"Getting started with Ramp: Detecting insults","banned_by":null,"subreddit":"MachineLearning","author_flair_text":"naive","link_flair_css_class":null,"secure_media":null,"thumbnail":"http://b.thumbs.redditmedia.com/4l7kiZwpW6Ps_Pgs.jpg","permalink":"/r/MachineLearning/comments/13wcyd/getting_started_with_ramp_detecting_insults/","stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"ups":10,"report_reasons":null,"media":null,"is_self":false,"edited":false,"id":"13wcyd"}
{"permalink":"/r/MachineLearning/comments/13w6yi/text_mining_with_weka_java_api/","ups":10,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"edited":false,"is_self":true,"id":"13w6yi","thumbnail":"self","secure_media":null,"created_utc":1354051023,"downs":0,"selftext":"Hi All - \n\nI am trying to use the WEKA API for some text classification.  I am struggling to find a bare-bones example.   Of course, this isn't a bare-bones task, so a simple example may not exist, but it seems that Google searches are leading me more to using the GUI.  \n\nAnyway, anything anyone can point me to would be much appreciated.\n\nThanks\n\n","author_flair_css_class":null,"retrieved_on":1413341025,"url":"http://www.reddit.com/r/MachineLearning/comments/13w6yi/text_mining_with_weka_java_api/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"Text mining with WEKA Java API","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"score":10,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All - &lt;/p&gt;\n\n&lt;p&gt;I am trying to use the WEKA API for some text classification.  I am struggling to find a bare-bones example.   Of course, this isn&amp;#39;t a bare-bones task, so a simple example may not exist, but it seems that Google searches are leading me more to using the GUI.  &lt;/p&gt;\n\n&lt;p&gt;Anyway, anything anyone can point me to would be much appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"NineSevenNine","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":10}
{"stickied":false,"over_18":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":7,"media":null,"permalink":"/r/MachineLearning/comments/13vqx5/are_kohonen_soms_all_that_different_from_kmeans/","id":"13vqx5","is_self":true,"edited":1354037725,"secure_media":null,"thumbnail":"self","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13vqx5/are_kohonen_soms_all_that_different_from_kmeans/","retrieved_on":1413341698,"selftext":"Hey all, \n\nSo recently took it upon myself to learn Kohonen SOMs by myself. I did this over the thanksgiving break while trying to dodge my 4 year old cousins from finding me in the house, so I am quite new to it. \n\nThat being said, I am already familiar with K-means. \n\nI have some questions about it in general I would like some feedback on:\n\n1) KSOMs adjust the actual weight vectors to the _actual_ points in the data set over time. So overtime, in a case with say two clusters of points and two output units, the first output units' weight vector will point to ~~one of the points in the first cluster~~ a point pretty much within the first cluster, and the second output units' weight vector will also point to a point within the second cluster. My question here is, ... isnt this very similar to K-means? \n\n2) Seeing as how (I think) this is very similar to K-means, what advantages might it confer over K-means? If I am completely off, then how is it different from K-means? In other words, why would I opt to use KSOMs over K-means? \n\n3) Regarding the fact that they adjust the weight vectors to become one of the actual points over time, is this a 'correlation' of some sort? At the end of the day, a novel point will come along, and its dot product will be taken with the (two in this example) output layer units. The one with largest score wins. Did I just do a correlation? Is dot-product = correlation?\n\n4) I also see some parallels with perceptrons. I know that perceptrons are used for supervised learning, where a vector is trained such that is adequately splits a space into categories. Are KSOMs a sort of, unsupervised version of perceptrons?\n\nI am trying to tie all those things together in my head. Thanks!!\n","author_flair_css_class":null,"downs":0,"created_utc":1354036896,"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"banned_by":null,"title":"Are Kohonen SOMs all that different from K-means?","subreddit":"MachineLearning","domain":"self.MachineLearning","author":"Ayakalam","user_reports":[],"score":7,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;So recently took it upon myself to learn Kohonen SOMs by myself. I did this over the thanksgiving break while trying to dodge my 4 year old cousins from finding me in the house, so I am quite new to it. &lt;/p&gt;\n\n&lt;p&gt;That being said, I am already familiar with K-means. &lt;/p&gt;\n\n&lt;p&gt;I have some questions about it in general I would like some feedback on:&lt;/p&gt;\n\n&lt;p&gt;1) KSOMs adjust the actual weight vectors to the &lt;em&gt;actual&lt;/em&gt; points in the data set over time. So overtime, in a case with say two clusters of points and two output units, the first output units&amp;#39; weight vector will point to &lt;del&gt;one of the points in the first cluster&lt;/del&gt; a point pretty much within the first cluster, and the second output units&amp;#39; weight vector will also point to a point within the second cluster. My question here is, ... isnt this very similar to K-means? &lt;/p&gt;\n\n&lt;p&gt;2) Seeing as how (I think) this is very similar to K-means, what advantages might it confer over K-means? If I am completely off, then how is it different from K-means? In other words, why would I opt to use KSOMs over K-means? &lt;/p&gt;\n\n&lt;p&gt;3) Regarding the fact that they adjust the weight vectors to become one of the actual points over time, is this a &amp;#39;correlation&amp;#39; of some sort? At the end of the day, a novel point will come along, and its dot product will be taken with the (two in this example) output layer units. The one with largest score wins. Did I just do a correlation? Is dot-product = correlation?&lt;/p&gt;\n\n&lt;p&gt;4) I also see some parallels with perceptrons. I know that perceptrons are used for supervised learning, where a vector is trained such that is adequately splits a space into categories. Are KSOMs a sort of, unsupervised version of perceptrons?&lt;/p&gt;\n\n&lt;p&gt;I am trying to tie all those things together in my head. Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":28,"mod_reports":[]}
{"downs":0,"selftext":"\nHey all, \n\nSo recently took it upon myself to learn Kohonen SOMs by myself. I did this over the thanksgiving break while trying to dodge my 4 year old cousins from finding me in the house, so I am quite new to it. \n\nThat being said, I am already familiar with K-means. \n\nI have some questions about it in general I would like some feedback on:\n\n1) KSOMs adjust the actual weight vectors to the _actual_ points in the data set over time. So overtime, in a case with say two clusters of points and two output units, the first output units' weight vector will point to one of the points in the first cluster, and the second output units' weight vector will point to one of the points in the second cluster. My question here is, ... isnt this very similar to K-means? \n\n2) Seeing as how (I think) this is very similar to K-means, what advantages might it confer over K-means? If I am completely off, then how is it different from K-means? In other words, why would I opt to use KSOMs over K-means? \n\n3) Regarding the fact that they adjust the weight vectors to become one of the actual points over time, is this a 'correlation' of some sort? At the end of the day, a novel point will come along, and its dot product will be taken with the (two in this example) output layer units. The one with largest score wins. Did I just do a correlation? Is dot-product = correlation?\n\n4) I also see some parallels with perceptrons. I know that perceptrons are used for supervised learning, where a vector is trained such that is adequately splits a space into categories. Are KSOMs a sort of, unsupervised version of perceptrons?\n\nI am trying to tie all those things together in my head. Thanks!!\n","author_flair_css_class":null,"created_utc":1354035867,"url":"http://www.reddit.com/r/MachineLearning/comments/13vpuf/are_kohonen_soms_and_kmeans_really_that_different/","subreddit_id":"t5_2r3gv","retrieved_on":1413341742,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Are Kohonen SOMs and K-means really that different?","banned_by":null,"link_flair_css_class":null,"author_flair_text":null,"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, &lt;/p&gt;\n\n&lt;p&gt;So recently took it upon myself to learn Kohonen SOMs by myself. I did this over the thanksgiving break while trying to dodge my 4 year old cousins from finding me in the house, so I am quite new to it. &lt;/p&gt;\n\n&lt;p&gt;That being said, I am already familiar with K-means. &lt;/p&gt;\n\n&lt;p&gt;I have some questions about it in general I would like some feedback on:&lt;/p&gt;\n\n&lt;p&gt;1) KSOMs adjust the actual weight vectors to the &lt;em&gt;actual&lt;/em&gt; points in the data set over time. So overtime, in a case with say two clusters of points and two output units, the first output units&amp;#39; weight vector will point to one of the points in the first cluster, and the second output units&amp;#39; weight vector will point to one of the points in the second cluster. My question here is, ... isnt this very similar to K-means? &lt;/p&gt;\n\n&lt;p&gt;2) Seeing as how (I think) this is very similar to K-means, what advantages might it confer over K-means? If I am completely off, then how is it different from K-means? In other words, why would I opt to use KSOMs over K-means? &lt;/p&gt;\n\n&lt;p&gt;3) Regarding the fact that they adjust the weight vectors to become one of the actual points over time, is this a &amp;#39;correlation&amp;#39; of some sort? At the end of the day, a novel point will come along, and its dot product will be taken with the (two in this example) output layer units. The one with largest score wins. Did I just do a correlation? Is dot-product = correlation?&lt;/p&gt;\n\n&lt;p&gt;4) I also see some parallels with perceptrons. I know that perceptrons are used for supervised learning, where a vector is trained such that is adequately splits a space into categories. Are KSOMs a sort of, unsupervised version of perceptrons?&lt;/p&gt;\n\n&lt;p&gt;I am trying to tie all those things together in my head. Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"author":"Ayakalam","user_reports":[],"domain":"self.MachineLearning","num_comments":0,"mod_reports":[],"permalink":"/r/MachineLearning/comments/13vpuf/are_kohonen_soms_and_kmeans_really_that_different/","media_embed":{},"over_18":false,"stickied":false,"media":null,"ups":0,"report_reasons":null,"secure_media_embed":{},"edited":false,"is_self":true,"id":"13vpuf","secure_media":null,"thumbnail":"self"}
{"num_comments":4,"mod_reports":[],"author":"peristimulus","user_reports":[],"domain":"self.MachineLearning","score":8,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Say I have a 2-class classification problem. I have 100 samples altogether in my data, and I train and test a classifier with 5-fold cross-validation. To estimate mean accuracy and confidence intervals, I resort to bootstrapping.&lt;/p&gt;\n\n&lt;p&gt;I this example, the sample I bootstrap on consists of 100 binary values that represent right or wrong prediction on my test samples, pooled across the 5 folds (80 training samples and 20 test samples per validation folds x 5 folds = 100 test samples). &lt;/p&gt;\n\n&lt;p&gt;Now, to bootstrap, I draw 100 samples with replacement, over 999 resample instances and then I compute the 2.5%ile and 97.5%ile values of the mean from each resample. So far so good!&lt;/p&gt;\n\n&lt;p&gt;My question is the following: \nSince I had only 20 samples per cross-validation fold, my resolution for the classifier accuracy is 1/20 ~ 5%. If I take all 5 folds together, my resolution is 1/100 ~ 1%. In other words, prior to bootstrapping, if I simply report the mean classifier accuracy, I cannot report 93.2%: the best I can resolve the accuracy to is 93%.&lt;/p&gt;\n\n&lt;p&gt;However, now that I have bootstrapped 999 times, does my resolution of the mean accuracy improve to 1/99900 ~ 0.001%? &lt;/p&gt;\n\n&lt;p&gt;The question applies to any summary statistic and not just the mean (e.g. 95% confidence intervals, as mentioned in the above example).&lt;/p&gt;\n\n&lt;p&gt;It probably makes no difference to my current article, but I&amp;#39;d like to get it right. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your input!&lt;/p&gt;\n\n&lt;p&gt;[Edit: Typos and language]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"Does bootstrapping legitimately increase the number of significant digits in my summary statistic?","banned_by":null,"url":"http://www.reddit.com/r/MachineLearning/comments/13v0wl/does_bootstrapping_legitimately_increase_the/","subreddit_id":"t5_2r3gv","retrieved_on":1413342872,"downs":0,"selftext":"Say I have a 2-class classification problem. I have 100 samples altogether in my data, and I train and test a classifier with 5-fold cross-validation. To estimate mean accuracy and confidence intervals, I resort to bootstrapping.\n\nI this example, the sample I bootstrap on consists of 100 binary values that represent right or wrong prediction on my test samples, pooled across the 5 folds (80 training samples and 20 test samples per validation folds x 5 folds = 100 test samples). \n\nNow, to bootstrap, I draw 100 samples with replacement, over 999 resample instances and then I compute the 2.5%ile and 97.5%ile values of the mean from each resample. So far so good!\n\nMy question is the following: \nSince I had only 20 samples per cross-validation fold, my resolution for the classifier accuracy is 1/20 ~ 5%. If I take all 5 folds together, my resolution is 1/100 ~ 1%. In other words, prior to bootstrapping, if I simply report the mean classifier accuracy, I cannot report 93.2%: the best I can resolve the accuracy to is 93%.\n\nHowever, now that I have bootstrapped 999 times, does my resolution of the mean accuracy improve to 1/99900 ~ 0.001%? \n\nThe question applies to any summary statistic and not just the mean (e.g. 95% confidence intervals, as mentioned in the above example).\n\nIt probably makes no difference to my current article, but I'd like to get it right. \n\nThanks for your input!\n\n[Edit: Typos and language]","author_flair_css_class":null,"created_utc":1353997302,"secure_media":null,"thumbnail":"self","id":"13v0wl","edited":false,"is_self":true,"media_embed":{},"over_18":false,"stickied":false,"ups":8,"media":null,"report_reasons":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/13v0wl/does_bootstrapping_legitimately_increase_the/"}
{"id":"13urfq","edited":1353990142,"is_self":true,"secure_media_embed":{},"media":null,"ups":0,"report_reasons":null,"stickied":false,"over_18":false,"media_embed":{},"permalink":"/r/MachineLearning/comments/13urfq/analytics_data_science_ms_or_coursera_udacity/","thumbnail":"default","secure_media":null,"author_flair_text":null,"link_flair_css_class":null,"banned_by":null,"title":"Analytics / Data Science MS or Coursera / Udacity, after Econ / Econometrics Undergrad ","subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"retrieved_on":1413343273,"subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13urfq/analytics_data_science_ms_or_coursera_udacity/","created_utc":1353988059,"author_flair_css_class":null,"selftext":"I earned my BA in Economics in 2011, mostly taking courses in econometrics and mathematical economics. Now, I'm interested in going into data science / analytics and was wondering if I should take the graduate school route and get a MS in Analytics or self-study ML, Data Mining, etc. through Coursera / Udacity. My CS background is limited to coursework up to Algorithms, and I don't currently work in IT (actually, I work in anti-money laundering, hah.) Honestly, I miss numbers and data from all the econometrics I studied, and am anxious to get out of finance and into tech, where honestly, I probably belong. \n\nGiven that I don't have work experience in IT, nor a degree in CS, I figure my best bet is going through a structured program and incurring the cost, even though I could probably just as easily learn the material myself, in order to take advantage of the extracurricular offerings through practicums, internships, and career services. What do you guys think?\n\nJust FYI, these are some of the MS programs I'm considering, with USF as my first choice: \n\n[USF MS in Analytics](http://www.usfca.edu/analytics)\n\n[Northwestern MS in Analytics](http://www.analytics.northwestern.edu)\n\n[NCSU MS in Analytics](http://analytics.ncsu.edu)\n","downs":0,"mod_reports":[],"num_comments":1,"domain":"self.MachineLearning","author":"[deleted]","user_reports":[],"gilded":0,"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I earned my BA in Economics in 2011, mostly taking courses in econometrics and mathematical economics. Now, I&amp;#39;m interested in going into data science / analytics and was wondering if I should take the graduate school route and get a MS in Analytics or self-study ML, Data Mining, etc. through Coursera / Udacity. My CS background is limited to coursework up to Algorithms, and I don&amp;#39;t currently work in IT (actually, I work in anti-money laundering, hah.) Honestly, I miss numbers and data from all the econometrics I studied, and am anxious to get out of finance and into tech, where honestly, I probably belong. &lt;/p&gt;\n\n&lt;p&gt;Given that I don&amp;#39;t have work experience in IT, nor a degree in CS, I figure my best bet is going through a structured program and incurring the cost, even though I could probably just as easily learn the material myself, in order to take advantage of the extracurricular offerings through practicums, internships, and career services. What do you guys think?&lt;/p&gt;\n\n&lt;p&gt;Just FYI, these are some of the MS programs I&amp;#39;m considering, with USF as my first choice: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.usfca.edu/analytics\"&gt;USF MS in Analytics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://www.analytics.northwestern.edu\"&gt;Northwestern MS in Analytics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://analytics.ncsu.edu\"&gt;NCSU MS in Analytics&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;"}
{"thumbnail":"http://f.thumbs.redditmedia.com/Xx5SVlIwIeQUW1kk.jpg","secure_media":null,"edited":false,"is_self":false,"id":"13uc5l","permalink":"/r/MachineLearning/comments/13uc5l/do_we_need_parsed_corpora_lousy_linguist/","ups":2,"media":null,"report_reasons":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"mod_reports":[],"num_comments":0,"gilded":0,"score":2,"selftext_html":null,"author":"HughJorgan1986","user_reports":[],"domain":"thelousylinguist.blogspot.com","subreddit":"MachineLearning","title":"Do we need parsed corpora - Lousy linguist","banned_by":null,"distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"created_utc":1353975242,"downs":0,"author_flair_css_class":null,"selftext":"","retrieved_on":1413343904,"url":"http://thelousylinguist.blogspot.com/2011/01/do-we-need-parsed-corpora.html","subreddit_id":"t5_2r3gv"}
{"num_comments":2,"mod_reports":[],"author":"[deleted]","user_reports":[],"domain":"self.MachineLearning","gilded":0,"score":1,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for some insight here.&lt;/p&gt;\n\n&lt;p&gt;I am working on a classification project and I am fairly new to WEKA but much more comfortable with it than R or another alternative.  I am doing a binary classification of imbalanced data.  I am told to use 5x2 cross-validation.&lt;/p&gt;\n\n&lt;p&gt;My data is in an .arff file, a big set of text + class. &lt;/p&gt;\n\n&lt;p&gt;In explorer I can load in this text, convert it with StringToWordVector (TF-IDF and Bag of Words), then run my classifier on it.  With the results I can produce a nice ROC curve and easily get the AUC.  But I can only do 1x10 cross-validation.  &lt;/p&gt;\n\n&lt;p&gt;In experimenter I can load in the data but I cannot preprocess it.  I can preprocess it in explorer the same way and then save that output, I tried this but I am getting errors saying that my class is not nominal.  I am not touching the data so I don&amp;#39;t know why all of a suddent it stops working.  I could then run 5x2 CV here on this data (if I can get it to load and run properly), apply the same classifier on it, but then I lose the ability to produce ROC curves, though I can still get it to produce the AUC.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how I can get this to work?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"5x2 Cross Validation in WEKA","banned_by":null,"url":"http://www.reddit.com/r/MachineLearning/comments/13y6x0/5x2_cross_validation_in_weka/","subreddit_id":"t5_2r3gv","retrieved_on":1413337870,"downs":0,"author_flair_css_class":null,"selftext":"I am looking for some insight here.\n\nI am working on a classification project and I am fairly new to WEKA but much more comfortable with it than R or another alternative.  I am doing a binary classification of imbalanced data.  I am told to use 5x2 cross-validation.\n\nMy data is in an .arff file, a big set of text + class. \n\nIn explorer I can load in this text, convert it with StringToWordVector (TF-IDF and Bag of Words), then run my classifier on it.  With the results I can produce a nice ROC curve and easily get the AUC.  But I can only do 1x10 cross-validation.  \n\nIn experimenter I can load in the data but I cannot preprocess it.  I can preprocess it in explorer the same way and then save that output, I tried this but I am getting errors saying that my class is not nominal.  I am not touching the data so I don't know why all of a suddent it stops working.  I could then run 5x2 CV here on this data (if I can get it to load and run properly), apply the same classifier on it, but then I lose the ability to produce ROC curves, though I can still get it to produce the AUC.\n\nAny suggestions on how I can get this to work?  ","created_utc":1354131992,"secure_media":null,"thumbnail":"default","id":"13y6x0","edited":false,"is_self":true,"media_embed":{},"over_18":false,"stickied":false,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/13y6x0/5x2_cross_validation_in_weka/"}
{"id":"13xd4r","edited":false,"is_self":false,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":48,"media":null,"permalink":"/r/MachineLearning/comments/13xd4r/deep_learning_reading_list/","secure_media":null,"thumbnail":"default","author_flair_text":"naive","link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"Deep Learning Reading List","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://deeplearning.net/reading-list/","retrieved_on":1413339211,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354093999,"num_comments":2,"mod_reports":[],"domain":"deeplearning.net","user_reports":[],"author":"cavedave","gilded":0,"score":48,"selftext_html":null}
{"link_flair_text":null,"distinguished":null,"title":"GE &amp; Kaggle announce $350k in prizes for two analytics quests: predicting flight delays and improving hospital operations ","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354213040,"subreddit_id":"t5_2r3gv","url":"http://www.gequest.com/","retrieved_on":1413334891,"num_comments":10,"mod_reports":[],"gilded":0,"score":28,"selftext_html":null,"domain":"gequest.com","author":"willis77","user_reports":[],"edited":false,"is_self":false,"id":"1406i9","permalink":"/r/MachineLearning/comments/1406i9/ge_kaggle_announce_350k_in_prizes_for_two/","over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"ups":28,"media":null,"report_reasons":null,"secure_media":null,"thumbnail":"default"}
{"url":"https://www.gequest.com/","subreddit_id":"t5_2r3gv","retrieved_on":1413334905,"downs":0,"selftext":"","author_flair_css_class":null,"created_utc":1354212728,"link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","title":"GE &amp; Kaggle announce $500k in prizes for two analytics quests: predicting flight delays and improving hospital operations","banned_by":null,"author":"[deleted]","user_reports":[],"domain":"gequest.com","score":1,"gilded":0,"selftext_html":null,"num_comments":0,"mod_reports":[],"media_embed":{},"stickied":false,"over_18":false,"ups":1,"report_reasons":null,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/14065k/ge_kaggle_announce_500k_in_prizes_for_two/","id":"14065k","is_self":false,"edited":false,"secure_media":null,"thumbnail":"default"}
{"num_comments":1,"mod_reports":[],"author":"longerislonger","user_reports":[],"domain":"self.MachineLearning","score":0,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m having a hard time understanding why logistic regression is used and when to use it.  &lt;/p&gt;\n\n&lt;p&gt;If I have a simple data set containing one independent variable and one binary dependent variable, how does transforming the result of my prediction function using the logistic function improve the quality of my fit?  Typically I’ve been normalizing my independent variables using the following formula: (x_i-avg(x))/(max(x)-min(x)).  If I use this normalization, does it still benefit me to use the logistic function, or is the logistic function itself an alternative method to normalize my independent variables?  &lt;/p&gt;\n\n&lt;p&gt;It’s just not clicking with me, any help would be appreciated, thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","link_flair_css_class":null,"author_flair_text":null,"distinguished":null,"link_flair_text":null,"subreddit":"MachineLearning","banned_by":null,"title":"Logistic regression isn't quite making sense to me","url":"http://www.reddit.com/r/MachineLearning/comments/13zwwr/logistic_regression_isnt_quite_making_sense_to_me/","subreddit_id":"t5_2r3gv","retrieved_on":1413335324,"downs":0,"selftext":"I’m having a hard time understanding why logistic regression is used and when to use it.  \n\nIf I have a simple data set containing one independent variable and one binary dependent variable, how does transforming the result of my prediction function using the logistic function improve the quality of my fit?  Typically I’ve been normalizing my independent variables using the following formula: (x_i-avg(x))/(max(x)-min(x)).  If I use this normalization, does it still benefit me to use the logistic function, or is the logistic function itself an alternative method to normalize my independent variables?  \n\nIt’s just not clicking with me, any help would be appreciated, thanks!!!\n","author_flair_css_class":null,"created_utc":1354204092,"secure_media":null,"thumbnail":"self","id":"13zwwr","edited":false,"is_self":true,"media_embed":{},"over_18":false,"stickied":false,"ups":0,"report_reasons":null,"media":null,"secure_media_embed":{},"permalink":"/r/MachineLearning/comments/13zwwr/logistic_regression_isnt_quite_making_sense_to_me/"}
{"score":13,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know how IP law works generally with respect to models produced using so called private data?  &lt;/p&gt;\n\n&lt;p&gt;Specifically, I&amp;#39;m referring to the case where someone uses private data to derive a model. &lt;/p&gt;\n\n&lt;p&gt;Can the &amp;#39;owner&amp;#39; of the data claim ownership of the model, even though the model may represent a domain far more generic than the privately owned data?  &lt;/p&gt;\n\n&lt;p&gt;(Let me know if this should be posted elsewhere... just thought you folks might have a bead on what the current view of this is.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","author":"Ju11ian","user_reports":[],"domain":"self.MachineLearning","mod_reports":[],"num_comments":27,"created_utc":1354203094,"downs":0,"selftext":"Does anyone know how IP law works generally with respect to models produced using so called private data?  \n\nSpecifically, I'm referring to the case where someone uses private data to derive a model. \n\nCan the 'owner' of the data claim ownership of the model, even though the model may represent a domain far more generic than the privately owned data?  \n\n(Let me know if this should be posted elsewhere... just thought you folks might have a bead on what the current view of this is.)\n  ","author_flair_css_class":null,"retrieved_on":1413335362,"url":"http://www.reddit.com/r/MachineLearning/comments/13zvxe/ip_law_and_machine_learning_who_owns_the_model/","subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","banned_by":null,"title":"IP law and machine learning, who owns the model?","distinguished":null,"link_flair_text":null,"link_flair_css_class":null,"author_flair_text":null,"thumbnail":"self","secure_media":null,"permalink":"/r/MachineLearning/comments/13zvxe/ip_law_and_machine_learning_who_owns_the_model/","ups":13,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"over_18":false,"stickied":false,"is_self":true,"edited":false,"id":"13zvxe"}
{"author_flair_text":null,"link_flair_css_class":null,"link_flair_text":null,"distinguished":null,"title":"Question: Can someone explain to me the difference between a cost function and the gradient ascent equation in logistic regression?","banned_by":null,"subreddit":"MachineLearning","subreddit_id":"t5_2r3gv","url":"http://www.reddit.com/r/MachineLearning/comments/13zjxo/question_can_someone_explain_to_me_the_difference/","retrieved_on":1413335870,"selftext":"Im going through the ML Class on coursera on Logistic Regression and also the Manning Book Machine Learning in Action im trying to learn by implementing everything in python. Im not able to understand the difference between the cost function and the gradient... there are examples on the net where people compute the cost funciton and then there are places where they dont and just go with the gradient descent funciton w :=w - (alpha) * (delta)w * f(w)\nwhat is the difference between the two? or is there any diference im not able to get my head around it ? :/","author_flair_css_class":null,"downs":0,"created_utc":1354183130,"num_comments":6,"mod_reports":[],"domain":"self.MachineLearning","author":"ilikerum2","user_reports":[],"score":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im going through the ML Class on coursera on Logistic Regression and also the Manning Book Machine Learning in Action im trying to learn by implementing everything in python. Im not able to understand the difference between the cost function and the gradient... there are examples on the net where people compute the cost funciton and then there are places where they dont and just go with the gradient descent funciton w :=w - (alpha) * (delta)w * f(w)\nwhat is the difference between the two? or is there any diference im not able to get my head around it ? :/&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","gilded":0,"id":"13zjxo","is_self":true,"edited":false,"over_18":false,"stickied":false,"media_embed":{},"secure_media_embed":{},"report_reasons":null,"ups":0,"media":null,"permalink":"/r/MachineLearning/comments/13zjxo/question_can_someone_explain_to_me_the_difference/","secure_media":null,"thumbnail":"self"}
{"permalink":"/r/MachineLearning/comments/13zdq5/information_geometry/","secure_media_embed":{},"report_reasons":null,"ups":22,"media":null,"over_18":false,"stickied":false,"media_embed":{},"edited":false,"is_self":false,"id":"13zdq5","thumbnail":"default","secure_media":null,"created_utc":1354171360,"selftext":"","author_flair_css_class":null,"downs":0,"retrieved_on":1413336136,"subreddit_id":"t5_2r3gv","url":"http://masi.cscs.lsa.umich.edu/~crshalizi/notabene/info-geo.html?em_x=22","title":"Information Geometry","banned_by":null,"subreddit":"MachineLearning","link_flair_text":null,"distinguished":null,"author_flair_text":null,"link_flair_css_class":null,"score":22,"gilded":0,"selftext_html":null,"domain":"masi.cscs.lsa.umich.edu","user_reports":[],"author":"cypherx","mod_reports":[],"num_comments":4}
{"link_flair_text":null,"distinguished":null,"title":"Geordie Rose on Quantum Computers implications for AI and machine learning","banned_by":null,"subreddit":"MachineLearning","author_flair_text":null,"link_flair_css_class":null,"author_flair_css_class":null,"selftext":"","downs":0,"created_utc":1354243983,"subreddit_id":"t5_2r3gv","url":"https://www.youtube.com/watch?v=cA31kiHEOBs","retrieved_on":1413333412,"num_comments":7,"mod_reports":[],"gilded":0,"score":10,"selftext_html":null,"domain":"youtube.com","author":"Buck-Nasty","user_reports":[],"edited":false,"is_self":false,"id":"1415nl","permalink":"/r/MachineLearning/comments/1415nl/geordie_rose_on_quantum_computers_implications/","over_18":false,"stickied":false,"media_embed":{"content":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/cA31kiHEOBs?fs=1&amp;feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","height":338,"width":600,"scrolling":false},"secure_media_embed":{},"ups":10,"media":{"oembed":{"provider_name":"YouTube","url":"http://www.youtube.com/watch?v=cA31kiHEOBs","provider_url":"http://www.youtube.com/","height":338,"type":"video","thumbnail_height":48,"description":"D-Wave recently received investment from the CIA, http://www.technologyreview.com/news/429429/the-cia-and-jeff-bezos-bet-on-quantum-computing/ D-Wave Systems, Inc. is a quantum computing company, based in Burnaby, British Columbia. On May 11, 2011, D-Wave System announced D-Wave One, labeled \"the world's first commercially available quantum computer,\" and also referred to it as an adiabatic quantum computer using quantum annealing to solve optimization problems operating on an 128 qubit chip-set.","html":"&lt;iframe width=\"600\" height=\"338\" src=\"http://www.youtube.com/embed/cA31kiHEOBs?fs=1&amp;feature=oembed\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;","width":600,"author_url":"http://www.youtube.com/user/Air420","thumbnail_width":48,"author_name":"Air420","version":"1.0","thumbnail_url":"http://s.ytimg.com/yts/img/silhouette48-vflLdu7sh.png","title":"Geordie Rose - D-Wave Quantum Computing - A Eureka Moment"},"type":"youtube.com"},"report_reasons":null,"secure_media":null,"thumbnail":"http://c.thumbs.redditmedia.com/2FtZRtC7Su4e9UBn.jpg"}
{"ups":6,"report_reasons":null,"media":null,"secure_media_embed":{},"media_embed":{},"stickied":false,"over_18":false,"permalink":"/r/MachineLearning/comments/14103v/ok_so_how_exactly_do_i_make_a_umatrix/","id":"14103v","is_self":true,"edited":false,"thumbnail":"self","secure_media":null,"retrieved_on":1413333643,"url":"http://www.reddit.com/r/MachineLearning/comments/14103v/ok_so_how_exactly_do_i_make_a_umatrix/","subreddit_id":"t5_2r3gv","created_utc":1354238931,"downs":0,"author_flair_css_class":null,"selftext":"\nHi r/MachineLearning. \n\nSo I asked a previous question about SOMs, and I learned a lot. This however put me on the path to studying how to visualize them, specifically, using what is known as a U-Matrix. \n\nNow... I am about to pull my hair out trying to figure out, how exactly, a U-matrix is constructed for visualization of Self-Organizing-Maps. (SOMS, aka Kohonen Nets).\n\nEvery last google result I have found does not help, is contradictory, has a massive number of typos, or otherwise very broad.\n\nI am asking a simple question: I have an output grid of 3x3 output units: How do I construct a U-matrix from this??\n\nLinks so far:\n\n1) [Original Paper](http://www.uni-marburg.de/fb12/datenbionik/pdf/pubs/1990/UltschSiemon90). (RIDDLED with errors, typos, and misleading information. The U-matrix part is so full of errors I do not know how this paper got published.)\n\n2) The [SOM toolbox manual](http://www.cis.hut.fi/somtoolbox/package/docs2/som_umat.html) that quotes the above paper. (Explains how to do it for an output line, but does not explain how to do it for an output grid).\n\n3) [Another paper](http://www.uni-marburg.de/fb12/datenbionik/pdf/pubs/2003/ultsch03ustar). (Explains how to make a U-matrix, but completely contradicts his first paper, and the SOM toolbox that it is based on).\n\n4) A [similar question](http://stackoverflow.com/questions/6485699/need-a-specific-example-of-u-matrix-in-self-organizing-map) on SE that didnt really get anywhere.\n\n--------------------------------------------\n\n\nTo facilitate this ... I have made up a very simple example, and this should be simple to answer for someone familiar with SOMs and U-Matricies. \n\nI have a 3x3 output grid, that means, 3x3 output neurons that have already been trained. All neurons have dimension, say, 4. Now I want to make a U-matrix. \n\n**How exactly do I do that?**\n\nPlease give me a step-by-step, I cannot seem to find anything on it! :-/\n\nThanks!!","link_flair_css_class":null,"author_flair_text":null,"subreddit":"MachineLearning","title":"Ok, so how -exactly- do I make a U-Matrix?...","banned_by":null,"distinguished":null,"link_flair_text":null,"author":"Ayakalam","user_reports":[],"domain":"self.MachineLearning","score":6,"gilded":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi r/MachineLearning. &lt;/p&gt;\n\n&lt;p&gt;So I asked a previous question about SOMs, and I learned a lot. This however put me on the path to studying how to visualize them, specifically, using what is known as a U-Matrix. &lt;/p&gt;\n\n&lt;p&gt;Now... I am about to pull my hair out trying to figure out, how exactly, a U-matrix is constructed for visualization of Self-Organizing-Maps. (SOMS, aka Kohonen Nets).&lt;/p&gt;\n\n&lt;p&gt;Every last google result I have found does not help, is contradictory, has a massive number of typos, or otherwise very broad.&lt;/p&gt;\n\n&lt;p&gt;I am asking a simple question: I have an output grid of 3x3 output units: How do I construct a U-matrix from this??&lt;/p&gt;\n\n&lt;p&gt;Links so far:&lt;/p&gt;\n\n&lt;p&gt;1) &lt;a href=\"http://www.uni-marburg.de/fb12/datenbionik/pdf/pubs/1990/UltschSiemon90\"&gt;Original Paper&lt;/a&gt;. (RIDDLED with errors, typos, and misleading information. The U-matrix part is so full of errors I do not know how this paper got published.)&lt;/p&gt;\n\n&lt;p&gt;2) The &lt;a href=\"http://www.cis.hut.fi/somtoolbox/package/docs2/som_umat.html\"&gt;SOM toolbox manual&lt;/a&gt; that quotes the above paper. (Explains how to do it for an output line, but does not explain how to do it for an output grid).&lt;/p&gt;\n\n&lt;p&gt;3) &lt;a href=\"http://www.uni-marburg.de/fb12/datenbionik/pdf/pubs/2003/ultsch03ustar\"&gt;Another paper&lt;/a&gt;. (Explains how to make a U-matrix, but completely contradicts his first paper, and the SOM toolbox that it is based on).&lt;/p&gt;\n\n&lt;p&gt;4) A &lt;a href=\"http://stackoverflow.com/questions/6485699/need-a-specific-example-of-u-matrix-in-self-organizing-map\"&gt;similar question&lt;/a&gt; on SE that didnt really get anywhere.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;To facilitate this ... I have made up a very simple example, and this should be simple to answer for someone familiar with SOMs and U-Matricies. &lt;/p&gt;\n\n&lt;p&gt;I have a 3x3 output grid, that means, 3x3 output neurons that have already been trained. All neurons have dimension, say, 4. Now I want to make a U-matrix. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How exactly do I do that?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Please give me a step-by-step, I cannot seem to find anything on it! :-/&lt;/p&gt;\n\n&lt;p&gt;Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","mod_reports":[],"num_comments":0}
