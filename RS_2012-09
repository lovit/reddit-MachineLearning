{"link_flair_css_class":null,"selftext":"OK so I'm going to share here something that has been bothering me.\n\nI am a CS graduate and I just started my Masters in ML.\nWhat I want to do is become, a really, really good ML scientist/expert/whatever.\n\nNow, I am a big fan of \"practice makes perfect\", and the 10000 hour rule. What has been bothering me though is:\n\nHow does one \"practice\" in Machine Learning?\n\nIn other disciplines it's pretty straightforward.\nIf you want to become a good weight lifter, you lift.\nIf you want to become a great violin player, you practice violin.\n\nHow does one become \"better\" at Machine Learning though?","score":24,"author_flair_text":null,"report_reasons":null,"created_utc":1346529862,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","gilded":0,"author":"Barbas","permalink":"/r/MachineLearning/comments/z74p9/ml_and_the_10000_hour_rule/","num_comments":16,"retrieved_on":1413612566,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OK so I&amp;#39;m going to share here something that has been bothering me.&lt;/p&gt;\n\n&lt;p&gt;I am a CS graduate and I just started my Masters in ML.\nWhat I want to do is become, a really, really good ML scientist/expert/whatever.&lt;/p&gt;\n\n&lt;p&gt;Now, I am a big fan of &amp;quot;practice makes perfect&amp;quot;, and the 10000 hour rule. What has been bothering me though is:&lt;/p&gt;\n\n&lt;p&gt;How does one &amp;quot;practice&amp;quot; in Machine Learning?&lt;/p&gt;\n\n&lt;p&gt;In other disciplines it&amp;#39;s pretty straightforward.\nIf you want to become a good weight lifter, you lift.\nIf you want to become a great violin player, you practice violin.&lt;/p&gt;\n\n&lt;p&gt;How does one become &amp;quot;better&amp;quot; at Machine Learning though?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":24,"secure_media_embed":{},"link_flair_text":null,"edited":false,"is_self":true,"title":"ML and the 10.000 hour rule","over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"z74p9","url":"http://www.reddit.com/r/MachineLearning/comments/z74p9/ml_and_the_10000_hour_rule/"}
{"link_flair_text":null,"edited":false,"title":"help with Multiclass classification using WEKA","is_self":true,"over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/z8u92/help_with_multiclass_classification_using_weka/","user_reports":[],"id":"z8u92","media_embed":{},"mod_reports":[],"subreddit":"MachineLearning","report_reasons":null,"created_utc":1346618206,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"score":1,"author_flair_text":null,"link_flair_css_class":null,"selftext":"So after a few months and some hard work and learning, I've progressed to multiclass classifiers.  The problem my research professor has given me now is to classify the dataset used in Blitzer, et al (though I likely will not achieve anything like the results that some of the research teams have, given my inexperience). \nProblem number one:  using the standard, textdirectoryloader, WEKA wants to take in data that is:  A. in raw text form of some sort, rather than pre-organized into some other sort of vector or feature file.    and B.  binary in nature   \n\nUnfortunately, the datasets provided in the Blitzer, et all are what appear to be feature vectors (you can go look for yourself).  After doing some somewhat rigorous searches, I am no more sure how to change these feature vectors into arff format, or how to arrange the directory structure.  I tried arranging the directory structure as a tree, for the most part, and it complained about all the branches, all the way down about how it was not binary.   Even with just two simple branches on one category, it didn't recognize the sparse vector nature of the data and simply assumed that the_dog:3 was a single feature, rather than something with its frequency already specified.\n\nSo what i'm asking for here is:  how do I get WEKA to take in the data in the format provided, and  how do I arrange the directories in this case so that it can be used with a multiclass classifier? \n\nYou can see the datasets (processed) at www.cs.jhu.edu/~mdredze/datasets/sentiment/\n\nI was trying to use the acl one as apposed to the stars one. Thank you very much.\n","retrieved_on":1413609980,"num_comments":12,"gilded":0,"author":"WEKAnewb","permalink":"/r/MachineLearning/comments/z8u92/help_with_multiclass_classification_using_weka/","distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So after a few months and some hard work and learning, I&amp;#39;ve progressed to multiclass classifiers.  The problem my research professor has given me now is to classify the dataset used in Blitzer, et al (though I likely will not achieve anything like the results that some of the research teams have, given my inexperience). \nProblem number one:  using the standard, textdirectoryloader, WEKA wants to take in data that is:  A. in raw text form of some sort, rather than pre-organized into some other sort of vector or feature file.    and B.  binary in nature   &lt;/p&gt;\n\n&lt;p&gt;Unfortunately, the datasets provided in the Blitzer, et all are what appear to be feature vectors (you can go look for yourself).  After doing some somewhat rigorous searches, I am no more sure how to change these feature vectors into arff format, or how to arrange the directory structure.  I tried arranging the directory structure as a tree, for the most part, and it complained about all the branches, all the way down about how it was not binary.   Even with just two simple branches on one category, it didn&amp;#39;t recognize the sparse vector nature of the data and simply assumed that the_dog:3 was a single feature, rather than something with its frequency already specified.&lt;/p&gt;\n\n&lt;p&gt;So what i&amp;#39;m asking for here is:  how do I get WEKA to take in the data in the format provided, and  how do I arrange the directories in this case so that it can be used with a multiclass classifier? &lt;/p&gt;\n\n&lt;p&gt;You can see the datasets (processed) at &lt;a href=\"http://www.cs.jhu.edu/%7Emdredze/datasets/sentiment/\"&gt;www.cs.jhu.edu/~mdredze/datasets/sentiment/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was trying to use the acl one as apposed to the stars one. Thank you very much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","ups":1,"secure_media_embed":{},"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null}
{"distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello redditers!&lt;/p&gt;\n\n&lt;p&gt;I would like to present a &lt;a href=\"https://github.com/neuromancer/libmind\"&gt;small project&lt;/a&gt; i was working during the last months. \nIt aims to solve the problem in Machine Learning of processing sequences of inputs, instead of a fixed size vector that many algorithms requires to perform classification or regression and also providing a minimalist interface to learn and predict sequences in almost any domain. &lt;/p&gt;\n\n&lt;p&gt;The basic idea is to use an &lt;a href=\"http://www.scholarpedia.org/article/Echo_state_network\"&gt;Echo State Networks&lt;/a&gt; to generate a single fixed size vector in R&lt;sup&gt;n&lt;/sup&gt; from a sequence of inputs of vectors in R&lt;sup&gt;k.&lt;/sup&gt; Then, we can use this representation to perform some kind of classification or regression. Also, a dual procedure can be used to reconstruct a sequence of vectors in R&lt;sup&gt;k&lt;/sup&gt; from a fixed size vector in R&lt;sup&gt;n.&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;This project is called &lt;a href=\"https://github.com/neuromancer/libmind\"&gt;libmind&lt;/a&gt;. It is also designed to be a simple way for a programmer to learn sequences of arbitrary elements, as long as they can be vectorized.   &lt;/p&gt;\n\n&lt;p&gt;In the repository of libmind two examples of its use are included: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Identification of part of speech (POS) of English words only using their letters.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Reduction of variableless propositional logic formulas.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would like to hear if it can be improved and/or extended. So, in the spirit of open research, i&amp;#39;m publishing all the code, data and instructions, so anyone can view and improve this small project.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"secure_media_embed":{},"ups":3,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","subreddit":"MachineLearning","media":null,"report_reasons":null,"created_utc":1346711631,"subreddit_id":"t5_2r3gv","banned_by":null,"author_flair_text":null,"score":3,"link_flair_css_class":null,"selftext":"Hello redditers!\n\n\nI would like to present a [small project](https://github.com/neuromancer/libmind) i was working during the last months. \nIt aims to solve the problem in Machine Learning of processing sequences of inputs, instead of a fixed size vector that many algorithms requires to perform classification or regression and also providing a minimalist interface to learn and predict sequences in almost any domain. \n\n\nThe basic idea is to use an [Echo State Networks](http://www.scholarpedia.org/article/Echo_state_network) to generate a single fixed size vector in R^n from a sequence of inputs of vectors in R^k. Then, we can use this representation to perform some kind of classification or regression. Also, a dual procedure can be used to reconstruct a sequence of vectors in R^k from a fixed size vector in R^n.\n\n\nThis project is called [libmind](https://github.com/neuromancer/libmind). It is also designed to be a simple way for a programmer to learn sequences of arbitrary elements, as long as they can be vectorized.   \n\n\nIn the repository of libmind two examples of its use are included: \n\n\n* Identification of part of speech (POS) of English words only using their letters.\n\n* Reduction of variableless propositional logic formulas.\n\n\nI would like to hear if it can be improved and/or extended. So, in the spirit of open research, i'm publishing all the code, data and instructions, so anyone can view and improve this small project.\n\n\nThanks!","retrieved_on":1413606948,"num_comments":0,"author":"galapag0","gilded":0,"permalink":"/r/MachineLearning/comments/zaxvw/looking_for_ideas_processing_and_predicting/","over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/zaxvw/looking_for_ideas_processing_and_predicting/","user_reports":[],"id":"zaxvw","media_embed":{},"mod_reports":[],"edited":1346711816,"link_flair_text":null,"title":"Looking for ideas: processing and predicting arbitrary sequences of inputs using libmind","is_self":true}
{"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By applying the finite element method to neural networks supercomputer scale machine learning can done.&lt;/p&gt;\n\n&lt;p&gt;In fact, supercomputer scale FULLY AUTOMATED machine learning.&lt;/p&gt;\n\n&lt;p&gt;The finite elements would be autoencoders and logistic regression. Bulk synchronous parallel graph database such as Pregel or Graphlab would be used for greatly simplify coding. Graph partitioning software Chaco would be used to partition the graph (as is often done in finite element on supercomputers).&lt;/p&gt;\n\n&lt;p&gt;Finite element method is used to avoid the need for expensive global optimization.\nIt would allow for a larger scale version of this work done by Jeff Dean of Google. &lt;a href=\"http://techtalks.tv/talks/57639/\"&gt;http://techtalks.tv/talks/57639/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":0,"secure_media_embed":{},"selftext":"By applying the finite element method to neural networks supercomputer scale machine learning can done.\n\nIn fact, supercomputer scale FULLY AUTOMATED machine learning.\n\nThe finite elements would be autoencoders and logistic regression. Bulk synchronous parallel graph database such as Pregel or Graphlab would be used for greatly simplify coding. Graph partitioning software Chaco would be used to partition the graph (as is often done in finite element on supercomputers).\n\nFinite element method is used to avoid the need for expensive global optimization.\nIt would allow for a larger scale version of this work done by Jeff Dean of Google. http://techtalks.tv/talks/57639/\n","link_flair_css_class":null,"score":0,"author_flair_text":null,"created_utc":1346699615,"banned_by":null,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"subreddit":"MachineLearning","author":"marshallp","permalink":"/r/MachineLearning/comments/zakxi/applying_finite_element_method_to_neural_networks/","gilded":0,"num_comments":0,"retrieved_on":1413607454,"over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zakxi","url":"http://www.reddit.com/r/MachineLearning/comments/zakxi/applying_finite_element_method_to_neural_networks/","edited":false,"link_flair_text":null,"is_self":true,"title":"Applying finite element method to neural networks can give supercomputer scale machine learning"}
{"downs":0,"distinguished":null,"stickied":false,"thumbnail":"http://a.thumbs.redditmedia.com/MhvQG6pQRSYM29mT.jpg","selftext_html":null,"secure_media_embed":{},"ups":1,"secure_media":null,"domain":"nuit-blanche.blogspot.dk","author_flair_css_class":null,"media":null,"report_reasons":null,"created_utc":1346673243,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":1,"num_comments":0,"retrieved_on":1413608323,"permalink":"/r/MachineLearning/comments/z9yy0/a_year_of_reproducible_research_in_compressive/","gilded":0,"author":"pegasusbridge","over_18":false,"user_reports":[],"id":"z9yy0","url":"http://nuit-blanche.blogspot.dk/2012/09/a-year-in-reproducible-results-in.html","mod_reports":[],"media_embed":{},"edited":false,"link_flair_text":null,"title":"A Year of Reproducible Research in Compressive Sensing, Advanced Matrix Factorization and more","is_self":false}
{"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a senior software engineer for a big European corporation. I&amp;#39;ve got master&amp;#39;s degree in machine learning, specialised in classification and semi-supervised learning. I haven&amp;#39;t put this knowledge to use in almost three years now, except for solving some simple clustering problems. I&amp;#39;d really appreciate any suggestions of open source projects that I could contribute to. I just want to keep my skills sharp. Since I don&amp;#39;t get any challenging tasks at work (I slack off), I&amp;#39;d rather spend this time doing something useful.&lt;/p&gt;\n\n&lt;p&gt;I have a slight preference for Java projects because I used to work with WEKA a lot. But I can learn a new language too.&lt;/p&gt;\n\n&lt;p&gt;Sorry for using throwaway, some of my colleagues are redditors, I&amp;#39;m pretty sure sure they don&amp;#39;t browse r/MachineLearning, but I don&amp;#39;t want to take any chances.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":34,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"report_reasons":null,"created_utc":1346753925,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"I'm a senior software engineer for a big European corporation. I've got master's degree in machine learning, specialised in classification and semi-supervised learning. I haven't put this knowledge to use in almost three years now, except for solving some simple clustering problems. I'd really appreciate any suggestions of open source projects that I could contribute to. I just want to keep my skills sharp. Since I don't get any challenging tasks at work (I slack off), I'd rather spend this time doing something useful.\n\nI have a slight preference for Java projects because I used to work with WEKA a lot. But I can learn a new language too.\n\nSorry for using throwaway, some of my colleagues are redditors, I'm pretty sure sure they don't browse r/MachineLearning, but I don't want to take any chances.","link_flair_css_class":null,"score":34,"author_flair_text":null,"num_comments":39,"retrieved_on":1413605625,"gilded":0,"author":"sse_throwaway","permalink":"/r/MachineLearning/comments/zbv6d/looking_for_an_open_source_machine_learning/","over_18":false,"user_reports":[],"id":"zbv6d","url":"http://www.reddit.com/r/MachineLearning/comments/zbv6d/looking_for_an_open_source_machine_learning/","mod_reports":[],"media_embed":{},"edited":false,"link_flair_text":null,"title":"Looking for an open source machine learning project","is_self":true}
{"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zbuzm","url":"http://metaoptimize.com/qa/questions/10931/why-hasnt-finite-element-method-been-applied-to-neural-networks","over_18":false,"is_self":false,"title":"Why hasn't finite element method been applied to neural networks?","edited":false,"link_flair_text":null,"secure_media":null,"domain":"metaoptimize.com","author_flair_css_class":null,"secure_media_embed":{},"ups":0,"stickied":false,"thumbnail":"default","selftext_html":null,"downs":0,"distinguished":null,"permalink":"/r/MachineLearning/comments/zbuzm/why_hasnt_finite_element_method_been_applied_to/","gilded":0,"author":"marshallp","num_comments":0,"retrieved_on":1413605634,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":0,"media":null,"report_reasons":null,"created_utc":1346753494,"subreddit_id":"t5_2r3gv","banned_by":null,"subreddit":"MachineLearning"}
{"selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":7,"media":null,"banned_by":null,"created_utc":1346939845,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","author":"cmmdevries","permalink":"/r/MachineLearning/comments/zg6m5/document_clustering_evaluation_divergence_from_a/","gilded":0,"num_comments":3,"retrieved_on":1413599449,"thumbnail":"default","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"secure_media":null,"domain":"eprints.qut.edu.au","author_flair_css_class":null,"secure_media_embed":{},"ups":7,"link_flair_text":null,"edited":false,"is_self":false,"title":"Document clustering evaluation: Divergence from a random baseline","over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zg6m5","url":"http://eprints.qut.edu.au/53371/"}
{"distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This paper I wrote describes an approach to dealing with ineffective or pathological cases in clustering evaluation. It can be applied to internal or external measures of quality. This paper introduces the approach and analyses it using document clustering.&lt;/p&gt;\n\n&lt;p&gt;Abstract:\nDivergence from a random baseline is a technique for the evaluation of document clustering. It ensures cluster quality measures are performing work that prevents ineffective clusterings from giving high scores to clusterings that provide no useful result. These concepts are defined and analysed using intrinsic and extrinsic approaches to the evaluation of document cluster quality. This includes the classical clusters to categories approach and a novel approach that uses ad hoc information retrieval. The divergence from a random baseline approach is able to differentiate ineffective clusterings encountered in the INEX XML Mining track. It also appears to perform a normalisation similar to the Normalised Mutual Information (NMI) measure but it can be applied to any measure of cluster quality. When it is applied to the intrinsic measure of distortion as measured by RMSE, subtraction from a random baseline provides a clear optimum that is not apparent otherwise. This approach can be applied to any clustering evaluation. This paper describes its use in the context of document clustering evaluation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"default","stickied":false,"secure_media_embed":{},"ups":1,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","subreddit":"MachineLearning","media":null,"created_utc":1346939672,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":1,"selftext":"This paper I wrote describes an approach to dealing with ineffective or pathological cases in clustering evaluation. It can be applied to internal or external measures of quality. This paper introduces the approach and analyses it using document clustering.\n\nAbstract:\nDivergence from a random baseline is a technique for the evaluation of document clustering. It ensures cluster quality measures are performing work that prevents ineffective clusterings from giving high scores to clusterings that provide no useful result. These concepts are defined and analysed using intrinsic and extrinsic approaches to the evaluation of document cluster quality. This includes the classical clusters to categories approach and a novel approach that uses ad hoc information retrieval. The divergence from a random baseline approach is able to differentiate ineffective clusterings encountered in the INEX XML Mining track. It also appears to perform a normalisation similar to the Normalised Mutual Information (NMI) measure but it can be applied to any measure of cluster quality. When it is applied to the intrinsic measure of distortion as measured by RMSE, subtraction from a random baseline provides a clear optimum that is not apparent otherwise. This approach can be applied to any clustering evaluation. This paper describes its use in the context of document clustering evaluation.","link_flair_css_class":null,"retrieved_on":1413599452,"num_comments":0,"gilded":0,"author":"[deleted]","permalink":"/r/MachineLearning/comments/zg6hj/document_clustering_evaluation_divergence_from_a/","over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/zg6hj/document_clustering_evaluation_divergence_from_a/","id":"zg6hj","user_reports":[],"media_embed":{},"mod_reports":[],"link_flair_text":null,"edited":false,"title":"Document clustering evaluation: Divergence from a random baseline","is_self":true}
{"gilded":0,"permalink":"/r/MachineLearning/comments/zfh5k/vending_ice_machines/","author":"icevendingnet","num_comments":0,"retrieved_on":1413600418,"selftext":"","link_flair_css_class":null,"score":1,"author_flair_text":null,"created_utc":1346899643,"subreddit_id":"t5_2r3gv","report_reasons":null,"banned_by":null,"media":null,"subreddit":"MachineLearning","domain":"ice-vending.net","secure_media":null,"author_flair_css_class":null,"ups":1,"secure_media_embed":{},"stickied":false,"thumbnail":"default","selftext_html":null,"downs":0,"distinguished":null,"is_self":false,"title":"Vending Ice Machines ","edited":false,"link_flair_text":null,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zfh5k","url":"http://ice-vending.net/vending-ice-machines/","over_18":false}
{"score":12,"author_flair_text":null,"selftext":"I am using a large number of features (around 200K) which are mostly zero with the occasional 1 and am performing a binary SVM classification task with this data. I was wondering what kind of kernels would be useful fo this data. Normalizing the data to have mean of zero and std of one is out of the question as it would stop the data being sparse. \n\nI have tried all the usual kernels RBF, Linear etc ... and am happy with the results, I just want to know if there are any specific kernels for sparse binary features as there are for string etc ..","link_flair_css_class":null,"subreddit":"MachineLearning","created_utc":1346892778,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"gilded":0,"author":"nickponline","permalink":"/r/MachineLearning/comments/zf9tu/what_is_a_good_kernel_for_sparse_binary_features/","retrieved_on":1413600696,"num_comments":6,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using a large number of features (around 200K) which are mostly zero with the occasional 1 and am performing a binary SVM classification task with this data. I was wondering what kind of kernels would be useful fo this data. Normalizing the data to have mean of zero and std of one is out of the question as it would stop the data being sparse. &lt;/p&gt;\n\n&lt;p&gt;I have tried all the usual kernels RBF, Linear etc ... and am happy with the results, I just want to know if there are any specific kernels for sparse binary features as there are for string etc ..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":12,"secure_media_embed":{},"edited":false,"link_flair_text":null,"is_self":true,"title":"What is a good kernel for sparse binary features?","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zf9tu/what_is_a_good_kernel_for_sparse_binary_features/","id":"zf9tu","user_reports":[]}
{"banned_by":null,"created_utc":1347053266,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"score":14,"author_flair_text":null,"num_comments":17,"retrieved_on":1413595541,"author":"arauhala","gilded":0,"permalink":"/r/MachineLearning/comments/ziy1s/reexpression_method_new_powerful_approach_for/","downs":0,"distinguished":null,"thumbnail":"http://c.thumbs.redditmedia.com/rjSOdpPM3e5uuxc8.jpg","stickied":false,"selftext_html":null,"ups":14,"secure_media_embed":{},"domain":"arauhala.github.com","secure_media":null,"author_flair_css_class":null,"link_flair_text":null,"edited":false,"title":"Re-expression method, new powerful approach for machine learning","is_self":false,"over_18":false,"id":"ziy1s","user_reports":[],"url":"http://arauhala.github.com/libreexpweb/","mod_reports":[],"media_embed":{}}
{"is_self":true,"title":"Looking for algorithms to 'sync' recordings of speech","link_flair_text":null,"edited":1347056413,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zinl8/looking_for_algorithms_to_sync_recordings_of/","id":"zinl8","user_reports":[],"over_18":false,"author":"[deleted]","gilded":0,"permalink":"/r/MachineLearning/comments/zinl8/looking_for_algorithms_to_sync_recordings_of/","retrieved_on":1413595922,"num_comments":5,"score":6,"author_flair_text":null,"selftext":"Let's say I have recording of two persons (A &amp; B) reading the same block of text. Of course their voices sound different, they read at different speeds, etc. But let's assume:\n\n* they read exactly the same text\n* they have the same accent\n* there's no coughing, hiccups, major background noise, etc.\n* no major reading errors\n\nBasically, we can assume both the reading and the recording are pretty 'clean'. What's a good algorithm to 'sync' up the recordings ?\n\nBy 'sync' I mean you somehow break up the recordings into a sequence of 'sound bites', and the two recording share the same sequence, and the algorithm should output the time each speaker spent on each sound bite. For example:\n\n* sound-bite-0: t=[0.1,0.3] for A; t=[0.11, 0.28] for B\n* sound-bite-1: t=[0.32,0.39] for A; t=[0.29, 0.35] for B\n\nEach 'sound bite' can be a word, a syllable, a group of words, etc, but let's say sound bites should be between 0.1 to 2 seconds (or any reasonable range) in length.\n\nThere's no need to do speech recognition on the recording, in fact if they read a list of unintelligible, random syllables, the algorithm should still work. Bonus point if the algorithm works on songs too.\n","link_flair_css_class":null,"subreddit":"MachineLearning","banned_by":null,"created_utc":1347043110,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":6,"secure_media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say I have recording of two persons (A &amp;amp; B) reading the same block of text. Of course their voices sound different, they read at different speeds, etc. But let&amp;#39;s assume:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;they read exactly the same text&lt;/li&gt;\n&lt;li&gt;they have the same accent&lt;/li&gt;\n&lt;li&gt;there&amp;#39;s no coughing, hiccups, major background noise, etc.&lt;/li&gt;\n&lt;li&gt;no major reading errors&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, we can assume both the reading and the recording are pretty &amp;#39;clean&amp;#39;. What&amp;#39;s a good algorithm to &amp;#39;sync&amp;#39; up the recordings ?&lt;/p&gt;\n\n&lt;p&gt;By &amp;#39;sync&amp;#39; I mean you somehow break up the recordings into a sequence of &amp;#39;sound bites&amp;#39;, and the two recording share the same sequence, and the algorithm should output the time each speaker spent on each sound bite. For example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;sound-bite-0: t=[0.1,0.3] for A; t=[0.11, 0.28] for B&lt;/li&gt;\n&lt;li&gt;sound-bite-1: t=[0.32,0.39] for A; t=[0.29, 0.35] for B&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Each &amp;#39;sound bite&amp;#39; can be a word, a syllable, a group of words, etc, but let&amp;#39;s say sound bites should be between 0.1 to 2 seconds (or any reasonable range) in length.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s no need to do speech recognition on the recording, in fact if they read a list of unintelligible, random syllables, the algorithm should still work. Bonus point if the algorithm works on songs too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"default","distinguished":null,"downs":0}
{"author_flair_text":null,"score":5,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","media":null,"subreddit_id":"t5_2r3gv","created_utc":1347040653,"report_reasons":null,"banned_by":null,"author":"pacmanisfun","permalink":"/r/MachineLearning/comments/zil84/toolbox_for_learning_machine_learning_and_data/","gilded":0,"retrieved_on":1413596014,"num_comments":0,"selftext_html":null,"thumbnail":"http://b.thumbs.redditmedia.com/cuuPLLrz0_nnUA17.jpg","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"k2company.com","secure_media_embed":{},"ups":5,"edited":false,"link_flair_text":null,"is_self":false,"title":"Toolbox for learning machine learning and data science","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://k2company.com/blog/2012/09/06/toolbox-for-learning-machine-learning-and-data-science/","id":"zil84","user_reports":[]}
{"user_reports":[],"id":"ziiyq","url":"http://blog.factual.com/the-wisdom-of-crowds","mod_reports":[],"media_embed":{},"over_18":false,"title":"The Wisdom of Crowds: Using Ensembles for Machine Learning","is_self":false,"edited":false,"link_flair_text":null,"secure_media_embed":{},"ups":15,"secure_media":null,"domain":"blog.factual.com","author_flair_css_class":null,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"http://b.thumbs.redditmedia.com/37P1OV71Dj6Dx1qC.jpg","selftext_html":null,"num_comments":12,"retrieved_on":1413596099,"gilded":0,"permalink":"/r/MachineLearning/comments/ziiyq/the_wisdom_of_crowds_using_ensembles_for_machine/","author":"rudyl313","media":null,"created_utc":1347038240,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":15}
{"title":"\"Neural Networks for Machine Learning\" - Geoffrey Hinton (Coursera course)","is_self":false,"edited":false,"link_flair_text":null,"user_reports":[],"id":"zje90","url":"https://www.coursera.org/course/neuralnets","mod_reports":[],"media_embed":{},"over_18":false,"num_comments":0,"retrieved_on":1413594932,"gilded":0,"author":"d3pd","permalink":"/r/MachineLearning/comments/zje90/neural_networks_for_machine_learning_geoffrey/","report_reasons":null,"created_utc":1347071172,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","score":2,"author_flair_text":null,"ups":2,"secure_media_embed":{},"domain":"coursera.org","secure_media":null,"author_flair_css_class":null,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/r-cyl8H1eT-eJ5lo.jpg","selftext_html":null}
{"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zlh2m","url":"http://www.reddit.com/r/MachineLearning/comments/zlh2m/clustering_high_dimensional_trajectories/","over_18":false,"is_self":true,"title":"Clustering high dimensional trajectories","edited":false,"link_flair_text":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":22,"secure_media_embed":{},"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not at all a machine learning expert and as I am setting out on a new project I thought I would ask for some pointers.&lt;/p&gt;\n\n&lt;p&gt;I would like to cluster some high-dimensional trajectories. They are quite short - between 10-20 time points in a 50-dimensional space. I will have several hundred trials and expect there to be around 5-20 clusters.&lt;/p&gt;\n\n&lt;p&gt;What would people recommend to approach this problem? At first I thought just ignore the time structure so I have 50 dims x 10 time points = 500 features. Another approach might be to define distance between two trajectories as the sum over time points of the euclidean distances in the 50dim space and then cluster the similarity matrix. Would this be better - I don&amp;#39;t have a good intuition for how it would differ from the full 500 feature vector? Are there any better techniques or approaches?&lt;/p&gt;\n\n&lt;p&gt;I actually have an external labelling of the data into 6 classes - but I would like to try an unsupervised approach to see if the same comes out of the data, or if there might be more structure - and also investigation the evolution with time of the cluster structure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"gilded":0,"permalink":"/r/MachineLearning/comments/zlh2m/clustering_high_dimensional_trajectories/","author":"thrope","num_comments":19,"retrieved_on":1413591738,"link_flair_css_class":null,"selftext":"I am not at all a machine learning expert and as I am setting out on a new project I thought I would ask for some pointers.\n\nI would like to cluster some high-dimensional trajectories. They are quite short - between 10-20 time points in a 50-dimensional space. I will have several hundred trials and expect there to be around 5-20 clusters.\n\nWhat would people recommend to approach this problem? At first I thought just ignore the time structure so I have 50 dims x 10 time points = 500 features. Another approach might be to define distance between two trajectories as the sum over time points of the euclidean distances in the 50dim space and then cluster the similarity matrix. Would this be better - I don't have a good intuition for how it would differ from the full 500 feature vector? Are there any better techniques or approaches?\n\nI actually have an external labelling of the data into 6 classes - but I would like to try an unsupervised approach to see if the same comes out of the data, or if there might be more structure - and also investigation the evolution with time of the cluster structure.","score":22,"author_flair_text":null,"created_utc":1347181746,"banned_by":null,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"subreddit":"MachineLearning"}
{"downs":0,"distinguished":null,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a personal project I want to create an extension of sorts that would highlight links that I would be likely to click on (based on history of clicked on and not clicked on)&lt;/p&gt;\n\n&lt;p&gt;I intend on using this as a filter as well in the future. Are there any projects like, is there anything I should know before embarking?&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":15,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"created_utc":1347295018,"banned_by":null,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"subreddit":"MachineLearning","selftext":"As a personal project I want to create an extension of sorts that would highlight links that I would be likely to click on (based on history of clicked on and not clicked on)\n\nI intend on using this as a filter as well in the future. Are there any projects like, is there anything I should know before embarking?\n\nThanks for reading!","link_flair_css_class":null,"score":15,"author_flair_text":null,"num_comments":18,"retrieved_on":1413588513,"author":"redditpad","gilded":0,"permalink":"/r/MachineLearning/comments/znqrk/machine_learning_of_links_that_i_would_click_on/","over_18":false,"user_reports":[],"id":"znqrk","url":"http://www.reddit.com/r/MachineLearning/comments/znqrk/machine_learning_of_links_that_i_would_click_on/","mod_reports":[],"media_embed":{},"edited":false,"link_flair_text":null,"title":"Machine Learning of Links that I would click on","is_self":true}
{"mod_reports":[],"media_embed":{},"user_reports":[],"id":"znjxb","url":"http://conveyorbeltsystems.net/screw-conveyor/","over_18":false,"is_self":false,"title":"Screw Conveyor","link_flair_text":null,"edited":false,"secure_media":null,"domain":"conveyorbeltsystems.net","author_flair_css_class":null,"secure_media_embed":{},"ups":1,"thumbnail":"default","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"gilded":0,"author":"conveyorbeltsystemsn","permalink":"/r/MachineLearning/comments/znjxb/screw_conveyor/","num_comments":0,"retrieved_on":1413588794,"selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":1,"media":null,"created_utc":1347287597,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning"}
{"over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zn2oc","url":"http://www.reddit.com/r/MachineLearning/comments/zn2oc/internship_opportunities_in_ml_as_an_undergrad/","edited":1347255534,"link_flair_text":null,"is_self":true,"title":"Internship Opportunities in ML as an Undergrad","stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a 4th year student in Bioinformatics who has taken a strong liking to ML. My university has a couple of ML courses that I have taken, and I have also coded my own suite of classifiers (SVM, HMM, kNN, kMeans, Perceptron, Naive Bayes, etc.) in Python. I am currently based out of San Diego, and was wondering if anyone had ideas as to where I could find a part time internship during the school year. Thank you all in advance, and thanks for creating such an awesome subreddit!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":0,"secure_media_embed":{},"link_flair_css_class":null,"selftext":"I am a 4th year student in Bioinformatics who has taken a strong liking to ML. My university has a couple of ML courses that I have taken, and I have also coded my own suite of classifiers (SVM, HMM, kNN, kMeans, Perceptron, Naive Bayes, etc.) in Python. I am currently based out of San Diego, and was wondering if anyone had ideas as to where I could find a part time internship during the school year. Thank you all in advance, and thanks for creating such an awesome subreddit!","score":0,"author_flair_text":null,"created_utc":1347255329,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","author":"punjabdasher","gilded":0,"permalink":"/r/MachineLearning/comments/zn2oc/internship_opportunities_in_ml_as_an_undergrad/","num_comments":0,"retrieved_on":1413589490}
{"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zq96b","url":"http://bag-sealer.net/vacuum-pack-machine/","over_18":false,"is_self":false,"title":"Vacuum Pack Machine","edited":false,"link_flair_text":null,"domain":"bag-sealer.net","secure_media":null,"author_flair_css_class":null,"ups":1,"secure_media_embed":{},"thumbnail":"default","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"gilded":0,"author":"bagsealernet","permalink":"/r/MachineLearning/comments/zq96b/vacuum_pack_machine/","num_comments":0,"retrieved_on":1413584875,"link_flair_css_class":null,"selftext":"","score":1,"author_flair_text":null,"created_utc":1347397412,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning"}
{"downs":0,"distinguished":null,"thumbnail":"default","stickied":false,"selftext_html":null,"ups":2,"secure_media_embed":{},"domain":"csee.wvu.edu","secure_media":null,"author_flair_css_class":null,"created_utc":1347383755,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"score":2,"author_flair_text":null,"num_comments":0,"retrieved_on":1413585499,"permalink":"/r/MachineLearning/comments/zpu10/the_unreasonable_effectiveness_of_data/","gilded":0,"author":"Naurgul","over_18":false,"user_reports":[],"id":"zpu10","url":"http://www.csee.wvu.edu/~gidoretto/courses/2011-fall-cp/reading/TheUnreasonable%20EffectivenessofData_IEEE_IS2009.pdf","mod_reports":[],"media_embed":{},"edited":false,"link_flair_text":null,"title":"The Unreasonable Effectiveness of Data","is_self":false}
{"author":"metaobject","gilded":0,"permalink":"/r/MachineLearning/comments/zphtb/ask_rml_seeking_advice_on_how_ml_techniques_can/","retrieved_on":1413585992,"num_comments":14,"score":7,"author_flair_text":null,"link_flair_css_class":null,"selftext":"We have about 30,000 XML files that contain financial information.  We're trying to parse 10 pieces of data from each of these files, but the problem is that the xml tags that contain the data are not consistently named.  Let's say we're looking for 'revenue'.  Some files may have 'revenues', some may have 'TotalRevenue', etc.  \n\nAre there any ML techniques that would allow us to extract this data that are better than the naive approach of building a dictionary of terms, scoring matches, and taking the one with the highest score?\n\nI would imagine that some sort of supervised learning algorithm that uses n-grams (or some other markov-like approach) might yield interesting results.  I'm just getting into NLP (hence the fascination with n-grams), so please forgive my misuse of any terms.  \n\nI'm sure this problem has been solved before ... is there a name for it?\n\nThank you so much for your time!","subreddit":"MachineLearning","created_utc":1347371441,"banned_by":null,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":7,"secure_media_embed":{},"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have about 30,000 XML files that contain financial information.  We&amp;#39;re trying to parse 10 pieces of data from each of these files, but the problem is that the xml tags that contain the data are not consistently named.  Let&amp;#39;s say we&amp;#39;re looking for &amp;#39;revenue&amp;#39;.  Some files may have &amp;#39;revenues&amp;#39;, some may have &amp;#39;TotalRevenue&amp;#39;, etc.  &lt;/p&gt;\n\n&lt;p&gt;Are there any ML techniques that would allow us to extract this data that are better than the naive approach of building a dictionary of terms, scoring matches, and taking the one with the highest score?&lt;/p&gt;\n\n&lt;p&gt;I would imagine that some sort of supervised learning algorithm that uses n-grams (or some other markov-like approach) might yield interesting results.  I&amp;#39;m just getting into NLP (hence the fascination with n-grams), so please forgive my misuse of any terms.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure this problem has been solved before ... is there a name for it?&lt;/p&gt;\n\n&lt;p&gt;Thank you so much for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","distinguished":null,"downs":0,"is_self":true,"title":"Ask r/ML:  Seeking advice on how ML techniques can be used to parse data from XML files","edited":false,"link_flair_text":null,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zphtb/ask_rml_seeking_advice_on_how_ml_techniques_can/","id":"zphtb","user_reports":[],"over_18":false}
{"is_self":true,"title":"In a Bayesian Network how do I \n\"neutralize\" a node?","link_flair_text":null,"edited":false,"mod_reports":[],"media_embed":{},"id":"zpayl","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zpayl/in_a_bayesian_network_how_do_i_neutralize_a_node/","over_18":false,"gilded":0,"permalink":"/r/MachineLearning/comments/zpayl/in_a_bayesian_network_how_do_i_neutralize_a_node/","author":"Jigsus","num_comments":11,"retrieved_on":1413586263,"link_flair_css_class":null,"selftext":"If I have a node in the network that's no longer functioning or observable what probability do I have to assign to it to neutralize it completely.\n\nSay for example that I have the [sprinkler, rain, wetgrass network example from wikipedia](http://upload.wikimedia.org/wikipedia/commons/f/f7/SimpleBayesNetNodes.png). If my sprinklers break how I do I make it so their variable doesn't  affect the outcome anymore? \n\nObvsiouly I could remove the variable from the network but that's quite difficult to do in programming because it alters the structure. Instead I would like to assign it a probability that no longer alters the final result.  What's the proper way to deal with this?","score":13,"author_flair_text":null,"created_utc":1347360866,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":13,"secure_media_embed":{},"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I have a node in the network that&amp;#39;s no longer functioning or observable what probability do I have to assign to it to neutralize it completely.&lt;/p&gt;\n\n&lt;p&gt;Say for example that I have the &lt;a href=\"http://upload.wikimedia.org/wikipedia/commons/f/f7/SimpleBayesNetNodes.png\"&gt;sprinkler, rain, wetgrass network example from wikipedia&lt;/a&gt;. If my sprinklers break how I do I make it so their variable doesn&amp;#39;t  affect the outcome anymore? &lt;/p&gt;\n\n&lt;p&gt;Obvsiouly I could remove the variable from the network but that&amp;#39;s quite difficult to do in programming because it alters the structure. Instead I would like to assign it a probability that no longer alters the final result.  What&amp;#39;s the proper way to deal with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null}
{"over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zox2k/how_can_we_make_machines_see/","user_reports":[],"id":"zox2k","edited":false,"link_flair_text":null,"is_self":true,"title":"How can we make machines see?","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a bit of a noob when it comes to these things, but I want to learn. I was thinking about AI and started to wonder how you could give it vision. Once you attach a camera to a robot, how is it able to use it to see? What would you connect the camera to so that it can see?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":0,"secure_media_embed":{},"score":0,"author_flair_text":null,"selftext":"I'm a bit of a noob when it comes to these things, but I want to learn. I was thinking about AI and started to wonder how you could give it vision. Once you attach a camera to a robot, how is it able to use it to see? What would you connect the camera to so that it can see?","link_flair_css_class":null,"subreddit":"MachineLearning","created_utc":1347335823,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"permalink":"/r/MachineLearning/comments/zox2k/how_can_we_make_machines_see/","author":"[deleted]","gilded":0,"retrieved_on":1413586817,"num_comments":5}
{"media":null,"created_utc":1347461513,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","selftext":"The course book we're using is 'Introduction to Machine Learning' by Ethem Alpayden, and feel like I have a difficulty of understanding his explanations within certain areas. I would like to have some extra literature for guidance when I feel like I'm stuck. Any recommendations?","link_flair_css_class":null,"author_flair_text":null,"score":14,"num_comments":13,"retrieved_on":1413582767,"gilded":0,"permalink":"/r/MachineLearning/comments/zrnf6/taking_beginners_course_in_machine_learning/","author":"HerrKanin","downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The course book we&amp;#39;re using is &amp;#39;Introduction to Machine Learning&amp;#39; by Ethem Alpayden, and feel like I have a difficulty of understanding his explanations within certain areas. I would like to have some extra literature for guidance when I feel like I&amp;#39;m stuck. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","secure_media_embed":{},"ups":14,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"edited":false,"link_flair_text":null,"title":"Taking beginner's course in machine learning focusing on NLP, and not very happy with the literature so far. What's your recommendation for beginner's book in ML?","is_self":true,"over_18":false,"user_reports":[],"id":"zrnf6","url":"http://www.reddit.com/r/MachineLearning/comments/zrnf6/taking_beginners_course_in_machine_learning/","mod_reports":[],"media_embed":{}}
{"created_utc":1347443887,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"Hi,\n\nI'm a programmer for some time now, working in web and video games.\nI have been fascinated by the things we can do with machine learning for a while and would like to know \"how to get my feet in\".\n\nIs there any resources for noobs or whatever you could share ?\n\nThanks :)","link_flair_css_class":null,"score":25,"author_flair_text":null,"num_comments":27,"retrieved_on":1413583240,"permalink":"/r/MachineLearning/comments/zrc82/please_dont_bite_i_would_like_to_know_where_to/","gilded":0,"author":"Malharhak","downs":0,"distinguished":null,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a programmer for some time now, working in web and video games.\nI have been fascinated by the things we can do with machine learning for a while and would like to know &amp;quot;how to get my feet in&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Is there any resources for noobs or whatever you could share ?&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":25,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"edited":false,"link_flair_text":null,"title":"[Please Don't Bite] I would like to know where to learn the basics","is_self":true,"over_18":false,"id":"zrc82","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zrc82/please_dont_bite_i_would_like_to_know_where_to/","mod_reports":[],"media_embed":{}}
{"over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"zukgd","url":"http://www.reddit.com/r/MachineLearning/comments/zukgd/gradient_boosting_annealing_vs_constant_learning/","link_flair_text":null,"edited":false,"is_self":true,"title":"Gradient Boosting: Annealing vs Constant Learning Rate","stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any theoretical or practical reason to use a constant learning rate in a gradient boosting machine? That is compared to annealing (decreasing the learning rate every iteration until it reaches zero or some trivial epsilon).&lt;/p&gt;\n\n&lt;p&gt;It seems that using a constant learning rate with a set number of iterations is just a degenerate case of annealing. One that doesn&amp;#39;t particularly make any sense as the learning rate stays constant than arbitrarily falls to zero at iteration N.&lt;/p&gt;\n\n&lt;p&gt;Also is there any reason to prefer linearly vs exponentially decaying annealing schedules? In practice when I&amp;#39;ve played around with this linear vs exponential tends to perform pretty similarly with most datasets I&amp;#39;ve applied it to. (Both of which tend to consistently beat out constant learning rates).&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m wondering if there&amp;#39;s any foundational logic or practical approach that dictates when certain types of annealing (or lack thereof) in gradient boosting will outperform other types.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"secure_media_embed":{},"ups":12,"link_flair_css_class":null,"selftext":"Is there any theoretical or practical reason to use a constant learning rate in a gradient boosting machine? That is compared to annealing (decreasing the learning rate every iteration until it reaches zero or some trivial epsilon).\n\nIt seems that using a constant learning rate with a set number of iterations is just a degenerate case of annealing. One that doesn't particularly make any sense as the learning rate stays constant than arbitrarily falls to zero at iteration N.\n\nAlso is there any reason to prefer linearly vs exponentially decaying annealing schedules? In practice when I've played around with this linear vs exponential tends to perform pretty similarly with most datasets I've applied it to. (Both of which tend to consistently beat out constant learning rates).\n\nBut I'm wondering if there's any foundational logic or practical approach that dictates when certain types of annealing (or lack thereof) in gradient boosting will outperform other types.","author_flair_text":null,"score":12,"media":null,"created_utc":1347579418,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","author":"CPlusPlusDeveloper","gilded":0,"permalink":"/r/MachineLearning/comments/zukgd/gradient_boosting_annealing_vs_constant_learning/","num_comments":6,"retrieved_on":1413578181}
{"num_comments":6,"retrieved_on":1413579210,"gilded":0,"author":"pcoat","permalink":"/r/MachineLearning/comments/ztxic/good_matrix_analysis_online_reference_for_machine/","banned_by":null,"created_utc":1347556846,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"subreddit":"MachineLearning","selftext":"Hi, folks, \n\nIs there any good online reference (open course) on advanced matrix analysis\nI know Stephen Boyd's convex optimization, but it is more on optimization side, \nand I also know pretty much about basic theory on spectral decomposition, eigenvalue, etc\n\nso I would like to know more about matrix analysis, like svd,  stability, all sorts of matrix product, matrix inequality\nthanks","link_flair_css_class":null,"score":7,"author_flair_text":null,"ups":7,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, folks, &lt;/p&gt;\n\n&lt;p&gt;Is there any good online reference (open course) on advanced matrix analysis\nI know Stephen Boyd&amp;#39;s convex optimization, but it is more on optimization side, \nand I also know pretty much about basic theory on spectral decomposition, eigenvalue, etc&lt;/p&gt;\n\n&lt;p&gt;so I would like to know more about matrix analysis, like svd,  stability, all sorts of matrix product, matrix inequality\nthanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","title":"good matrix analysis online reference (for machine learning)?","is_self":true,"edited":false,"link_flair_text":null,"user_reports":[],"id":"ztxic","url":"http://www.reddit.com/r/MachineLearning/comments/ztxic/good_matrix_analysis_online_reference_for_machine/","mod_reports":[],"media_embed":{},"over_18":false}
{"over_18":false,"media_embed":{},"mod_reports":[],"url":"http://i.imgur.com/C2NVK.jpg","user_reports":[],"id":"zvybw","edited":false,"link_flair_text":null,"is_self":false,"title":"[Ask ML] Does the Reduced Allowable LASSO dominate the LASSO approach?","selftext_html":null,"thumbnail":"default","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"i.imgur.com","secure_media_embed":{},"ups":1,"author_flair_text":null,"score":1,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","media":null,"created_utc":1347645721,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","permalink":"/r/MachineLearning/comments/zvybw/ask_ml_does_the_reduced_allowable_lasso_dominate/","gilded":0,"author":"[deleted]","retrieved_on":1413575998,"num_comments":0}
{"over_18":false,"id":"zve1y","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/zve1y/what_is_a_graph_kernel/","mod_reports":[],"media_embed":{},"link_flair_text":null,"edited":false,"title":"What is a \"graph kernel\"","is_self":true,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello ML community.&lt;/p&gt;\n\n&lt;p&gt;I just found this place and probably will lurk around more from now on. Some great threads here!.&lt;/p&gt;\n\n&lt;p&gt;Maybe I will just go straight to the question:&lt;/p&gt;\n\n&lt;p&gt;Can someone explain to me the idea behind &amp;quot;graph kernels&amp;quot; in some simple terms? I haven&amp;#39;t been able to find any educational material on it, just read some papers. It looked like they are reconstructing the graph into various parts (like walks, paths, etc) and then comparing those parts to get the distances between graphs. Is it the whole idea or did I miss it?&lt;/p&gt;\n\n&lt;p&gt;So for example: would simply taking all minimum spanning trees of some weighted graphs and then comparing the sets of those trees be a kernel? Or does it have to be something more (like satisfy triangle inequalities or something else).&lt;/p&gt;\n\n&lt;p&gt;best,\nPMW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","secure_media_embed":{},"ups":16,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"media":null,"created_utc":1347621454,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","selftext":"Hello ML community.\n\nI just found this place and probably will lurk around more from now on. Some great threads here!.\n\nMaybe I will just go straight to the question:\n\nCan someone explain to me the idea behind \"graph kernels\" in some simple terms? I haven't been able to find any educational material on it, just read some papers. It looked like they are reconstructing the graph into various parts (like walks, paths, etc) and then comparing those parts to get the distances between graphs. Is it the whole idea or did I miss it?\n\nSo for example: would simply taking all minimum spanning trees of some weighted graphs and then comparing the sets of those trees be a kernel? Or does it have to be something more (like satisfy triangle inequalities or something else).\n\nbest,\nPMW.","link_flair_css_class":null,"author_flair_text":null,"score":16,"num_comments":15,"retrieved_on":1413576912,"author":"PlayMeWhile","gilded":0,"permalink":"/r/MachineLearning/comments/zve1y/what_is_a_graph_kernel/"}
{"title":"Big data from the viewpoint of statisticians -- interesting discussion at crossvalidated","is_self":false,"link_flair_text":null,"edited":false,"url":"http://stats.stackexchange.com/questions/35971/is-sampling-relevant-in-the-time-of-big-data","user_reports":[],"id":"zxu96","media_embed":{},"mod_reports":[],"over_18":false,"retrieved_on":1413573180,"num_comments":0,"author":"wookietrader","gilded":0,"permalink":"/r/MachineLearning/comments/zxu96/big_data_from_the_viewpoint_of_statisticians/","subreddit":"MachineLearning","banned_by":null,"created_utc":1347739998,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"score":28,"author_flair_text":null,"selftext":"","link_flair_css_class":null,"ups":28,"secure_media_embed":{},"author_flair_css_class":null,"domain":"stats.stackexchange.com","secure_media":null,"distinguished":null,"downs":0,"selftext_html":null,"stickied":false,"thumbnail":"default"}
{"subreddit":"MachineLearning","media":null,"banned_by":null,"created_utc":1347827591,"report_reasons":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":41,"selftext":"I'll start.  I really enjoyed [Brains, Sex, and Machine Learning](http://www.youtube.com/watch?v=DleXA5ADG78) by Geoffrey Hinton.\n\nI'm limiting it to YouTube for the selfish reason that I like to be able to download lectures so that I can watch them on TV :-)","link_flair_css_class":null,"retrieved_on":1413570661,"num_comments":11,"gilded":0,"author":"sanity","permalink":"/r/MachineLearning/comments/zzir6/what_is_your_favorite_machine_learning_lecture_on/","distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll start.  I really enjoyed &lt;a href=\"http://www.youtube.com/watch?v=DleXA5ADG78\"&gt;Brains, Sex, and Machine Learning&lt;/a&gt; by Geoffrey Hinton.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m limiting it to YouTube for the selfish reason that I like to be able to download lectures so that I can watch them on TV :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"secure_media_embed":{},"ups":41,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","edited":false,"link_flair_text":null,"title":"What is your favorite Machine Learning lecture on YouTube?","is_self":true,"over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/zzir6/what_is_your_favorite_machine_learning_lecture_on/","user_reports":[],"id":"zzir6","media_embed":{},"mod_reports":[]}
{"num_comments":6,"retrieved_on":1413569322,"author":"the_awesome_machine","gilded":0,"permalink":"/r/MachineLearning/comments/100fob/a_nonmathematical_introduction_to_using_neural/","report_reasons":null,"created_utc":1347863476,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"score":22,"author_flair_text":null,"ups":22,"secure_media_embed":{},"domain":"heatonresearch.com","secure_media":null,"author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"http://b.thumbs.redditmedia.com/_rNKh0RNGEohwZCO.jpg","stickied":false,"selftext_html":null,"title":"A Non-Mathematical Introduction to Using Neural Networks","is_self":false,"edited":false,"link_flair_text":null,"user_reports":[],"id":"100fob","url":"http://www.heatonresearch.com/content/non-mathematical-introduction-using-neural-networks","mod_reports":[],"media_embed":{},"over_18":false}
{"over_18":false,"mod_reports":[],"media_embed":{},"id":"103rik","user_reports":[],"url":"http://www.overkillanalytics.net/kaggles-wordpress-challenge-the-like-graph/","link_flair_text":null,"edited":false,"is_self":false,"title":"Kaggle's WordPress Challenge: The Like Graph","stickied":false,"thumbnail":"http://a.thumbs.redditmedia.com/5zKs5uz6d7Wu3KT6.jpg","selftext_html":null,"downs":0,"distinguished":null,"secure_media":null,"domain":"overkillanalytics.net","author_flair_css_class":null,"secure_media_embed":{},"ups":19,"link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":19,"media":null,"created_utc":1348007931,"subreddit_id":"t5_2r3gv","banned_by":null,"report_reasons":null,"subreddit":"MachineLearning","gilded":0,"author":"willis77","permalink":"/r/MachineLearning/comments/103rik/kaggles_wordpress_challenge_the_like_graph/","num_comments":1,"retrieved_on":1413564414}
{"retrieved_on":1413564638,"num_comments":1,"author":"stevestoe","gilded":0,"permalink":"/r/MachineLearning/comments/103lw5/the_most_comprehensive_concise_and_up_to_date/","subreddit":"MachineLearning","media":null,"created_utc":1348002911,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":1,"selftext":"","link_flair_css_class":null,"secure_media_embed":{},"ups":1,"author_flair_css_class":null,"secure_media":null,"domain":"artificialbrains.com","distinguished":null,"downs":0,"selftext_html":null,"thumbnail":"http://a.thumbs.redditmedia.com/JpGwDGiAUh--_CDH.jpg","stickied":false,"title":"The most comprehensive, concise and up to date resource on major funded AI &amp; Neural network simulation projects I've found (includes grant details, project timelines, milestones hit etc)","is_self":false,"edited":false,"link_flair_text":null,"url":"http://www.artificialbrains.com/","user_reports":[],"id":"103lw5","media_embed":{},"mod_reports":[],"over_18":false}
{"retrieved_on":1413562008,"num_comments":8,"gilded":0,"author":"reidhoch","permalink":"/r/MachineLearning/comments/105g43/big_data_and_its_big_problems/","subreddit":"MachineLearning","created_utc":1348082735,"subreddit_id":"t5_2r3gv","report_reasons":null,"banned_by":null,"media":null,"score":23,"author_flair_text":null,"link_flair_css_class":null,"selftext":"","ups":23,"secure_media_embed":{},"author_flair_css_class":null,"domain":"npr.org","secure_media":null,"distinguished":null,"downs":0,"selftext_html":null,"thumbnail":"http://c.thumbs.redditmedia.com/94D_vD9D1hqnXftt.jpg","stickied":false,"title":"Big Data And Its Big Problems","is_self":false,"link_flair_text":null,"edited":false,"url":"http://www.npr.org/blogs/13.7/2012/09/18/161334704/big-data-and-its-big-problems","user_reports":[],"id":"105g43","media_embed":{},"mod_reports":[],"over_18":false}
{"title":"Videos of all talks from DataGotham","is_self":false,"edited":false,"link_flair_text":null,"url":"http://www.youtube.com/playlist?list=PLokLecCHtd-9EnGjHRqgFIW0pm5P60rbL&amp;feature=plcp","id":"1050hm","user_reports":[],"media_embed":{"scrolling":false,"height":450,"width":600,"content":"&lt;iframe src=\"http://www.youtube.com/embed/videoseries?list=PLokLecCHtd-9EnGjHRqgFIW0pm5P60rbL\" width=\"600\" height=\"450\" frameborder=\"0\"&gt;&lt;/iframe&gt;"},"mod_reports":[],"over_18":false,"retrieved_on":1413562622,"num_comments":0,"author":"agconway","gilded":0,"permalink":"/r/MachineLearning/comments/1050hm/videos_of_all_talks_from_datagotham/","subreddit":"MachineLearning","media":{"oembed":{"provider_name":"YouTube","width":600,"description":"Share your videos with friends, family, and the world","version":"1.0","height":450,"provider_url":"http://youtube.com","html":"&lt;iframe src=\"http://www.youtube.com/embed/videoseries?list=PLokLecCHtd-9EnGjHRqgFIW0pm5P60rbL\" width=\"600\" height=\"450\" frameborder=\"0\"&gt;&lt;/iframe&gt;","title":"YouTube - Broadcast Yourself.","type":"video"},"type":"youtube.com"},"created_utc":1348067915,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":1,"selftext":"","link_flair_css_class":null,"secure_media_embed":{},"ups":1,"author_flair_css_class":null,"secure_media":null,"domain":"youtube.com","distinguished":null,"downs":0,"selftext_html":null,"stickied":false,"thumbnail":"default"}
{"edited":false,"link_flair_text":null,"title":"Reservoir Computing","is_self":false,"over_18":false,"user_reports":[],"id":"1074rh","url":"http://en.wikipedia.org/wiki/Reservoir_computing","mod_reports":[],"media_embed":{},"media":null,"banned_by":null,"created_utc":1348157532,"subreddit_id":"t5_2r3gv","report_reasons":null,"subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":0,"num_comments":5,"retrieved_on":1413559683,"gilded":0,"permalink":"/r/MachineLearning/comments/1074rh/reservoir_computing/","author":"marshallp","downs":0,"distinguished":null,"thumbnail":"default","stickied":false,"selftext_html":null,"secure_media_embed":{},"ups":0,"secure_media":null,"domain":"en.wikipedia.org","author_flair_css_class":null}
{"link_flair_css_class":null,"selftext":"","author_flair_text":"naive","score":30,"media":null,"created_utc":1348150021,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","gilded":0,"author":"cavedave","permalink":"/r/MachineLearning/comments/106xdc/xerox_now_leaves_all_hiring_for_its_48700/","num_comments":10,"retrieved_on":1413559957,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/HkLwgBUXKH-VZukc.jpg","selftext_html":null,"downs":0,"distinguished":null,"secure_media":null,"domain":"online.wsj.com","author_flair_css_class":null,"secure_media_embed":{},"ups":30,"edited":false,"link_flair_text":null,"is_self":false,"title":"Xerox now leaves all hiring for its 48,700 call-center jobs to software ","over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"106xdc","url":"http://online.wsj.com/article/SB10000872396390443890304578006252019616768.html"}
{"is_self":true,"title":"Time series: Finding the point of saturation. Or: What's the value of D for T -&gt; \\infinity","edited":1348142042,"link_flair_text":null,"mod_reports":[],"media_embed":{},"id":"106rcc","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/106rcc/time_series_finding_the_point_of_saturation_or/","over_18":false,"gilded":0,"author":"qwertz_guy","permalink":"/r/MachineLearning/comments/106rcc/time_series_finding_the_point_of_saturation_or/","num_comments":2,"retrieved_on":1413560187,"link_flair_css_class":null,"selftext":"Hi folk,\nI'm from physics and very new to this topic, so I don't know a good approach yet and I hope you can help me.\n\nHere is the material of interest: http://imgur.com/DagJD,f9OcR,g7BKf,8crYb\nThese are some examples of how my data looks like. The data is based on numerical integration and I have a lot more time series (&gt;10000). Qualitatively, they all look similar: They start at T=0, D=0, have a maximum at some point T_max, fall of (some fall off more, some less, depends on the parameters) and then they go into saturation, oscillating (due to the noise) around the value of D for T-&gt;oo (infinity).\n\nSince the time, which the time series needs to go into saturation, is in general not known, I have to write an on-line algorithm/method (I don't save the data, it's too much) which detects when the time series is oscillating about a specific value of D and doesn't change anymore (so there is no bias/trend anymore).\n\nMeanwhile, I'm working on this specific problem for several weeks now and I haven't found a satisfying solution yet. Is there any known method for problems like that which I don't know about yet? Do you have any ideas or a good approach?\n\nIf it is of interest: I'm programming in C (using GSL).\n_________________________________________________\n\n\nWhat I've done so far: juggling with average/variance/standard-deviation of time windows of a fixed size. My first approach was averaging over these windows and then looking for abs(WindowAverage_1 - WindowAverage_2)/WindowAverage_2 to drop below an arbitrary treshold (which I set to a small number). The time series I linked above are the result of this method. Nevertheless, I'm not sure if this is finally a good way, because although my treshold is very small, there could still be a small bias/trend which lead to another value of D than calculated with this method.\n\n\nMy second approach, which I haven't finished yet due to some problems, is plotting D over 1/T instead of T. The sense behind this is, that at some point I can extrapolate my data to the y-Axis (D-Axis) to receive the value of D for T-&gt;inifnity. So I reduced the problem from infinity to finite. The problem is, that with increasing T, 1/T decreases stronger and with decreasing (1/T_1 - 1/T_2) the data oscillation increases, which does not make it easier.\n\n_________________________________________________\n\n\nMy mind is open for any ideas.\nI hope that this is the right subreddit. Excuse my English please, it's not my native language.\n","author_flair_text":null,"score":2,"media":null,"created_utc":1348141318,"subreddit_id":"t5_2r3gv","banned_by":null,"report_reasons":null,"subreddit":"MachineLearning","secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"secure_media_embed":{},"ups":2,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folk,\nI&amp;#39;m from physics and very new to this topic, so I don&amp;#39;t know a good approach yet and I hope you can help me.&lt;/p&gt;\n\n&lt;p&gt;Here is the material of interest: &lt;a href=\"http://imgur.com/DagJD,f9OcR,g7BKf,8crYb\"&gt;http://imgur.com/DagJD,f9OcR,g7BKf,8crYb&lt;/a&gt;\nThese are some examples of how my data looks like. The data is based on numerical integration and I have a lot more time series (&amp;gt;10000). Qualitatively, they all look similar: They start at T=0, D=0, have a maximum at some point T_max, fall of (some fall off more, some less, depends on the parameters) and then they go into saturation, oscillating (due to the noise) around the value of D for T-&amp;gt;oo (infinity).&lt;/p&gt;\n\n&lt;p&gt;Since the time, which the time series needs to go into saturation, is in general not known, I have to write an on-line algorithm/method (I don&amp;#39;t save the data, it&amp;#39;s too much) which detects when the time series is oscillating about a specific value of D and doesn&amp;#39;t change anymore (so there is no bias/trend anymore).&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, I&amp;#39;m working on this specific problem for several weeks now and I haven&amp;#39;t found a satisfying solution yet. Is there any known method for problems like that which I don&amp;#39;t know about yet? Do you have any ideas or a good approach?&lt;/p&gt;\n\n&lt;p&gt;If it is of interest: I&amp;#39;m programming in C (using GSL).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;What I&amp;#39;ve done so far: juggling with average/variance/standard-deviation of time windows of a fixed size. My first approach was averaging over these windows and then looking for abs(WindowAverage_1 - WindowAverage_2)/WindowAverage_2 to drop below an arbitrary treshold (which I set to a small number). The time series I linked above are the result of this method. Nevertheless, I&amp;#39;m not sure if this is finally a good way, because although my treshold is very small, there could still be a small bias/trend which lead to another value of D than calculated with this method.&lt;/p&gt;\n\n&lt;p&gt;My second approach, which I haven&amp;#39;t finished yet due to some problems, is plotting D over 1/T instead of T. The sense behind this is, that at some point I can extrapolate my data to the y-Axis (D-Axis) to receive the value of D for T-&amp;gt;inifnity. So I reduced the problem from infinity to finite. The problem is, that with increasing T, 1/T decreases stronger and with decreasing (1/T_1 - 1/T_2) the data oscillation increases, which does not make it easier.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;My mind is open for any ideas.\nI hope that this is the right subreddit. Excuse my English please, it&amp;#39;s not my native language.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null}
{"id":"106qqs","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/106qqs/mapping_functions_to_reduce_time_complexity_phd/","mod_reports":[],"media_embed":{},"over_18":false,"title":"Mapping functions to reduce Time Complexity? PhD Qual Item","is_self":true,"edited":false,"link_flair_text":null,"secure_media_embed":{},"ups":0,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This was on my last comp stat qual. I gave an answer I thought was pretty good. We just get our score on the exam, not whether we got specific questions right. Hoping the community can give guidance on this one, I am not interested in the answer so much as what is being tested and where I can go read more about it and get some practice before the next exam.&lt;/p&gt;\n\n&lt;p&gt;At first glance it looks like a time complexity question, but when it starts talking about mapping-functions and pre-sorting data, I am not sure how to handle. So how would you answer?&lt;/p&gt;\n\n&lt;p&gt;Here it is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Given a set of items X = {x1, x2, ..., xn} drawn from some domain Z, your task is to find if a query item q in Z occurs in the set. For simplicity you may assume each item occurs exactly once in X and that it takes O(l) amount of time to compare any two items in Z.\n\n(a) Write pseudo-code for an algorithm which checks if q in X. What is the worst case time complexity of your algorithm?\n\n(b) If l is very large (e.g. if each element of X is a long video) then one needs efficient algorithms to check if q \\in X. Suppose you are given access to k functions h_i: Z -&amp;gt; {1, 2, ..., m} which uniformly map an element of Z to a number between 1 and m, and let k &amp;lt;&amp;lt; l and m &amp;gt; n. Write pseudo-code for an algorithm which uses the function h_1...h_k to check if q \\in X. Note that you are allowed to preprocess the data. What is the worst case time complexity of your algorithm?\n\nBe explicit about the inputs, outputs, and assumptions in your pseudocode.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":2,"retrieved_on":1413560210,"gilded":0,"permalink":"/r/MachineLearning/comments/106qqs/mapping_functions_to_reduce_time_complexity_phd/","author":"osazuwa","media":null,"subreddit_id":"t5_2r3gv","created_utc":1348140064,"report_reasons":null,"banned_by":null,"subreddit":"MachineLearning","selftext":"\n\nThis was on my last comp stat qual. I gave an answer I thought was pretty good. We just get our score on the exam, not whether we got specific questions right. Hoping the community can give guidance on this one, I am not interested in the answer so much as what is being tested and where I can go read more about it and get some practice before the next exam.\n\nAt first glance it looks like a time complexity question, but when it starts talking about mapping-functions and pre-sorting data, I am not sure how to handle. So how would you answer?\n\nHere it is:\n\n    Given a set of items X = {x1, x2, ..., xn} drawn from some domain Z, your task is to find if a query item q in Z occurs in the set. For simplicity you may assume each item occurs exactly once in X and that it takes O(l) amount of time to compare any two items in Z.\n\n    (a) Write pseudo-code for an algorithm which checks if q in X. What is the worst case time complexity of your algorithm?\n\n    (b) If l is very large (e.g. if each element of X is a long video) then one needs efficient algorithms to check if q \\in X. Suppose you are given access to k functions h_i: Z -&gt; {1, 2, ..., m} which uniformly map an element of Z to a number between 1 and m, and let k &lt;&lt; l and m &gt; n. Write pseudo-code for an algorithm which uses the function h_1...h_k to check if q \\in X. Note that you are allowed to preprocess the data. What is the worst case time complexity of your algorithm?\n\n    Be explicit about the inputs, outputs, and assumptions in your pseudocode.\n\n","link_flair_css_class":null,"author_flair_text":null,"score":0}
{"edited":false,"link_flair_text":null,"is_self":true,"title":"Understanding gain and decision trees","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/1064xb/understanding_gain_and_decision_trees/","user_reports":[],"id":"1064xb","author_flair_text":null,"score":2,"link_flair_css_class":null,"selftext":"I am trying to understand the gain formula for decision trees and I am currently going off of this slide deck: http://www.site.uottawa.ca/~stan/csi5387/set1.pdf\n\n(Look at slides 25-30).\n\nI am not quite following this information as the deck isn't the best put together.  Do you know of any sources that can explain the same topic in a way I can actually understand?\n\nThanks","subreddit":"MachineLearning","media":null,"created_utc":1348105914,"report_reasons":null,"subreddit_id":"t5_2r3gv","banned_by":null,"permalink":"/r/MachineLearning/comments/1064xb/understanding_gain_and_decision_trees/","gilded":0,"author":"[deleted]","retrieved_on":1413561049,"num_comments":4,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to understand the gain formula for decision trees and I am currently going off of this slide deck: &lt;a href=\"http://www.site.uottawa.ca/%7Estan/csi5387/set1.pdf\"&gt;http://www.site.uottawa.ca/~stan/csi5387/set1.pdf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;(Look at slides 25-30).&lt;/p&gt;\n\n&lt;p&gt;I am not quite following this information as the deck isn&amp;#39;t the best put together.  Do you know of any sources that can explain the same topic in a way I can actually understand?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"default","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","secure_media_embed":{},"ups":2}
{"over_18":false,"mod_reports":[],"media_embed":{},"user_reports":[],"id":"109dbg","url":"http://gigaom.com/data/forget-your-fancy-data-science-try-overkill-analytics/","edited":false,"link_flair_text":null,"is_self":false,"title":"Forget your fancy data science, try overkill analytics","thumbnail":"http://b.thumbs.redditmedia.com/-RrXpZmjn9MTC-nO.jpg","stickied":false,"selftext_html":null,"downs":0,"distinguished":null,"domain":"gigaom.com","secure_media":null,"author_flair_css_class":null,"ups":14,"secure_media_embed":{},"link_flair_css_class":null,"selftext":"","score":14,"author_flair_text":null,"created_utc":1348252104,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","gilded":0,"permalink":"/r/MachineLearning/comments/109dbg/forget_your_fancy_data_science_try_overkill/","author":"ecsibleyjr","num_comments":6,"retrieved_on":1413556347}
{"report_reasons":null,"created_utc":1348353253,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","selftext":"I have recently enrolled in a beginner's course in machine learning for NLP, and we are now getting familiar with using Weka for decision tree and information gain analysis. For this, we got a data set of German plural relations, and was tasked to perform information gain analysis using InfoGainAttributeEval algorithm. Later, we classified the data using J48, and one thing that hit me when looking at the result was that the root node in the decision tree (\"gender\") wasn't the same feature as the one scoring the best in the IG analysis (\"p6\"). \n\nWhat is the reason for this?\n\nSome data from the IG analysis:\n\n    Ranked attributes:\n     0.79096   7 p6\n     0.71869   8 gender\n     0.70808   6 p5\n     0.30862   5 p4\n     0.17876   3 p2\n     0.14806   4 p3\n     0.11683   2 p1\n     0.00266   1 frequency\n\nEDIT: You will find the output from the classifier [here](http://pastebin.com/Q4WjwUYy).","link_flair_css_class":null,"score":6,"author_flair_text":null,"num_comments":5,"retrieved_on":1413553496,"permalink":"/r/MachineLearning/comments/10bezv/decision_trees_and_information_gain_highest_ig/","gilded":0,"author":"HerrKanin","downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently enrolled in a beginner&amp;#39;s course in machine learning for NLP, and we are now getting familiar with using Weka for decision tree and information gain analysis. For this, we got a data set of German plural relations, and was tasked to perform information gain analysis using InfoGainAttributeEval algorithm. Later, we classified the data using J48, and one thing that hit me when looking at the result was that the root node in the decision tree (&amp;quot;gender&amp;quot;) wasn&amp;#39;t the same feature as the one scoring the best in the IG analysis (&amp;quot;p6&amp;quot;). &lt;/p&gt;\n\n&lt;p&gt;What is the reason for this?&lt;/p&gt;\n\n&lt;p&gt;Some data from the IG analysis:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Ranked attributes:\n 0.79096   7 p6\n 0.71869   8 gender\n 0.70808   6 p5\n 0.30862   5 p4\n 0.17876   3 p2\n 0.14806   4 p3\n 0.11683   2 p1\n 0.00266   1 frequency\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;EDIT: You will find the output from the classifier &lt;a href=\"http://pastebin.com/Q4WjwUYy\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","ups":6,"secure_media_embed":{},"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"link_flair_text":null,"edited":1348357290,"title":"Decision trees and Information Gain – highest IG doesn't necessarily mean root?","is_self":true,"over_18":false,"user_reports":[],"id":"10bezv","url":"http://www.reddit.com/r/MachineLearning/comments/10bezv/decision_trees_and_information_gain_highest_ig/","mod_reports":[],"media_embed":{}}
{"ups":15,"secure_media_embed":{},"author_flair_css_class":null,"domain":"sharpneat.sourceforge.net","secure_media":null,"distinguished":null,"downs":0,"selftext_html":null,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/Ctj8GaNySNuJ4mm-.jpg","retrieved_on":1413553683,"num_comments":0,"author":"locster","gilded":0,"permalink":"/r/MachineLearning/comments/10b9sy/sharpneat_evolution_of_neural_networks_in_c_net/","subreddit":"MachineLearning","created_utc":1348347203,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","media":null,"score":15,"author_flair_text":null,"link_flair_css_class":null,"selftext":"","url":"http://sharpneat.sourceforge.net/","user_reports":[],"id":"10b9sy","media_embed":{},"mod_reports":[],"over_18":false,"title":"SharpNEAT - Evolution of neural networks (in C# / .Net / Task Parallel Library)","is_self":false,"link_flair_text":null,"edited":false}
{"secure_media_embed":{},"ups":6,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"downs":0,"distinguished":null,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wrote code for an evolutionary algorithm that performs feature selection on a fairly feature rich (300-500 feature) dataset whose fitness function is an SVM.&lt;/p&gt;\n\n&lt;p&gt;It functions correctly, but the speed of the algorithm leaves much to be desired.  I was wondering what some suggestions for parallelizing or scaling up the code might be.  Specifically, in a given generation, I will have 200-400 chromosomes whose fitnesses (area under ROC) need to be determined but can be done so independently.&lt;/p&gt;\n\n&lt;p&gt;However, an additional challenge is that the dataset changes each generation (both a resampling and noise perturbation), and thus the data might need to be synchronized if the solution is network based.&lt;/p&gt;\n\n&lt;p&gt;The algorithm itself is not my design, and thus I cannot change the way it fundamentally functions to speed it up.  I have heard of MPI, OpenMP, and Hadoop, but I would like to get some input before I learn a new technique.&lt;/p&gt;\n\n&lt;p&gt;I should note that I have decent experience with C/C++, Java, Python, C#, R, and have done a little with PHP/MySQL a while ago.  I dual boot Windows and Linux, and can use both proficiently.  I have a hexacore processor in my personal desktop, and I also have access to 10-20 desktop machines (friend&amp;#39;s computers).&lt;/p&gt;\n\n&lt;p&gt;Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","num_comments":14,"retrieved_on":1413554960,"gilded":0,"permalink":"/r/MachineLearning/comments/10acpk/best_way_to_scale_upparallelize_evolutionary/","author":"Omega037","media":null,"report_reasons":null,"created_utc":1348292390,"subreddit_id":"t5_2r3gv","banned_by":null,"subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"I wrote code for an evolutionary algorithm that performs feature selection on a fairly feature rich (300-500 feature) dataset whose fitness function is an SVM.\n\nIt functions correctly, but the speed of the algorithm leaves much to be desired.  I was wondering what some suggestions for parallelizing or scaling up the code might be.  Specifically, in a given generation, I will have 200-400 chromosomes whose fitnesses (area under ROC) need to be determined but can be done so independently.\n\nHowever, an additional challenge is that the dataset changes each generation (both a resampling and noise perturbation), and thus the data might need to be synchronized if the solution is network based.\n\nThe algorithm itself is not my design, and thus I cannot change the way it fundamentally functions to speed it up.  I have heard of MPI, OpenMP, and Hadoop, but I would like to get some input before I learn a new technique.\n\nI should note that I have decent experience with C/C++, Java, Python, C#, R, and have done a little with PHP/MySQL a while ago.  I dual boot Windows and Linux, and can use both proficiently.  I have a hexacore processor in my personal desktop, and I also have access to 10-20 desktop machines (friend's computers).\n\nAny advice?","author_flair_text":null,"score":6,"id":"10acpk","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10acpk/best_way_to_scale_upparallelize_evolutionary/","mod_reports":[],"media_embed":{},"over_18":false,"title":"Best way to scale up/parallelize evolutionary algorithms?","is_self":true,"edited":1348294075,"link_flair_text":null}
{"id":"10ccbg","user_reports":[],"url":"http://highscalability.com/blog/2012/7/30/prismatic-architecture-using-machine-learning-on-social-netw.html","mod_reports":[],"media_embed":{},"over_18":false,"title":"Prismatic Architecture - Using Machine Learning On Social Networks To Figure Out What You Should Read On The Web ","is_self":false,"link_flair_text":null,"edited":false,"secure_media_embed":{},"ups":2,"secure_media":null,"domain":"highscalability.com","author_flair_css_class":null,"downs":0,"distinguished":null,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/MEYBwAj0BsIjdWlc.jpg","selftext_html":null,"num_comments":0,"retrieved_on":1413552228,"author":"greenrd","gilded":0,"permalink":"/r/MachineLearning/comments/10ccbg/prismatic_architecture_using_machine_learning_on/","media":null,"created_utc":1348411587,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","link_flair_css_class":null,"selftext":"","author_flair_text":null,"score":2}
{"edited":false,"link_flair_text":null,"title":"How to choose k in cross validation?","is_self":true,"over_18":false,"url":"http://www.reddit.com/r/MachineLearning/comments/10espa/how_to_choose_k_in_cross_validation/","user_reports":[],"id":"10espa","media_embed":{},"mod_reports":[],"subreddit":"MachineLearning","banned_by":null,"created_utc":1348519203,"subreddit_id":"t5_2r3gv","report_reasons":null,"media":null,"score":9,"author_flair_text":null,"link_flair_css_class":null,"selftext":"How should one choose k when doing a k-fold cross validation? Are there any advantages/disadvantages to going lower or higher than 10? If the training set is relatively small, would it be better to use a lower k?\n\nThank you!","retrieved_on":1413548859,"num_comments":11,"permalink":"/r/MachineLearning/comments/10espa/how_to_choose_k_in_cross_validation/","author":"MicturitionSyncope","gilded":0,"distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How should one choose k when doing a k-fold cross validation? Are there any advantages/disadvantages to going lower or higher than 10? If the training set is relatively small, would it be better to use a lower k?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"ups":9,"secure_media_embed":{},"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null}
{"selftext":"Given that it is September, I, and many others like me are taking out first ML course at a university (or other places).\n\nI was given an .arff file of data and can easily figure out on weka how to make a decision tree, both pruned and unpruned.  What I am to do next is where I am having problems.  \n\nI am to evaluate the results of these trees by using cross-validation, and then execute statistical tests (T and Wilconox).  We are given the hint to use experimentalComparison() in the DMwR package on R as well as the function compAnalysis() for the Wilconox test.  This is where I am getting stuck.  Is that cross-validation not part of Weka when building the decision tree?  i.e. using 10-fold cross validation.\n\nAnother issue is I can't figure out (by using Google to first try and help) to take these trees from Weka, place them in R and do these tests.  I guess a part of this problem is never having used R.  \n\nWhat are some good resources on R that might help me with figuring this out?","link_flair_css_class":null,"author_flair_text":null,"score":3,"media":null,"report_reasons":null,"created_utc":1348505834,"banned_by":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","author":"[deleted]","gilded":0,"permalink":"/r/MachineLearning/comments/10eek5/learning_ml_on_weka_and_r_with_decisions_trees/","num_comments":7,"retrieved_on":1413549400,"stickied":false,"thumbnail":"default","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given that it is September, I, and many others like me are taking out first ML course at a university (or other places).&lt;/p&gt;\n\n&lt;p&gt;I was given an .arff file of data and can easily figure out on weka how to make a decision tree, both pruned and unpruned.  What I am to do next is where I am having problems.  &lt;/p&gt;\n\n&lt;p&gt;I am to evaluate the results of these trees by using cross-validation, and then execute statistical tests (T and Wilconox).  We are given the hint to use experimentalComparison() in the DMwR package on R as well as the function compAnalysis() for the Wilconox test.  This is where I am getting stuck.  Is that cross-validation not part of Weka when building the decision tree?  i.e. using 10-fold cross validation.&lt;/p&gt;\n\n&lt;p&gt;Another issue is I can&amp;#39;t figure out (by using Google to first try and help) to take these trees from Weka, place them in R and do these tests.  I guess a part of this problem is never having used R.  &lt;/p&gt;\n\n&lt;p&gt;What are some good resources on R that might help me with figuring this out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"secure_media_embed":{},"ups":3,"edited":false,"link_flair_text":null,"is_self":true,"title":"Learning ML on Weka and R with Decisions Trees","over_18":false,"mod_reports":[],"media_embed":{},"id":"10eek5","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10eek5/learning_ml_on_weka_and_r_with_decisions_trees/"}
{"media_embed":{},"mod_reports":[],"url":"http://www.stonetemple.com/a-mathematical-model-for-assessing-page-quality/","id":"10ggxx","user_reports":[],"over_18":false,"is_self":false,"title":"A mathematical model for accessing web page quality","link_flair_text":null,"edited":false,"author_flair_css_class":null,"domain":"stonetemple.com","secure_media":null,"ups":13,"secure_media_embed":{},"selftext_html":null,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/HPbiY7jNpvSY4ONK.jpg","distinguished":null,"downs":0,"author":"rrenaud","gilded":0,"permalink":"/r/MachineLearning/comments/10ggxx/a_mathematical_model_for_accessing_web_page/","retrieved_on":1413546610,"num_comments":0,"score":13,"author_flair_text":null,"selftext":"","link_flair_css_class":null,"subreddit":"MachineLearning","report_reasons":null,"created_utc":1348591500,"subreddit_id":"t5_2r3gv","banned_by":null,"media":null}
{"over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10gfj0/statistical_significance_between_classifiers/","user_reports":[],"id":"10gfj0","edited":1348591551,"link_flair_text":null,"is_self":true,"title":"Statistical significance between classifiers using different sets","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to preface this in saying that I have almost no statistics background, so forgive my ignorance.&lt;/p&gt;\n\n&lt;p&gt;I have a four-class classification problem where I am using both a SVM and bagging with logistic regression as the base classifier.&lt;/p&gt;\n\n&lt;p&gt;My validation set has 35 instances (10,8,10,7 for each class) and my test set has 38 instances (10, 9, 11, 8 for each class).  The sets are small but it&amp;#39;s a medical imaging problem and getting more data isn&amp;#39;t feasible.  &lt;/p&gt;\n\n&lt;p&gt;When I run the bagging on the validation set I get a classification rate of about 89%, and 94% on the test set.&lt;/p&gt;\n\n&lt;p&gt;On the SVM I get 83% on the validation set but 94% on the test set, which seems abnormal.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking that because the sets are so small that the difference isn&amp;#39;t really that significant even if it looks a lot in percent form.&lt;/p&gt;\n\n&lt;p&gt;I know you can use t-tests to look for significant differences between two classifiers which use the same set, but how would I go about checking for non-significant difference between two classifiers on different sets?  &lt;/p&gt;\n\n&lt;p&gt;edit:  poor title choice.  What I am really interested in is checking for non-significant difference between the SVM on the validation set and the SVM on the test set.  The training set has 73 instances.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"default","distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","secure_media_embed":{},"ups":7,"author_flair_text":null,"score":7,"link_flair_css_class":null,"selftext":"I'm going to preface this in saying that I have almost no statistics background, so forgive my ignorance.\n\nI have a four-class classification problem where I am using both a SVM and bagging with logistic regression as the base classifier.\n\nMy validation set has 35 instances (10,8,10,7 for each class) and my test set has 38 instances (10, 9, 11, 8 for each class).  The sets are small but it's a medical imaging problem and getting more data isn't feasible.  \n\nWhen I run the bagging on the validation set I get a classification rate of about 89%, and 94% on the test set.\n\nOn the SVM I get 83% on the validation set but 94% on the test set, which seems abnormal.  \n\nI'm thinking that because the sets are so small that the difference isn't really that significant even if it looks a lot in percent form.\n\nI know you can use t-tests to look for significant differences between two classifiers which use the same set, but how would I go about checking for non-significant difference between two classifiers on different sets?  \n\nedit:  poor title choice.  What I am really interested in is checking for non-significant difference between the SVM on the validation set and the SVM on the test set.  The training set has 73 instances.","subreddit":"MachineLearning","media":null,"banned_by":null,"created_utc":1348590223,"subreddit_id":"t5_2r3gv","report_reasons":null,"gilded":0,"author":"[deleted]","permalink":"/r/MachineLearning/comments/10gfj0/statistical_significance_between_classifiers/","retrieved_on":1413546660,"num_comments":3}
{"retrieved_on":1413543588,"num_comments":10,"gilded":0,"author":"tigergirl22","permalink":"/r/MachineLearning/comments/10iqxu/askmachinelearning_can_anyone_define_a/","subreddit":"MachineLearning","media":null,"created_utc":1348685816,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","author_flair_text":null,"score":3,"link_flair_css_class":null,"selftext":"I am reading Sutton and Barto's Reinforcement Learning for a paper on temporal difference learning for my neuroscience major. I have only taken calculus and some statistics so the math is quite challenging for me. Could anyone explain these concepts to me? Thanks! ","secure_media_embed":{},"ups":3,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am reading Sutton and Barto&amp;#39;s Reinforcement Learning for a paper on temporal difference learning for my neuroscience major. I have only taken calculus and some statistics so the math is quite challenging for me. Could anyone explain these concepts to me? Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","title":"AskMachineLearning: can anyone define a deterministic vs. a stochastic policy for a beginner? What it means to \"back up\" states? ","is_self":true,"edited":false,"link_flair_text":null,"url":"http://www.reddit.com/r/MachineLearning/comments/10iqxu/askmachinelearning_can_anyone_define_a/","user_reports":[],"id":"10iqxu","media_embed":{},"mod_reports":[],"over_18":false}
{"secure_media":null,"domain":"self.MachineLearning","author_flair_css_class":null,"secure_media_embed":{},"ups":2,"thumbnail":"self","stickied":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m clustering the given data using k-means and I&amp;#39;m trying to find the accuracy of that clustering by comparing the labels got from clustering to class labels I&amp;#39;ve. The K-means gives its own ordering ie whole of class 1 in my labeling might be called class 2 by k-means which doesn&amp;#39;t indicate that clustering is wrong.\nHow do I find the accuracy of clustering in cases like these?&lt;/p&gt;\n\n&lt;p&gt;A matlab code snippet might be most helpful to me. \nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"permalink":"/r/MachineLearning/comments/10i725/finding_accuracy_of_kmeans_xpost_form_rmatlab/","gilded":0,"author":"rorschach122","num_comments":9,"retrieved_on":1413544354,"selftext":"I'm clustering the given data using k-means and I'm trying to find the accuracy of that clustering by comparing the labels got from clustering to class labels I've. The K-means gives its own ordering ie whole of class 1 in my labeling might be called class 2 by k-means which doesn't indicate that clustering is wrong.\nHow do I find the accuracy of clustering in cases like these?\n\nA matlab code snippet might be most helpful to me. \nThanks","link_flair_css_class":null,"author_flair_text":null,"score":2,"media":null,"subreddit_id":"t5_2r3gv","created_utc":1348666193,"banned_by":null,"report_reasons":null,"subreddit":"MachineLearning","mod_reports":[],"media_embed":{},"user_reports":[],"id":"10i725","url":"http://www.reddit.com/r/MachineLearning/comments/10i725/finding_accuracy_of_kmeans_xpost_form_rmatlab/","over_18":false,"is_self":true,"title":"Finding accuracy of k-Means (xpost form /r/matlab)","edited":false,"link_flair_text":null}
{"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are many facial recognition algorithms, eye and mouth recognition algorithms. I was wondering if anyone had tried utilizing them to enhance voice recognition algorithms. &lt;/p&gt;\n\n&lt;p&gt;I thought of this while I was trying to dictate on my Mac and was staring at the little Hal9000 camera in the middle of the screen. I searched around on Google Scholar and couldn&amp;#39;t find anything.&lt;/p&gt;\n\n&lt;p&gt;Just thought I&amp;#39;d ask.&lt;/p&gt;\n\n&lt;p&gt;cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","secure_media_embed":{},"ups":17,"author_flair_text":null,"score":17,"link_flair_css_class":null,"selftext":"There are many facial recognition algorithms, eye and mouth recognition algorithms. I was wondering if anyone had tried utilizing them to enhance voice recognition algorithms. \n\nI thought of this while I was trying to dictate on my Mac and was staring at the little Hal9000 camera in the middle of the screen. I searched around on Google Scholar and couldn't find anything.\n\nJust thought I'd ask.\n\ncheers","subreddit":"MachineLearning","media":null,"report_reasons":null,"created_utc":1348624479,"banned_by":null,"subreddit_id":"t5_2r3gv","permalink":"/r/MachineLearning/comments/10hgdr/askmachinelearning_has_anyone_enhanced_the/","gilded":0,"author":"skytomorrownow","retrieved_on":1413545320,"num_comments":8,"over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10hgdr/askmachinelearning_has_anyone_enhanced_the/","id":"10hgdr","user_reports":[],"edited":false,"link_flair_text":null,"is_self":true,"title":"AskMachineLearning: has anyone enhanced the quality of audio voice recognition by reading lips?"}
{"link_flair_text":null,"edited":false,"title":"A few useful things to know about machine learning (CACM article, pdf)","is_self":false,"over_18":false,"id":"10kg4s","user_reports":[],"url":"http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf","mod_reports":[],"media_embed":{},"media":null,"created_utc":1348761346,"banned_by":null,"report_reasons":null,"subreddit_id":"t5_2r3gv","subreddit":"MachineLearning","selftext":"","link_flair_css_class":null,"author_flair_text":null,"score":35,"num_comments":4,"retrieved_on":1413541282,"gilded":0,"author":"kmjn","permalink":"/r/MachineLearning/comments/10kg4s/a_few_useful_things_to_know_about_machine/","downs":0,"distinguished":null,"stickied":false,"thumbnail":"default","selftext_html":null,"secure_media_embed":{},"ups":35,"secure_media":null,"domain":"homes.cs.washington.edu","author_flair_css_class":null}
{"retrieved_on":1413541328,"num_comments":6,"gilded":0,"permalink":"/r/MachineLearning/comments/10kes5/where_to_start_in_nlp/","author":"urmyheartBeatStopR","subreddit":"MachineLearning","media":null,"created_utc":1348759999,"banned_by":null,"subreddit_id":"t5_2r3gv","report_reasons":null,"author_flair_text":null,"score":1,"selftext":"Hi, I am currently review my maths and following this suggestion (http://pindancing.blogspot.in/2010/01/learning-about-machine-learniing.html).\n\nI would like the shortest route or base math requirement before tackling NLP subject. But, I want to have a solid foundation first before even trying NLP.\n\nWhat math does NLP requires?  \n\nAfter reviewing my math where do I start with NLP? I saw a coursera class for it and will take it eventually. I also bought O'Reilly NLP python book.\n\nWhat programming languages does the industries mostly use for NLP? I've seen Scala &amp; Python. What about Mathlab, R, Java? \n\nThank you for taking the time to read this.","link_flair_css_class":null,"secure_media_embed":{},"ups":1,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am currently review my maths and following this suggestion (&lt;a href=\"http://pindancing.blogspot.in/2010/01/learning-about-machine-learniing.html\"&gt;http://pindancing.blogspot.in/2010/01/learning-about-machine-learniing.html&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;I would like the shortest route or base math requirement before tackling NLP subject. But, I want to have a solid foundation first before even trying NLP.&lt;/p&gt;\n\n&lt;p&gt;What math does NLP requires?  &lt;/p&gt;\n\n&lt;p&gt;After reviewing my math where do I start with NLP? I saw a coursera class for it and will take it eventually. I also bought O&amp;#39;Reilly NLP python book.&lt;/p&gt;\n\n&lt;p&gt;What programming languages does the industries mostly use for NLP? I&amp;#39;ve seen Scala &amp;amp; Python. What about Mathlab, R, Java? &lt;/p&gt;\n\n&lt;p&gt;Thank you for taking the time to read this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","stickied":false,"thumbnail":"self","title":"Where to start in NLP?","is_self":true,"edited":false,"link_flair_text":null,"url":"http://www.reddit.com/r/MachineLearning/comments/10kes5/where_to_start_in_nlp/","id":"10kes5","user_reports":[],"media_embed":{},"mod_reports":[],"over_18":false}
{"gilded":0,"permalink":"/r/MachineLearning/comments/10mdtf/lsa_vs_plsa_vs_lda/","author":"GratefulTony","retrieved_on":1413538552,"num_comments":5,"author_flair_text":null,"score":9,"selftext":"Hi all, I am having a bit of trouble getting my head around LSA and pLSA. I have been working for a while in the field of generative-model-type NLP algorithms like LDA PAM and CTM, but I can't seem to fold the non-generative method LSA into my cognitive \"fold\"... as it were of conceptual continuity.\n\nComparing the generative algorithms is fairly strightforward for me as thier model is explicitly stated in thier formation, however, I am trying to understand the differences in the interpretation between LSA and generative model results.\n\nCan anyone help me out or point me to a good paper?","link_flair_css_class":null,"subreddit":"MachineLearning","media":null,"banned_by":null,"created_utc":1348845614,"subreddit_id":"t5_2r3gv","report_reasons":null,"author_flair_css_class":null,"secure_media":null,"domain":"self.MachineLearning","secure_media_embed":{},"ups":9,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I am having a bit of trouble getting my head around LSA and pLSA. I have been working for a while in the field of generative-model-type NLP algorithms like LDA PAM and CTM, but I can&amp;#39;t seem to fold the non-generative method LSA into my cognitive &amp;quot;fold&amp;quot;... as it were of conceptual continuity.&lt;/p&gt;\n\n&lt;p&gt;Comparing the generative algorithms is fairly strightforward for me as thier model is explicitly stated in thier formation, however, I am trying to understand the differences in the interpretation between LSA and generative model results.&lt;/p&gt;\n\n&lt;p&gt;Can anyone help me out or point me to a good paper?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"distinguished":null,"downs":0,"is_self":true,"title":"LSA vs pLSA vs LDA","link_flair_text":null,"edited":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10mdtf/lsa_vs_plsa_vs_lda/","user_reports":[],"id":"10mdtf","over_18":false}
{"selftext":"I'm trying to interpret the results of some measurements in geophysics and thought this crowd might know the answer:\n\nIf a set of events A and B have a joint probability distribution P(A,B), how might this causality be described (in laymen English) between A and B from this joint probability? \n\nIf A and B have a joint expectation E(AB), how can causality be described from the elements of the matrix that can be written for each of the expected interactions A and B? [For instance, E(A_1B_1), E(A_2B_1), and so on]","link_flair_css_class":null,"score":14,"author_flair_text":null,"created_utc":1348952073,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"subreddit":"MachineLearning","author":"revocation","gilded":0,"permalink":"/r/MachineLearning/comments/10okc6/relationship_between_joint_probability/","num_comments":7,"retrieved_on":1413535540,"stickied":false,"thumbnail":"self","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to interpret the results of some measurements in geophysics and thought this crowd might know the answer:&lt;/p&gt;\n\n&lt;p&gt;If a set of events A and B have a joint probability distribution P(A,B), how might this causality be described (in laymen English) between A and B from this joint probability? &lt;/p&gt;\n\n&lt;p&gt;If A and B have a joint expectation E(AB), how can causality be described from the elements of the matrix that can be written for each of the expected interactions A and B? [For instance, E(A_1B_1), E(A_2B_1), and so on]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","downs":0,"distinguished":null,"domain":"self.MachineLearning","secure_media":null,"author_flair_css_class":null,"ups":14,"secure_media_embed":{},"edited":false,"link_flair_text":null,"is_self":true,"title":"Relationship between joint probability distribution and causality?","over_18":false,"mod_reports":[],"media_embed":{},"id":"10okc6","user_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10okc6/relationship_between_joint_probability/"}
{"url":"http://www.reddit.com/r/MachineLearning/comments/10nvy0/what_would_be_a_trivial_statistical_baseline_for/","id":"10nvy0","user_reports":[],"media_embed":{},"mod_reports":[],"over_18":false,"title":"What would be a trivial statistical baseline for comparison with SVM based text categorization?","is_self":true,"edited":false,"link_flair_text":null,"ups":2,"secure_media_embed":{},"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"distinguished":null,"downs":0,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got an assignment to implement a classifier that will categorize documents in quite large corpus. I decided to go with SVMs with linear kernel, achieved good results, moved on to theoretical questions and got stuck on this one...&lt;/p&gt;\n\n&lt;p&gt;I admit that my statistical knowledge has gotten a little bit rusty, so... halp! I&amp;#39;m not looking for complete solutions, any suggestions would be much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"default","stickied":false,"retrieved_on":1413536471,"num_comments":5,"gilded":0,"author":"[deleted]","permalink":"/r/MachineLearning/comments/10nvy0/what_would_be_a_trivial_statistical_baseline_for/","subreddit":"MachineLearning","created_utc":1348915823,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"score":2,"author_flair_text":null,"link_flair_css_class":null,"selftext":"I got an assignment to implement a classifier that will categorize documents in quite large corpus. I decided to go with SVMs with linear kernel, achieved good results, moved on to theoretical questions and got stuck on this one...\n\nI admit that my statistical knowledge has gotten a little bit rusty, so... halp! I'm not looking for complete solutions, any suggestions would be much appreciated."}
{"link_flair_text":null,"edited":false,"is_self":false,"title":"A tutorial on Latent Semantic Analysis with Python code","over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.puffinwarellc.com/index.php/news-and-articles/articles/33.html?showall=1","user_reports":[],"id":"10qafp","author_flair_text":null,"score":16,"link_flair_css_class":null,"selftext":"","subreddit":"MachineLearning","media":null,"created_utc":1349043331,"report_reasons":null,"banned_by":null,"subreddit_id":"t5_2r3gv","gilded":0,"author":"rrenaud","permalink":"/r/MachineLearning/comments/10qafp/a_tutorial_on_latent_semantic_analysis_with/","retrieved_on":1413533171,"num_comments":0,"selftext_html":null,"stickied":false,"thumbnail":"http://d.thumbs.redditmedia.com/QJd-BmHFnGfQefzE.jpg","distinguished":null,"downs":0,"author_flair_css_class":null,"secure_media":null,"domain":"puffinwarellc.com","secure_media_embed":{},"ups":16}
{"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As machine learning algorithms are trained on more and more data, their prediction for any given set of input attributes is likely to converge to a specific prediction.&lt;/p&gt;\n\n&lt;p&gt;While they have less data (or, perhaps just less data about the particular combination of attributes we&amp;#39;re interested in), there is greater &amp;quot;uncertainty&amp;quot; about what the prediction might ultimately be given abundant training data.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have an algorithm that could inject randomness into it&amp;#39;s predictions in direct proportion to it&amp;#39;s uncertainty about its prediction.&lt;/p&gt;\n\n&lt;p&gt;You could think of this as a generalization of the beta distribution.  Let&amp;#39;s say you had no input attributes, and 5 of your outcomes were 1, and 10 were 0.  The most likely outcome probability is 1/3rd, but it could be 1/2 or 1/4.  We could generate random numbers within this uncertainty by looking at &lt;a href=\"http://www.wolframalpha.com/input/?i=beta+distribution+%285%2C+10%29\"&gt;Beta Distribution (5, 10)&lt;/a&gt; (note the &amp;quot;random sample from the distribution&amp;quot; provided by wolfram alpha).&lt;/p&gt;\n\n&lt;p&gt;However this only works with no input attributes, what I need is more of a &amp;quot;contextual beta distribution&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Any ideas?&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;edit:&lt;/em&gt; I guess I&amp;#39;m hoping for an extension to an existing supervised learning example.  For example, I&amp;#39;ve considered a bayesian network learner where the value of P(A|B) is determined using a beta random variable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","thumbnail":"self","stickied":false,"distinguished":null,"downs":0,"author_flair_css_class":null,"domain":"self.MachineLearning","secure_media":null,"ups":3,"secure_media_embed":{},"score":3,"author_flair_text":null,"link_flair_css_class":null,"selftext":"As machine learning algorithms are trained on more and more data, their prediction for any given set of input attributes is likely to converge to a specific prediction.\n\nWhile they have less data (or, perhaps just less data about the particular combination of attributes we're interested in), there is greater \"uncertainty\" about what the prediction might ultimately be given abundant training data.\n\nI'd like to have an algorithm that could inject randomness into it's predictions in direct proportion to it's uncertainty about its prediction.\n\nYou could think of this as a generalization of the beta distribution.  Let's say you had no input attributes, and 5 of your outcomes were 1, and 10 were 0.  The most likely outcome probability is 1/3rd, but it could be 1/2 or 1/4.  We could generate random numbers within this uncertainty by looking at [Beta Distribution (5, 10)](http://www.wolframalpha.com/input/?i=beta+distribution+%285%2C+10%29) (note the \"random sample from the distribution\" provided by wolfram alpha).\n\nHowever this only works with no input attributes, what I need is more of a \"contextual beta distribution\".\n\nAny ideas?\n\n*edit:* I guess I'm hoping for an extension to an existing supervised learning example.  For example, I've considered a bayesian network learner where the value of P(A|B) is determined using a beta random variable.","subreddit":"MachineLearning","report_reasons":null,"created_utc":1349030033,"banned_by":null,"subreddit_id":"t5_2r3gv","media":null,"author":"sanity","gilded":0,"permalink":"/r/MachineLearning/comments/10pxol/is_there_a_machine_learning_algorithm_that_can/","retrieved_on":1413533664,"num_comments":22,"over_18":false,"media_embed":{},"mod_reports":[],"url":"http://www.reddit.com/r/MachineLearning/comments/10pxol/is_there_a_machine_learning_algorithm_that_can/","user_reports":[],"id":"10pxol","edited":1349031000,"link_flair_text":null,"is_self":true,"title":"Is there a machine learning algorithm that can inject randomness in proportion to it's uncertainty about a predicted outcome?"}
